```{r setup, include=FALSE}
options(
    tibble.max_extra_cols = 6,
    tibble.width = 60
)
```


## üëã Welcome to SOCI 3040!

This is an introduction the knowledge and skills you need to tell credible stories with quantitative data...

- TODO: Add intro slides and overview
- They read the chapter for Thursday, but Thursday will be intro to R in lab session

## Why Data Storytelling Matters

> "A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing." [@alexander2023telling]

- Core foundation of quantitative research methods
- Bridge between analysis and understanding
- Essential skill for modern researchers

## Common Concerns

Five Key Questions for Data Stories

- What is the dataset? Who generated it and why?
- What is the underlying process? What's missing?
- What is the dataset trying to say? What else could it say?
- What do we want others to see? How do we convince them?
- Who is affected? Are they represented in the data?

::: {.notes}
1. What is the dataset? Who generated the dataset and why?
2. What is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?
3. What is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?
4. What are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?
5. Who is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?
:::

## Core Workflow Components

```{mermaid}
%%| label: fig-workflow
%%| fig-cap: "Rohan Alexander's [-@alexander2023telling] workflow for telling stories with data."
flowchart LR
    p[[Plan]]
    sim[[Simulate]]
    a[[Acquire]]
    e[[Explore / Analyze]]
    s[[Share]]

    p --> sim --> a --> e --> s
```

- **Plan** and sketch endpoint
- **Simulate** and consider data
- **Acquire** and prepare data
- **Explore** and understand data
- **Share** findings

## {{< bi 1-circle >}} Plan and Sketch

:::: {.columns}
::: {.column width="60%"}
- deliberate, reasoned decisions
- purposeful adjustments
- even 10 minutes of planning is valuable
:::

::: {.column width="40%"}
![Think of Alice's conversation with the Cheshire Cat üò∏. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.](../media/cheshire-cat.png)
:::
::::


::: {.notes}
**Planning and sketching an endpoint** is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice's conversation with the Cheshire Cat in Alice's Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn't require extensive time‚Äîoften just ten minutes with paper and pen can provide significant value.
:::

## {{< bi 2-circle >}} Simulate Data

- Forces detailed thinking
- Clarifies expected data structure and distributions.
- Helps with cleaning and preparation
- Identifies potential issues beforehand.
- Provides clear testing framework
- Ensures data meets expectations.
- ‚ÄúAlmost free‚Äù with modern computing
- Provides ‚Äúan intimate feeling for the situation‚Äù [@hamming1996]

::: {.notes}
**Simulating data** is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides ‚Äúan intimate feeling for the situation‚Äù and helps build confidence in our analytical tools.
:::


## {{< bi 3-circle >}} Acquire and Prepare

- Often overlooked but crucial stage
- Many difficult decisions required: data sources, formats, permissions.
- Can significantly affect statistical results [@huntington2021influence]
- Common challenges: **quantity** (too little or too much data) and **quality**

::: {.notes}
**Acquiring and preparing** the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed‚Äîeither by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked.
:::


## {{< bi 4-circle >}} Explore and Understand

- Begin with descriptive statistics
- Move to statistical models
- Remember: Models are tools, not truth
- Models reflect:
  - Earlier decisions
  - Data acquisition choices
  - Cleaning procedures

::: {.notes}
In the fourth step, we **explore and understand** the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It's important to remember that statistical models are tools‚Äînot absolute truths‚Äîand they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow.
:::


## {{< bi 5-circle >}} Share Findings

- High-fidelity communication essential
- Document all decisions
- Build credibility through transparency

Include:

- What was done
- Why it was done
- What was found
- Weaknesses of the approach

::: {.notes}
The final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it's essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process‚Äîfrom data acquisition to analysis‚Äîbuilds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice.
:::


## The Foundation

<br>

:::: {.columns}
::: {.column width="35%"}
<br>
Communication<br>
Reproducibility<br>
Ethics<br>
Questions<br>
Measurement<br>
Data Collection<br>
Data Cleaning<br>
Exploratory Data Analysis<br>
Modeling<br>
Scaling<br>
:::

::: {.column width="10%"}
:::

::: {.column width="55%"}
![Essential foundation for the data storytelling workflow.](../media/tswd-figures/iceberg.png){width=60%}
:::
::::

## {{< bi 1-circle >}} Communication (Most Important)

> "Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly." [@alexander2023telling]

> "One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it." [@alexander2023telling]

- Write in plain language
- Use tables, graphs, and models effectively
- Focus on the audience's perspective

## {{< bi 2-circle >}} Reproducibility

*Everything must be independently repeatable.* 

Requirements:

- Open access to code
- Data availability or simulation
- Automated testing
- Clear documentation
- Aim for autonomous end-to-end reproducibility

## {{< bi 3-circle >}} Ethics

> "This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?" [@alexander2023telling]

- Consider the full context of the dataset [@datafeminism2020]
- Acknowledge the social, cultural, and political forces [@crawford]
- Use data ethically with concern for impact and equity

## {{< bi 4-circle >}} Questions

- Questions evolve through understanding
- Challenge of operationalizing variables
- Curiosity is essential, drives deeper exploration
- Value of "hybrid" knowledge that combines multiple disciplines
- Comfort with asking "dumb" questions

::: {.notes}
Curiosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This **iterative questioning** contrasts with the traditional **Popperian approach** of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables. 

Strategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a ‚Äúhybrid‚Äù‚Äîcultivating knowledge across various disciplines‚Äîand being comfortable with asking seemingly simple or ‚Äúdumb‚Äù questions are particularly valuable in enhancing our understanding and fostering meaningful insights.
:::


## {{< bi 5-circle >}} Measurement

> "The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect." [@alexander2023telling]

:::: {.columns}
::: {.column width="60%"}
Measuring even simple things is challenging

Example: Measuring height

- Shoes on or off?
- Time of day affects height.
- Different tools yield different results.

More complex measurements are even harder. How do we measure happiness or pain?

- Measurement requires decisions and is not value-free
- Context and purpose guide all measurement choices
:::

::: {.column width="40%"}
![Picasso's dog and the challenges of **reduction**.](../media/tswd-figures/lump.png)
:::
::::

::: {.notes}
Measurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person's height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso's minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we're measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives.
:::


## {{< bi 6-circle >}} & {{< bi 7-circle >}} Data Collection & Cleaning

> "Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them." [@alexander2023telling]

- **Collection determines possibilities**
  - What and how we measure matters.
- **Cleaning requires many decisions**
  - Example: Survey responses on gender
  - Options: "man", "woman", "prefer not to say", "other"
  - Handling "prefer not to say" and open-text responses.
- **Document every step**
  - Ensures transparency and reproducibility.
- **Consider implications of choices**
  - Ethical considerations and representation.

::: {.notes}
Data cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like ‚Äúman,‚Äù ‚Äúwoman,‚Äù ‚Äúprefer not to say,‚Äù and ‚Äúother‚Äù (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude ‚Äúprefer not to say‚Äù responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents' specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it's vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them.
:::


## {{< bi 8-circle >}}+ EDA, Modeling, & Scaling

### Exploratory Data Analysis (EDA)
- Iterative process
- Never truly complete
- Shapes understanding

### Modeling

- Tool for understanding
- Not a recipe to follow
- Just one representation of reality
- Statistical significance $\neq$ scientific significance
- Statistical models help us explore the shape of the data; are like **echolocation**

### Scaling

- **Using programming languages like R and Python**
  - Handle large datasets efficiently
  - Automate repetitive tasks
  - Share work widely and quickly
- **Outputs can reach many people easily**
- **APIs can make analyses accessible in real-time**

::: {.notes}
**Exploratory Data Analysis (EDA)** is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there's always more to explore. Although it's challenging to delineate where EDA ends and formal statistical modeling begins‚Äîsince our beliefs and understanding evolve continuously‚ÄîEDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it's crucial that all steps taken during EDA are recorded and shared.

**Statistical modeling** builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it's a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference‚Äîthe process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.

**Scaling our work** becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it's essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly‚Äîwhether it's one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility.
:::


## How Do Our Worlds Become Data?

> "There is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality." [@hamming1996, 177]

## How Do Our Worlds Become Data?

> To a certain extent we are wasting our time. We have a perfect model of the world---it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, **we must simplify things to that which is plausibly measurable, and it is that which we define as data**. Our data are a simplification of the messy, complex world from which they were derived. 
><br><br>
> There are different approximations of "plausibly measurable". Hence, **datasets are always the result of choices**. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. [@alexander2023telling]

## How Do Our Worlds Become Data?

<br><br>
**Through skillful<br>[reduction üë®‚Äçüç≥]{.huge}**

::: {.notes}
Just as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data‚Äîplausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.

As we employ statistical models to explore and understand these datasets, it's crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment‚Äîmuch like a chef monitoring a reduction to achieve the desired consistency without overcooking‚Äîto determine when simplification is appropriate and when it risks losing critical information.

Measurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors‚Äîfrom selecting what to measure to deciding on the methods and accuracy required. Advances in instruments‚Äîfrom telescopes in astronomy to real-time internet data collection‚Äîhave expanded our ability to gather data, much like new culinary techniques enhance a chef's ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our ‚Äúreduction‚Äù process may have altered or omitted important aspects of reality.
:::

## What is Data Science?

> "Data science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict." [@alexander2023telling]

Key Principles

- Data are generated, and must be gathered, cleaned, and prepared
- These decisions matter
- The process will be difficult
- Develop resilience and intrinsic motivation

## The Power of Multiple Perspectives

> "The strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past." [@alexander2023telling]

- Data science is multi-disciplinary
- Combines statistics, software engineering, subject-matter expertise, and more.
- Diversity enhances understanding
- Different perspectives lead to better questions and solutions.
- Collaboration is key
- Respect and integrate insights from various fields.

## Embracing the Challenge

Our world is messy, and so are our data. Telling stories with data is difficult but rewarding.

- Develop resilience and intrinsic motivation
- Accept that failure is part of the process.
- Consider possibilities and probabilities
- Learn to make trade-offs.
- No perfect analysis exists
- Aim for transparency and continuous improvement.

> "Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world." [@alexander2023telling]

## Key Takeaways

- Data storytelling bridges analysis and understanding
- Effective communication is paramount
- Ethics and reproducibility are foundational
- Ask meaningful questions and measure thoughtfully and transparently
- Data collection and cleaning shape your analysis
- Embrace the iterative nature of exploration and modeling
- Leverage technology to scale and share your work
- Be mindful of the limitations of your data

