[
  {
    "objectID": "lectures/lecture-09-content.html",
    "href": "lectures/lecture-09-content.html",
    "title": "Remember…",
    "section": "",
    "text": "Graphs are an essential component of data storytelling. They help us recognize patterns, understand distributions, and communicate findings effectively. This notebook introduces best practices for graphing in R using ggplot2 and demonstrates how summary statistics can sometimes be misleading if we do not visualize our data."
  },
  {
    "objectID": "lectures/lecture-09-content.html#learning-objectives",
    "href": "lectures/lecture-09-content.html#learning-objectives",
    "title": "Remember…",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will: - Understand the importance of graphing raw data before relying on summary statistics. - Learn to use ggplot2 for scatterplots, bar charts, line plots, and faceted plots. - Explore different themes, color palettes, and aesthetic modifications. - Apply these techniques to real-world datasets."
  },
  {
    "objectID": "lectures/lecture-09-content.html#setup-loading-required-packages",
    "href": "lectures/lecture-09-content.html#setup-loading-required-packages",
    "title": "Remember…",
    "section": "Setup: Loading Required Packages",
    "text": "Setup: Loading Required Packages\n\n# Load necessary libraries\nlibrary(tidyverse) # Core tidyverse package for data wrangling & visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(datasauRus) # Fun dataset illustrating the importance of visualization\nlibrary(ggplot2) # Graphing package\nlibrary(janitor) # Cleaning column names\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(WDI) # Accessing World Bank economic indicators\nlibrary(carData) # British Election Panel Study dataset\nlibrary(patchwork) # Combining multiple plots\nlibrary(tidygeocoder) # Geocoding support\nlibrary(tinytable) # Nice formatted tables\n\n# Set theme for all plots\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "lectures/lecture-09-content.html#why-graphing-your-data-is-important",
    "href": "lectures/lecture-09-content.html#why-graphing-your-data-is-important",
    "title": "Remember…",
    "section": "Why Graphing Your Data is Important",
    "text": "Why Graphing Your Data is Important\n\nExample 1: The Datasaurus Dozen\nThe datasaurus_dozen dataset illustrates why we should always plot our data instead of relying solely on summary statistics.\n\n# Display the dataset\ndatasaurus_dozen\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# ℹ 1,836 more rows\n\n\nEach subset of this dataset has the same mean and standard deviation for x and y, yet their visual patterns are completely different.\n\nComputing Summary Statistics\nThis code below is a pipeline that processes the datasaurus_dozen dataset to compute and display summary statistics (mean and standard deviation) for four selected datasets: “dino”, “star”, “away”, and “bullseye”. The pipeline begins by using filter(dataset %in% c(“dino”, “star”, “away”, “bullseye”)), which subsets the data to only include these four specific datasets. The summarise() function then calculates the mean and standard deviation for both the x and y variables, applying the across() function to compute these statistics for each dataset separately. The .by = dataset argument ensures that the summary statistics are grouped by dataset, so each subset receives its own computed values.\nAfter computing the summary statistics, the code formats the output into a visually appealing table. The tt() function (from the tinytable package) is used to create a neatly formatted table, and style_tt(j = 2:5, align = “r”) aligns the numeric columns (x mean, x sd, y mean, y sd) to the right for better readability. The format_tt(digits = 1, num_fmt = “decimal”) function ensures that numerical values are displayed with one decimal place. Finally, setNames(c(“Dataset”, “x mean”, “x sd”, “y mean”, “y sd”)) renames the columns to more descriptive labels for clarity. This pipeline efficiently extracts, summarizes, and presents key statistical insights from the datasaurus_dozen dataset while emphasizing the importance of looking beyond summary statistics to understand data distributions visually.\n\ndatasaurus_dozen |&gt;\n    filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n    summarise(\n        across(c(x, y), list(mean = mean, sd = sd)),\n        .by = dataset\n    ) |&gt;\n    tt() |&gt;\n    style_tt(j = 2:5, align = \"r\") |&gt;\n    format_tt(digits = 1, num_fmt = \"decimal\") |&gt;\n    setNames(c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Dataset\n                x mean\n                x sd\n                y mean\n                y sd\n              \n        \n        \n        \n                \n                  dino\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  away\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  star\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n                \n                  bullseye\n                  54.3\n                  16.8\n                  47.8\n                  26.9\n                \n        \n      \n    \n\n\n\n👉 Key Takeaway: These datasets appear identical in summary statistics, but let’s plot them.\nRecall that the mean, or average, is a measure of central tendency that represents the typical value in a dataset. It is calculated by adding up all the values and dividing by the total number of values. The mean gives us a sense of where most of the data points are centered. The standard deviation measures how spread out the values are from the mean. If the standard deviation is small, most values are close to the mean; if it’s large, the values are more spread out.\nIn short:\n\nlow standard deviation = data points are close together\nhigh standard deviation = data points are more spread out\n\nThese concepts help us understand how typical or how varied our data is.\n\n\nVisualizing the Datasaurus Dozen\n\n# Plot the datasets\ndatasaurus_dozen |&gt;\n    filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n    ggplot(aes(x = x, y = y, colour = dataset)) +\n    geom_point() +\n    facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n    labs(color = \"Dataset\")\n\n\n\n\n\n\n\n\n👉 Observation: Despite having identical summary statistics, each dataset has a distinct shape!\n\n\n\nExample 2: Anscombe’s Quartet\nFrank Anscombe developed Anscombe’s Quartet to highlight the same issue.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n\n\nThis dataset contains four sets of (x, y) values that share identical means, variances, and regression lines.\n\nTidying the Data\nWe use pivot_longer() to convert it into tidy format.\n\ntidy_anscombe &lt;- anscombe |&gt; pivot_longer(\n    everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n)\ntidy_anscombe\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n\nThis code reshapes the anscombe dataset into a tidy format using the pivot_longer() function. The tidy format (or tidy data) is a structured way of organizing data where each row represents an observation, each column represents a variable, and each cell contains a single value. This format, introduced by Hadley Wickham, makes data easier to manipulate, visualize, and analyze using tools like ggplot2 and dplyr. For example, in a non-tidy format, you might have separate columns for x1, y1, x2, y2, etc. (like in Anscombe’s Quartet). In a tidy format, you would restructure the data so that there are only three columns: set (indicating the dataset), x, and y, with each row representing one observation. Tidy data is particularly useful because it works seamlessly with the tidyverse, allowing for easier grouping, filtering, summarizing, and plotting.\nThe everything() argument ensures that all columns in the dataset are transformed. The names_to = c(“.value”, “set”) argument tells pivot_longer() to split the original column names into two parts: one representing the variable (x or y) and the other representing the dataset number (1, 2, 3, or 4). The names_pattern = “(.)(.)” uses regular expressions to separate column names based on their structure (e.g., x1, y1 → x, y for dataset 1). As a result, the tidy_anscombe dataset now has three columns: set (identifying the dataset number), x (the independent variable), and y (the dependent variable). This transformation makes the data more structured and easier to work with, particularly for grouped analysis and visualization in ggplot2.\n\n\nComputing Summary Statistics\n\ntidy_anscombe |&gt;\n    summarise(\n        across(c(x, y), list(mean = mean, sd = sd)),\n        .by = set\n    ) |&gt;\n    tt() |&gt;\n    style_tt(j = 2:5, align = \"r\") |&gt;\n    format_tt(digits = 1, num_fmt = \"decimal\") |&gt;\n    setNames(c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Dataset\n                x mean\n                x sd\n                y mean\n                y sd\n              \n        \n        \n        \n                \n                  1\n                  9\n                  3.3\n                  7.5\n                  2\n                \n                \n                  2\n                  9\n                  3.3\n                  7.5\n                  2\n                \n                \n                  3\n                  9\n                  3.3\n                  7.5\n                  2\n                \n                \n                  4\n                  9\n                  3.3\n                  7.5\n                  2\n                \n        \n      \n    \n\n\n\n\n\nVisualizing Anscombe’s Quartet\n\ntidy_anscombe |&gt;\n    ggplot(aes(x = x, y = y, colour = set)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE) +\n    facet_wrap(vars(set), nrow = 2, ncol = 2) +\n    labs(colour = \"Dataset\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n👉 Insight: Again, summary statistics don’t tell the full story!"
  },
  {
    "objectID": "lectures/lecture-09-content.html#bar-charts-comparing-categorical-variables",
    "href": "lectures/lecture-09-content.html#bar-charts-comparing-categorical-variables",
    "title": "Remember…",
    "section": "Bar Charts: Comparing Categorical Variables",
    "text": "Bar Charts: Comparing Categorical Variables\nWe now explore bar charts using the British Election Panel Study.\n\nLoading and Cleaning the Data\n\nbeps &lt;-\n    BEPS |&gt;\n    as_tibble() |&gt;\n    clean_names() |&gt;\n    select(age, vote, gender, political_knowledge)\n\n\n\nCreating Age Groups\n\nbeps &lt;- beps |&gt;\n    mutate(\n        age_group = case_when(\n            age &lt; 35 ~ \"&lt;35\",\n            age &lt; 50 ~ \"35-49\",\n            age &lt; 65 ~ \"50-64\",\n            age &lt; 80 ~ \"65-79\",\n            age &lt; 100 ~ \"80-99\"\n        ),\n        age_group = factor(age_group, levels = c(\"&lt;35\", \"35-49\", \"50-64\", \"65-79\", \"80-99\"))\n    )\n\n\n\nPlotting the Distribution of Age Groups\n\nbeps |&gt; ggplot(aes(x = age_group)) +\n    geom_bar() +\n    labs(x = \"Age group\", y = \"Number of respondents\")"
  },
  {
    "objectID": "lectures/lecture-09-content.html#scatterplots-exploring-relationships-between-variables",
    "href": "lectures/lecture-09-content.html#scatterplots-exploring-relationships-between-variables",
    "title": "Remember…",
    "section": "Scatterplots: Exploring Relationships Between Variables",
    "text": "Scatterplots: Exploring Relationships Between Variables\nUsing World Bank Data, we analyze GDP growth and inflation.\n\nDownloading the Data\n\nworld_bank_data &lt;- WDI(\n    indicator = c(\"FP.CPI.TOTL.ZG\", \"NY.GDP.MKTP.KD.ZG\"),\n    country = c(\"AU\", \"ET\", \"IN\", \"US\")\n) |&gt;\n    rename(inflation = FP.CPI.TOTL.ZG, gdp_growth = NY.GDP.MKTP.KD.ZG)\n\n\n\nPlotting GDP Growth vs. Inflation\n\nworld_bank_data |&gt;\n    ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n    geom_point() +\n    labs(x = \"GDP Growth\", y = \"Inflation\", color = \"Country\")\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "lectures/lecture-09-content.html#line-plots-time-series-data",
    "href": "lectures/lecture-09-content.html#line-plots-time-series-data",
    "title": "Remember…",
    "section": "Line Plots: Time-Series Data",
    "text": "Line Plots: Time-Series Data\nLet’s analyze US GDP growth over time.\n\nworld_bank_data |&gt;\n    filter(country == \"United States\") |&gt;\n    ggplot(aes(x = year, y = gdp_growth)) +\n    geom_line() +\n    labs(x = \"Year\", y = \"GDP Growth\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Quantitative Research Methods",
    "section": "",
    "text": "Course Instructor John McLevey (he/him) Professor, Department of Sociology Memorial University\n\n  mclevey@mun.ca Note: I do not check or respond to email in the evenings or on weekends.\n\n\nGraduate Assistant Felix Morrow, PhD Student Department of Sociology, Memorial University fpcmorrow@mun.ca\n\nWhere is class? CP-2003 (Chemistry-Physics, Computer Lab) When is class? Tuesdays & Thursdays, 1:30 - 2:50 pm Office Hours: A4054, Tuesdays & Thursdays, 3:00 - 4:00 pm\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)\n\n\n\n\n👋 Hello!\n\n\n\n\n\n\nThis course is built around Rohan Alexander’s (2023) Telling Stories with Data.\n\n\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s (2023) Telling Stories with Data, you will learn how to:\n\nplan an analysis and sketch your data and endpoint\nsimulate some data to “force you into the details”\nacquire, assess, and prepare empirical data for analysis\nexplore and analyze data by creating visualizations and fitting models\nshare the results of your work with the world!\n\nYou will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear and generalized linear models. Reproducibility and research ethics are considered throughout the workflow, and the entire course.\n\n\nAbout the Instructor\nJohn McLevey (he/him) Pronounced like mic-Leave-ee\n\n\nLand Acknowledgement\nWe acknowledge that the lands on which Memorial University’s campuses are situated are in the traditional territories of diverse Indigenous groups, and we acknowledge with respect the diverse histories and cultures of the Beothuk, Mi’kmaq, Innu, and Inuit of this province.\n\n\n\n\n\nReferences\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC."
  },
  {
    "objectID": "lectures/lecture-04-notes.html",
    "href": "lectures/lecture-04-notes.html",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "",
    "text": "Required:   Ch 2\nRecommended:   Ch 1\n\n\n\n\nIn class work in RStudio! See notes below."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#reading-assignment",
    "href": "lectures/lecture-04-notes.html#reading-assignment",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "",
    "text": "Required:   Ch 2\nRecommended:   Ch 1"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#lecture-slides",
    "href": "lectures/lecture-04-notes.html#lecture-slides",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "",
    "text": "In class work in RStudio! See notes below."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#import-libraries",
    "href": "lectures/lecture-04-notes.html#import-libraries",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.1 Import Libraries",
    "text": "1.1 Import Libraries\nYou might see a lot of text print to the console when you import these libraries. You can ignore that for now!\n\nlibrary(\"janitor\") # For cleaning and formatting column names and data.\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(\"knitr\") # For creating tables and reports.\nlibrary(\"lubridate\") # For working with dates and times.\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(\"opendatatoronto\") # For accessing Toronto's open data directly.\nlibrary(\"tidyverse\") # A collection of R packages for data manipulation, visualization, and more.\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.4     ✔ readr   2.1.5\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ ggplot2 3.5.1     ✔ tibble  3.2.1\n✔ purrr   1.0.2     ✔ tidyr   1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"here\") # For managing file paths in a project-oriented workflow.\n\nhere() starts at /Users/johnmclevey/SOCI3040\n\n\nThe libraries listed here are essential for managing the workflow. Each library serves a specific purpose:\n\njanitor: Simplifies data cleaning tasks, such as renaming columns or identifying missing values.\nknitr: Provides functionality to integrate R code into reports and create professional tables.\nlubridate: Makes working with dates easier, such as extracting months or years from a date column.\nopendatatoronto: Offers tools to download and work with datasets provided by the City of Toronto.\ntidyverse: A suite of tools for data science, including dplyr for data manipulation and ggplot2 for visualization.\nhere: Ensures consistent file paths regardless of the working directory."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#plan",
    "href": "lectures/lecture-04-notes.html#plan",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.2 Plan",
    "text": "1.2 Plan\nThe dataset we are interested in needs to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is shown in Figure Figure 1.\n\n\n\n\n\n\nFigure 1: Quick sketch of a dataset\n\n\n\nWe aim to create a table summarizing the monthly average number of beds occupied each night. A sketch of such a table is shown in Figure Figure 2.\n\n\n\n\n\n\nFigure 2: Quick sketch of a table\n\n\n\nThese sketches provide a conceptual understanding of the expected output and guide the workflow. First, we simulate data to refine our understanding of the data-generating process."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate",
    "href": "lectures/lecture-04-notes.html#simulate",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.3 Simulate",
    "text": "1.3 Simulate\nSimulation is a crucial step for understanding the problem before analyzing real data. It allows us to:\n\nDefine assumptions about the data.\nCreate a test dataset that mimics the real dataset’s structure.\n\nHere, we simulate a dataset representing the daily occupancy of three shelters over one year (2021):\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            )\n    )\n\nsimulated_occupancy_data\n\n# A tibble: 1,095 × 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ℹ 1,085 more rows\n\n\n\n1.3.1 Code Breakdown:\n\nset.seed(853): Ensures reproducibility of random numbers.\ndate column:\n\nas.Date(\"2021-01-01\"): Creates the starting date (January 1, 2021).\n+ c(0:364): Adds consecutive days to generate a sequence for the entire year.\nrep(..., times = 3): Repeats the year-long sequence for three shelters.\n\nshelter column:\n\nCategorical variable indicating which shelter the data belongs to.\nrep(..., times = 365): Repeats the shelter name for each day of the year.\n\nnumber_occupied column:\n\nSimulated using the Poisson distribution (rpois).\nlambda = 30: Assumes an average of 30 beds occupied per shelter per day.\n\n\nThe simulated dataset has three columns:\n\ndate: The date of observation.\nshelter: The shelter’s name.\nnumber_occupied: The number of beds occupied on that date."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#acquire",
    "href": "lectures/lecture-04-notes.html#acquire",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.4 Acquire",
    "text": "1.4 Acquire\nThe next step is to download and process the real dataset from Toronto’s Open Data portal.\n\ntoronto_shelters &lt;-\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    filter(name == \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)\n\n\n1.4.1 Code Breakdown:\n\nlist_package_resources():\n\nRetrieves metadata for datasets in the specified package.\nThe package ID is specific to the Toronto shelter data.\n\nfilter():\n\nExtracts the 2021 dataset by matching the dataset name.\n\nget_resource():\n\nDownloads the selected dataset.\n\nwrite_csv():\n\nSaves the dataset locally for future use.\n\n\nNext, we clean the dataset:\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 × 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME        SHELTER_ID\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n1     1 21-01-01                    24 COSTI Immigrant Services         40\n2     2 21-01-01                    24 COSTI Immigrant Services         40\n3     3 21-01-01                    24 COSTI Immigrant Services         40\n4     4 21-01-01                    24 COSTI Immigrant Services         40\n5     5 21-01-01                    24 COSTI Immigrant Services         40\n6     6 21-01-01                    24 COSTI Immigrant Services         40\n# ℹ 27 more variables: SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;, LOCATION_POSTAL_CODE &lt;chr&gt;,\n#   LOCATION_CITY &lt;chr&gt;, LOCATION_PROVINCE &lt;chr&gt;, PROGRAM_ID &lt;dbl&gt;,\n#   PROGRAM_NAME &lt;chr&gt;, SECTOR &lt;chr&gt;, PROGRAM_MODEL &lt;chr&gt;,\n#   OVERNIGHT_SERVICE_TYPE &lt;chr&gt;, PROGRAM_AREA &lt;chr&gt;, SERVICE_USER_COUNT &lt;dbl&gt;,\n#   CAPACITY_TYPE &lt;chr&gt;, CAPACITY_ACTUAL_BED &lt;dbl&gt;, CAPACITY_FUNDING_BED &lt;dbl&gt;,\n#   OCCUPIED_BEDS &lt;dbl&gt;, UNOCCUPIED_BEDS &lt;dbl&gt;, UNAVAILABLE_BEDS &lt;dbl&gt;, …\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\n\n1.4.2 Code Breakdown:\n\nread_csv(): Reads the downloaded CSV file.\nclean_names(): Converts column names to snake_case for easier handling.\nmutate(): Converts the occupancy_date column to a date format.\nselect(): Retains only the relevant columns (occupancy_date and occupied_beds)."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#exploreunderstand",
    "href": "lectures/lecture-04-notes.html#exploreunderstand",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.5 Explore/Understand",
    "text": "1.5 Explore/Understand\nThe cleaned dataset is now ready for exploration. We compute the monthly average number of occupied beds:\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\n#| label: tbl-homelessoccupancyd\n#| tbl-cap: \"Shelter usage in Toronto in 2021\"\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426\n\n\n\n\n\n\n1.5.1 Code Breakdown:\n\nmutate():\n\nAdds a new column, occupancy_month, extracted from the occupancy_date.\nUses month() from lubridate to get the month name.\n\ndrop_na(): Removes rows with missing values in occupied_beds.\nsummarise(): Groups data by month and calculates the mean.\nkable(): Creates a neatly formatted table.\n\nThe table provides insights into monthly shelter usage."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#share",
    "href": "lectures/lecture-04-notes.html#share",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.6 Share",
    "text": "1.6 Share\nThe findings are summarized in a brief report:\n\n\n\n\n\n\n“Toronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R as well as the tidyverse, janitor, opendatatoronto, lubridate, and knitr. We then made a table of the average number of occupied beds each night for each month.\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July. More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.”"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#share-1",
    "href": "lectures/lecture-04-notes.html#share-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.7 SHARE!",
    "text": "1.7 SHARE!\nYour turn…"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#the-firehose",
    "href": "lectures/lecture-03-slides.html#the-firehose",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.1 the firehose",
    "text": "1.1 the firehose\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s (2023) Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#the-firehose-1",
    "href": "lectures/lecture-03-slides.html#the-firehose-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.2 the firehose",
    "text": "1.2 the firehose\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by Barrett (2021)\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan Alexander (2023)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#import-libraries",
    "href": "lectures/lecture-03-slides.html#import-libraries",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.3 import libraries",
    "text": "1.3 import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#plan-1",
    "href": "lectures/lecture-03-slides.html#plan-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "2.1 plan",
    "text": "2.1 plan\n\n2.1.1 Australian Elections\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#plan-2",
    "href": "lectures/lecture-03-slides.html#plan-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "2.2 plan",
    "text": "2.2 plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure 1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a “division” in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-1",
    "href": "lectures/lecture-03-slides.html#simulate-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.1 simulate",
    "text": "3.1 simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-2",
    "href": "lectures/lecture-03-slides.html#simulate-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.2 simulate",
    "text": "3.2 simulate\n\nWe’ll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-3",
    "href": "lectures/lecture-03-slides.html#simulate-3",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.3 simulate",
    "text": "3.3 simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we’re creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named “Division”. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it’s used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options — “Liberal”, “Labor”, “National”, “Green”, and “Other” — into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the “Division” column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we’re picking “with replacement” (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn’t match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-4",
    "href": "lectures/lecture-03-slides.html#simulate-4",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.4 simulate",
    "text": "3.4 simulate\n🤘 We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Liberal \n 2        2 National\n 3        3 Other   \n 4        4 Green   \n 5        5 Labor   \n 6        6 National\n 7        7 National\n 8        8 Other   \n 9        9 Other   \n10       10 Other   \n# ℹ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-1",
    "href": "lectures/lecture-03-slides.html#acquire-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.1 acquire",
    "text": "4.1 acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-2",
    "href": "lectures/lecture-03-slides.html#acquire-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.2 acquire",
    "text": "4.2 acquire\n\nWe’ll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n✌️ R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief “Ode to the here package,” “here here,” which you can read… here."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-3",
    "href": "lectures/lecture-03-slides.html#acquire-3",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.3 acquire",
    "text": "4.3 acquire\n🤘 We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 × 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA…\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather… KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge… ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE…\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ℹ 141 more rows\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-4",
    "href": "lectures/lecture-03-slides.html#acquire-4",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.4 acquire",
    "text": "4.4 acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA…\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri… KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-5",
    "href": "lectures/lecture-03-slides.html#acquire-5",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.5 acquire",
    "text": "4.5 acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma… STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO…\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-6",
    "href": "lectures/lecture-03-slides.html#acquire-6",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.6 acquire",
    "text": "4.6 acquire\n\n“We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.” (Alexander 2023)\n\n\nLet’s clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-7",
    "href": "lectures/lecture-03-slides.html#acquire-7",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.7 acquire",
    "text": "4.7 acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let’s look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ℹ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-8",
    "href": "lectures/lecture-03-slides.html#acquire-8",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.8 acquire",
    "text": "4.8 acquire\n\n\n\n✌️ R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-9",
    "href": "lectures/lecture-03-slides.html#acquire-9",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.9 acquire",
    "text": "4.9 acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-10",
    "href": "lectures/lecture-03-slides.html#acquire-10",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.10 acquire",
    "text": "4.10 acquire\n\n\n\n\n✌️ R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet’s rename."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-11",
    "href": "lectures/lecture-03-slides.html#acquire-11",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.11 acquire",
    "text": "4.11 acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-12",
    "href": "lectures/lecture-03-slides.html#acquire-12",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.12 acquire",
    "text": "4.12 acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let’s simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-13",
    "href": "lectures/lecture-03-slides.html#acquire-13",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.13 acquire",
    "text": "4.13 acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-14",
    "href": "lectures/lecture-03-slides.html#acquire-14",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.14 acquire",
    "text": "4.14 acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! 😎"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#aus_elections_clean_path",
    "href": "lectures/lecture-03-slides.html#aus_elections_clean_path",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.15 acquire",
    "text": "4.15 acquire\n\nLet’s save the cleaned data so that we can start with it data in the next stage. We’ll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-1",
    "href": "lectures/lecture-03-slides.html#explore-understand-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.1 explore / understand",
    "text": "5.1 explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-2",
    "href": "lectures/lecture-03-slides.html#explore-understand-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.2 explore / understand",
    "text": "5.2 explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n✌️ R Tip\n\n\nI’m using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won’t work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-3",
    "href": "lectures/lecture-03-slides.html#explore-understand-3",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.3 explore / understand",
    "text": "5.3 explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n😎"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-4",
    "href": "lectures/lecture-03-slides.html#explore-understand-4",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.4 explore / understand",
    "text": "5.4 explore / understand\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-5",
    "href": "lectures/lecture-03-slides.html#explore-understand-5",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.5 explore / understand",
    "text": "5.5 explore / understand\n\n\n\n\n\n\nRemember, we’re trying to make something like this.\n\n\n\n\n\n\n✌️ R Tip\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer’s understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer’s eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-6",
    "href": "lectures/lecture-03-slides.html#explore-understand-6",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.6 explore / understand",
    "text": "5.6 explore / understand\n\nLet’s visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it’s cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-6-output",
    "href": "lectures/lecture-03-slides.html#explore-understand-6-output",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.6 explore / understand",
    "text": "5.6 explore / understand\n\n\n\n\n\n\n\nFigure 2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-7",
    "href": "lectures/lecture-03-slides.html#explore-understand-7",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.7 explore / understand",
    "text": "5.7 explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-7-output",
    "href": "lectures/lecture-03-slides.html#explore-understand-7-output",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.7 explore / understand",
    "text": "5.7 explore / understand\n\n\n\n\n\n\n\nFigure 3: Number of seats won, by political party, at the 2022 Australian Federal Election. 😎"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#section",
    "href": "lectures/lecture-03-slides.html#section",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.8 ",
    "text": "5.8 \ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure 4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#share-1",
    "href": "lectures/lecture-03-slides.html#share-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "6.1 share",
    "text": "6.1 share\nExample taken directly from Alexander (2023), here.\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure 3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#references",
    "href": "lectures/lecture-03-slides.html#references",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "6.2 References",
    "text": "6.2 References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nBarrett, Malcolm. 2021. Data Science as an Atomic Habit. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\nBronner, Laura. 2020. “Why Statistics Don’t Capture the Full Extent of the Systemic Bias in Policing.” FiveThirtyEight, June. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\nCardoso, Tom. 2020. “Bias behind bars: A Globe investigation finds a prison system stacked against Black and Indigenous inmates.” The Globe and Mail, October. https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "lectures/lecture-24-notes.html",
    "href": "lectures/lecture-24-notes.html",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required:   Ch 13\nRecommended:   Ch 6\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#reading-assignment",
    "href": "lectures/lecture-24-notes.html#reading-assignment",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required:   Ch 13\nRecommended:   Ch 6"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#lecture-slides",
    "href": "lectures/lecture-24-notes.html#lecture-slides",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-22-notes.html",
    "href": "lectures/lecture-22-notes.html",
    "title": "Linear Models",
    "section": "",
    "text": "Required:   Ch 12\nRecommended:   Geocentric Models\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#reading-assignment",
    "href": "lectures/lecture-22-notes.html#reading-assignment",
    "title": "Linear Models",
    "section": "",
    "text": "Required:   Ch 12\nRecommended:   Geocentric Models"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#lecture-slides",
    "href": "lectures/lecture-22-notes.html#lecture-slides",
    "title": "Linear Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-20-notes.html",
    "href": "lectures/lecture-20-notes.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:   Ch 11\nRecommended:   The Garden of Forking Data\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#reading-assignment",
    "href": "lectures/lecture-20-notes.html#reading-assignment",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:   Ch 11\nRecommended:   The Garden of Forking Data"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#lecture-slides",
    "href": "lectures/lecture-20-notes.html#lecture-slides",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-18-notes.html",
    "href": "lectures/lecture-18-notes.html",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:   Ch 9\nRecommended:   Statistical Golems\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#reading-assignment",
    "href": "lectures/lecture-18-notes.html#reading-assignment",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:   Ch 9\nRecommended:   Statistical Golems"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#lecture-slides",
    "href": "lectures/lecture-18-notes.html#lecture-slides",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-16-notes.html",
    "href": "lectures/lecture-16-notes.html",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required:   Ch 8\nRecommended:   Ch 8\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#reading-assignment",
    "href": "lectures/lecture-16-notes.html#reading-assignment",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required:   Ch 8\nRecommended:   Ch 8"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#lecture-slides",
    "href": "lectures/lecture-16-notes.html#lecture-slides",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-14-notes.html",
    "href": "lectures/lecture-14-notes.html",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required:   Ch 7\nRecommended:   Ch 7\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#reading-assignment",
    "href": "lectures/lecture-14-notes.html#reading-assignment",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required:   Ch 7\nRecommended:   Ch 7"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#lecture-slides",
    "href": "lectures/lecture-14-notes.html#lecture-slides",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-12-notes.html",
    "href": "lectures/lecture-12-notes.html",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required:   Ch 6\nRecommended:   Ch 5\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#reading-assignment",
    "href": "lectures/lecture-12-notes.html#reading-assignment",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required:   Ch 6\nRecommended:   Ch 5"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#lecture-slides",
    "href": "lectures/lecture-12-notes.html#lecture-slides",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-10-notes.html",
    "href": "lectures/lecture-10-notes.html",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required:   Ch 5\nRecommended:   Ch 4\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-10-notes.html#reading-assignment",
    "href": "lectures/lecture-10-notes.html#reading-assignment",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required:   Ch 5\nRecommended:   Ch 4"
  },
  {
    "objectID": "lectures/lecture-10-notes.html#lecture-slides",
    "href": "lectures/lecture-10-notes.html#lecture-slides",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-02-notes.html",
    "href": "lectures/lecture-02-notes.html",
    "title": "Telling Stories with Data + R & the Tidyverse",
    "section": "",
    "text": "Required:   Ch 1\nRecommended:   Appendix A\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#reading-assignment",
    "href": "lectures/lecture-02-notes.html#reading-assignment",
    "title": "Telling Stories with Data + R & the Tidyverse",
    "section": "",
    "text": "Required:   Ch 1\nRecommended:   Appendix A"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#lecture-slides",
    "href": "lectures/lecture-02-notes.html#lecture-slides",
    "title": "Telling Stories with Data + R & the Tidyverse",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-08-notes.html",
    "href": "lectures/lecture-08-notes.html",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required:   Ch 4\nRecommended:   Ch 3"
  },
  {
    "objectID": "lectures/lecture-08-notes.html#reading-assignment",
    "href": "lectures/lecture-08-notes.html#reading-assignment",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required:   Ch 4\nRecommended:   Ch 3"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#references",
    "href": "lectures/lecture-02-slides.html#references",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-01-content.html",
    "href": "lectures/lecture-01-content.html",
    "title": "Who are you?",
    "section": "",
    "text": "My name is [John, Professor McLevey, Dr. McLevey] (he/him).\nProfessor & Head of Sociology New to Memorial after 11 years at University of Waterloo"
  },
  {
    "objectID": "lectures/lecture-01-content.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-content.html#welcome-to-soci-3040",
    "title": "Who are you?",
    "section": "",
    "text": "My name is [John, Professor McLevey, Dr. McLevey] (he/him).\nProfessor & Head of Sociology New to Memorial after 11 years at University of Waterloo"
  },
  {
    "objectID": "lectures/lecture-01-content.html#agenda.",
    "href": "lectures/lecture-01-content.html#agenda.",
    "title": "Who are you?",
    "section": "agenda.",
    "text": "agenda.\n\nWho are you? Background? Expectations?\nWhat is this course about?\nWhat will we do? Where is everything?"
  },
  {
    "objectID": "lectures/lecture-01-content.html#section",
    "href": "lectures/lecture-01-content.html#section",
    "title": "Who are you?",
    "section": "",
    "text": "Who are you? Do you have any previous quant courses / experience? What are your expectations for this course?"
  },
  {
    "objectID": "lectures/lecture-01-content.html#calendar-description",
    "href": "lectures/lecture-01-content.html#calendar-description",
    "title": "Who are you?",
    "section": "3040 Calendar Description",
    "text": "3040 Calendar Description\n\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)"
  },
  {
    "objectID": "lectures/lecture-01-content.html#this-section-001",
    "href": "lectures/lecture-01-content.html#this-section-001",
    "title": "Who are you?",
    "section": "This Section (001)",
    "text": "This Section (001)\n\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data, you will learn how to:\n\nplan an analysis and sketch your data and endpoint simulate some data to “force you into the details” acquire, assess, and prepare empirical data for analysis explore and analyze data by creating visualizations and fitting models share the results of your work with the world!\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s"
  },
  {
    "objectID": "lectures/lecture-01-content.html#this-section-001-1",
    "href": "lectures/lecture-01-content.html#this-section-001-1",
    "title": "Who are you?",
    "section": "This Section (001)",
    "text": "This Section (001)\n\n“A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.” [@alexander2023telling]\n\n\nCore foundation of quantitative research methods Bridge between analysis and understanding Essential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-content.html#this-section-001-2",
    "href": "lectures/lecture-01-content.html#this-section-001-2",
    "title": "Who are you?",
    "section": "This Section (001)",
    "text": "This Section (001)\n\n\n\n\n\n\n\n You will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear and generalized linear models. Reproducibility and research ethics are considered throughout the workflow, and the entire course."
  },
  {
    "objectID": "lectures/lecture-01-content.html#common-concerns-key-questions",
    "href": "lectures/lecture-01-content.html#common-concerns-key-questions",
    "title": "Who are you?",
    "section": "Common Concerns & Key Questions",
    "text": "Common Concerns & Key Questions\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What’s missing or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data? Have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components",
    "href": "lectures/lecture-01-content.html#core-workflow-components",
    "title": "Who are you?",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\nPlan and Sketch\ndeliberate, reasoned decisions purposeful adjustments even 10 minutes of planning is valuable\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn’t require extensive time—often just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components-1",
    "href": "lectures/lecture-01-content.html#core-workflow-components-1",
    "title": "Who are you?",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\nSimulate Data\nForces detailed thinking Clarifies expected data structure and distributions. Helps with cleaning and preparation Identifies potential issues beforehand. Provides clear testing framework Ensures data meets expectations. “Almost free” with modern computing Provides “an intimate feeling for the situation” [@hamming1996]\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides “an intimate feeling for the situation” and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components-2",
    "href": "lectures/lecture-01-content.html#core-workflow-components-2",
    "title": "Who are you?",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\nAcquire and Prepare\nOften overlooked but crucial stage Many difficult decisions required: data sources, formats, permissions. Can significantly affect statistical results [@huntington2021influence] Common challenges: quantity (too little or too much data) and quality\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed—either by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components-3",
    "href": "lectures/lecture-01-content.html#core-workflow-components-3",
    "title": "Who are you?",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\nExplore and Understand\nBegin with descriptive statistics Move to statistical models\nRemember: Models are tools, not truth, and they reflect our previous decisions, data acquisition choices, and cleaning procedures.\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It’s important to remember that statistical models are tools—not absolute truths—and they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components-4",
    "href": "lectures/lecture-01-content.html#core-workflow-components-4",
    "title": "Who are you?",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\nShare Findings\nHigh-fidelity communication is essential Document all decisions Build credibility through transparency\n\nInclude:\nWhat was done Why it was done What was found Weaknesses of the approach\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it’s essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process—from data acquisition to analysis—builds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-content.html#quantitative-research-essentials-1",
    "href": "lectures/lecture-01-content.html#quantitative-research-essentials-1",
    "title": "Who are you?",
    "section": "Quantitative Research Essentials",
    "text": "Quantitative Research Essentials\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-content.html#communication-most-important",
    "href": "lectures/lecture-01-content.html#communication-most-important",
    "title": "Who are you?",
    "section": "Communication (Most Important)",
    "text": "Communication (Most Important)\n\n“Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly.” [@alexander2023telling]\n\n\n“One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.” [@alexander2023telling]\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience’s perspective"
  },
  {
    "objectID": "lectures/lecture-01-content.html#reproducibility",
    "href": "lectures/lecture-01-content.html#reproducibility",
    "title": "Who are you?",
    "section": "Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\n\nRequirements:\nOpen access to code Data availability or simulation Automated testing Clear documentation Aim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-content.html#ethics",
    "href": "lectures/lecture-01-content.html#ethics",
    "title": "Who are you?",
    "section": "Ethics",
    "text": "Ethics\n\n“This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?” [@alexander2023telling]\n\nConsider the full context of the dataset [@datafeminism2020] Acknowledge the social, cultural, and political forces [@crawford] Use data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-content.html#questions",
    "href": "lectures/lecture-01-content.html#questions",
    "title": "Who are you?",
    "section": "Questions",
    "text": "Questions\nQuestions evolve through understanding Challenge of operationalizing variables Curiosity is essential, drives deeper exploration Value of “hybrid” knowledge that combines multiple disciplines Comfort with asking “dumb” questions\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a “hybrid”—cultivating knowledge across various disciplines—and being comfortable with asking seemingly simple or “dumb” questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-content.html#measurement",
    "href": "lectures/lecture-01-content.html#measurement",
    "title": "Who are you?",
    "section": "Measurement",
    "text": "Measurement\n\n“The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.” [@alexander2023telling]\n\n\n\nMeasuring even simple things is challenging (e.g., measuring height: Shoes on or off? Time of day affects height. Different tools yield different results). More complex measurements are even harder. How do we measure happiness or pain?\n Measurement requires decisions and is not value-free. Context and purpose guide all measurement choices.\n\n\n\n\nPicasso’s dog and the challenges of reduction.\n\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person’s height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso’s minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we’re measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-content.html#data-collection-cleaning",
    "href": "lectures/lecture-01-content.html#data-collection-cleaning",
    "title": "Who are you?",
    "section": "Data Collection & Cleaning",
    "text": "Data Collection & Cleaning\n\n“Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.” [@alexander2023telling]\n\nCollection determines possibilities What and how we measure matters.\nCleaning requires many decisions E.g., Handling “prefer not to say” and open-text responses.\nDocument every step To ensure transparency and reproducibility.\nConsider implications of choices E.g., ethics, representation.\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Decisions such as whether to exclude “prefer not to say” responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents’ specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it’s vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-content.html#exploratory-data-analysis-eda",
    "href": "lectures/lecture-01-content.html#exploratory-data-analysis-eda",
    "title": "Who are you?",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nIterative process Never truly complete Shapes understanding\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there’s always more to explore. Although it’s challenging to delineate where EDA ends and formal statistical modeling begins—since our beliefs and understanding evolve continuously—EDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it’s crucial that all steps taken during EDA are recorded and shared."
  },
  {
    "objectID": "lectures/lecture-01-content.html#modeling",
    "href": "lectures/lecture-01-content.html#modeling",
    "title": "Who are you?",
    "section": "Modeling",
    "text": "Modeling\nTool for understanding Not a recipe to follow Just one representation of reality Statistical significance \\(\\neq\\) scientific significance Statistical models help us explore the shape of the data; are like echolocation\n\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it’s a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference—the process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality."
  },
  {
    "objectID": "lectures/lecture-01-content.html#scaling",
    "href": "lectures/lecture-01-content.html#scaling",
    "title": "Who are you?",
    "section": "Scaling",
    "text": "Scaling\nUsing programming languages like R and Python\nHandle large datasets efficiently Automate repetitive tasks Share work widely and quickly Outputs can reach many people easily APIs can make analyses accessible in real-time\n\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it’s essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly—whether it’s one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-1",
    "title": "Who are you?",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. [@alexander2023telling]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-2",
    "title": "Who are you?",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction 👨‍🍳\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data—plausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it’s crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment—much like a chef monitoring a reduction to achieve the desired consistency without overcooking—to determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors—from selecting what to measure to deciding on the methods and accuracy required. Advances in instruments—from telescopes in astronomy to real-time internet data collection—have expanded our ability to gather data, much like new culinary techniques enhance a chef’s ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our “reduction” process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-content.html#embracing-the-challenge-1",
    "href": "lectures/lecture-01-content.html#embracing-the-challenge-1",
    "title": "Who are you?",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\n\n“Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.” [@alexander2023telling]\n\nTelling good stories with data is difficult but rewarding.\nDevelop resilience and intrinsic motivation. Accept that failure is part of the process. Consider possibilities and probabilities. Learn to make trade-offs. No perfect analysis exists. Aim for transparency and continuous improvement."
  },
  {
    "objectID": "lectures/lecture-01-content.html#key-takeaways",
    "href": "lectures/lecture-01-content.html#key-takeaways",
    "title": "Who are you?",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-01-content.html#section-1",
    "href": "lectures/lecture-01-content.html#section-1",
    "title": "Who are you?",
    "section": "",
    "text": "Brightspace Course materials website: johnmclevey.com/SOCI3040/"
  },
  {
    "objectID": "lectures/lecture-01-content.html#next-class",
    "href": "lectures/lecture-01-content.html#next-class",
    "title": "Who are you?",
    "section": "Next class",
    "text": "Next class\n\nBefore class: Complete the assigned reading In class: Introduction to R and RStudio"
  },
  {
    "objectID": "lectures/lecture-05-notes.html",
    "href": "lectures/lecture-05-notes.html",
    "title": "Reproducible Workflows with R & RStudio",
    "section": "",
    "text": "Required:   Ch 3\nRecommended:   Ch 2\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#reading-assignment",
    "href": "lectures/lecture-05-notes.html#reading-assignment",
    "title": "Reproducible Workflows with R & RStudio",
    "section": "",
    "text": "Required:   Ch 3\nRecommended:   Ch 2"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#lecture-slides",
    "href": "lectures/lecture-05-notes.html#lecture-slides",
    "title": "Reproducible Workflows with R & RStudio",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-4.html",
    "href": "deliverables/submissions/assignment-4/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-2.html",
    "href": "deliverables/submissions/assignment-4/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-5.html",
    "href": "deliverables/submissions/assignment-3/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula.\nPraesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-3.html",
    "href": "deliverables/submissions/assignment-3/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula.\nPraesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-1.html",
    "href": "deliverables/submissions/assignment-3/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-4.html",
    "href": "deliverables/submissions/assignment-2/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-2.html",
    "href": "deliverables/submissions/assignment-2/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nVestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-5.html",
    "href": "deliverables/submissions/assignment-1/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nVestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-3.html",
    "href": "deliverables/submissions/assignment-1/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nPraesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\nProin sodales neque erat, varius cursus diam tincidunt sit amet. Etiam scelerisque fringilla nisl eu venenatis. Donec sem ipsum, scelerisque ac venenatis quis, hendrerit vel mauris. Praesent semper erat sit amet purus condimentum, sit amet auctor mi feugiat. In hac habitasse platea dictumst. Nunc ac mauris in massa feugiat bibendum id in dui. Praesent accumsan urna at lacinia aliquet. Proin ultricies eu est quis pellentesque. In vel lorem at nisl rhoncus cursus eu quis mi. In eu rutrum ante, quis placerat justo. Etiam euismod nibh nibh, sed elementum nunc imperdiet in. Praesent gravida nunc vel odio lacinia, at tempus nisl placerat. Aenean id ipsum sed est sagittis hendrerit non in tortor."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-1.html",
    "href": "deliverables/submissions/assignment-1/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi.\nAenean placerat luctus tortor vitae molestie. Nulla at aliquet nulla. Sed efficitur tellus orci, sed fringilla lectus laoreet eget. Vivamus maximus quam sit amet arcu dignissim, sed accumsan massa ullamcorper. Sed iaculis tincidunt feugiat. Nulla in est at nunc ultricies dictum ut vitae nunc. Aenean convallis vel diam at malesuada. Suspendisse arcu libero, vehicula tempus ultrices a, placerat sit amet tortor. Sed dictum id nulla commodo mattis. Aliquam mollis, nunc eu tristique faucibus, purus lacus tincidunt nulla, ac pretium lorem nunc ut enim. Curabitur eget mattis nisl, vitae sodales augue. Nam felis massa, bibendum sit amet nulla vel, vulputate rutrum lacus. Aenean convallis odio pharetra nulla mattis consequat."
  },
  {
    "objectID": "syllabus/policies.html#department-and-faculty-policies",
    "href": "syllabus/policies.html#department-and-faculty-policies",
    "title": " Policies",
    "section": "Department and Faculty Policies",
    "text": "Department and Faculty Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/policies.html#university-policies",
    "href": "syllabus/policies.html#university-policies",
    "title": " Policies",
    "section": "University Policies",
    "text": "University Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/conventions.html",
    "href": "syllabus/conventions.html",
    "title": " Conventions",
    "section": "",
    "text": "This page will develop as needed as the course progresses."
  },
  {
    "objectID": "syllabus/assessment.html",
    "href": "syllabus/assessment.html",
    "title": " Assessment",
    "section": "",
    "text": "There are four lab assignments (i.e., “Data Stories”) and a final exam in this course.\n\n\n\nDue (on or before)\nBy\nAssignment\nWeight\n\n\n\n\nJanuary 28, 2025\n11:59 pm\nData Stories 1\n10%\n\n\nFebruary 20, 2025\n11:59 pm\nData Stories 2\n15%\n\n\nMarch 20, 2025\n11:59 pm\nData Stories 3\n25%\n\n\nApril 8, 2025\n11:59 pm\nData Stories 4\n25%\n\n\nTBA\nTBA\nFinal Exam\n25%\n\n\n\n\n\n Data Stories 1\nAssignment information will be posted after the second class. Work will be done in and outside of class. \n\n\n Data Stories 2\nAssignment information will be posted after the second class. Work will be done in and outside of class.\n\n\n\n Data Stories 3\nAssignment information will be posted after the second class. Work will be done in and outside of class.\n\n\n\n Data Stories 4\nAssignment information will be posted after the second class. Work will be done in and outside of class.\n\n\n\nFinal Exam\nNot yet scheduled. Will be in the final exam block scheduled by the Registrars Office (RO).\n\nMultiple choice\nTrue / false\nShort answer questions\nSketch models and graphs\nInterpret models and graphs\nExplain code snippets at a high-level",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Assessment"
    ]
  },
  {
    "objectID": "lectures/lecture-25-slides.html#references",
    "href": "lectures/lecture-25-slides.html#references",
    "title": "Project Work",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#references",
    "href": "lectures/lecture-23-slides.html#references",
    "title": "Generalized Linear Models (GLMs)",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#references",
    "href": "lectures/lecture-21-slides.html#references",
    "title": "Linear Models",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#references",
    "href": "lectures/lecture-19-slides.html#references",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#references",
    "href": "lectures/lecture-17-slides.html#references",
    "title": "Cleaning, Preparing, and Testing",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#references",
    "href": "lectures/lecture-15-slides.html#references",
    "title": "Experiments and Surveys",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#references",
    "href": "lectures/lecture-13-slides.html#references",
    "title": "APIs, Scraping, and Parsing",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-11-slides.html#references",
    "href": "lectures/lecture-11-slides.html#references",
    "title": "Measurement, Censuses, and Sampling",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#references",
    "href": "lectures/lecture-09-slides.html#references",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#references",
    "href": "lectures/lecture-07-slides.html#references",
    "title": "Writing and Developing Research Questions",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#references",
    "href": "lectures/lecture-05-slides.html#references",
    "title": "Reproducible Workflows with R and RStudio",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "deliverables/assignment-5.html",
    "href": "deliverables/assignment-5.html",
    "title": " Final Exam",
    "section": "",
    "text": "Not yet scheduled!\nInformation coming soon…\n\nMultiple choice\nTrue / false\nShort answer\nSketch models and graphs\nInterpret models and graphs\nExplain code snippets at a high-level"
  },
  {
    "objectID": "deliverables/assignment-3.html",
    "href": "deliverables/assignment-3.html",
    "title": " Data Stories 3",
    "section": "",
    "text": "Assignment information will be finalized by the end of the week.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 3"
    ]
  },
  {
    "objectID": "deliverables/assignment-3.html#assignment-instructions",
    "href": "deliverables/assignment-3.html#assignment-instructions",
    "title": " Data Stories 3",
    "section": "",
    "text": "Assignment information will be finalized by the end of the week.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 3"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html",
    "href": "deliverables/assignment-1.html",
    "title": " Data Stories 1",
    "section": "",
    "text": "You may complete the assignment individually or in groups of 2-3. If working collaboratively, each member must submit their own assignment with the group’s names listed in the author metadata above.\nSteps to Complete the Assignment:\n\nCreate a free or student account ($5/month) with PositCloud.\nCreate a new workspace.\nIn the workspace, create a new project from a Github repository using this link: https://github.com/mclevey/3040-2025-assignment-1-posit.git.\nOpen assignment-1.qmd and update the author metadata at the top of the file.\nFollow the instructions in the notebook, executing the code cells as you go. Note that one of the instructions asks you to insert a citation to the assigned reading. Citation information is already entered into the refs.bib file.\n“Knit” your document to an HTML file. Make sure it executes without errors and preview the result.\nSave the HTML file and upload it to the Dropbox on Brightspace.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 1"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html#assignment-1-overview",
    "href": "deliverables/assignment-1.html#assignment-1-overview",
    "title": " Data Stories 1",
    "section": "",
    "text": "You may complete the assignment individually or in groups of 2-3. If working collaboratively, each member must submit their own assignment with the group’s names listed in the author metadata above.\nSteps to Complete the Assignment:\n\nCreate a free or student account ($5/month) with PositCloud.\nCreate a new workspace.\nIn the workspace, create a new project from a Github repository using this link: https://github.com/mclevey/3040-2025-assignment-1-posit.git.\nOpen assignment-1.qmd and update the author metadata at the top of the file.\nFollow the instructions in the notebook, executing the code cells as you go. Note that one of the instructions asks you to insert a citation to the assigned reading. Citation information is already entered into the refs.bib file.\n“Knit” your document to an HTML file. Make sure it executes without errors and preview the result.\nSave the HTML file and upload it to the Dropbox on Brightspace.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 1"
    ]
  },
  {
    "objectID": "lectures/lecture-07-content.html",
    "href": "lectures/lecture-07-content.html",
    "title": "Work on Assignment 1 in Class",
    "section": "",
    "text": "Work on Assignment 1 in Class\nYou may complete the assignment individually or in groups of 2-3. If working collaboratively, each member must submit their own assignment with the group’s names listed in the author metadata above.\nSteps to Complete the Assignment:\n\nCreate a free or student account ($5/month) with PositCloud.\nCreate a new workspace.\nIn the workspace, create a new project from a Github repository using this link: https://github.com/mclevey/3040-2025-assignment-1-posit.git.\nOpen assignment-1.qmd and update the author metadata at the top of the file.\nFollow the instructions in the notebook, executing the code cells as you go. Note that one of the instructions asks you to insert a citation to the assigned reading. Citation information is already entered into the refs.bib file.\n“Knit” your document to an HTML file. Make sure it executes without errors and preview the result.\nSave the HTML file and upload it to the Dropbox on Brightspace."
  },
  {
    "objectID": "lectures/lecture-05-content.html",
    "href": "lectures/lecture-05-content.html",
    "title": "🔥 Quantitative Research Methods",
    "section": "",
    "text": "Sociology Student Society? Rep?\nHave made some decisions about pacing and assignments\n\npacing and delivery: more walkthroughs, pace seems OK so far?\nfirst assignment: basic use of RStudio, the workflow components, more writing, limited code, will not include Git or GitHub\n\n\nToday:\n\nR Projects\nQuarto – examples form chapter, more metadata\nLoad the data from last class"
  },
  {
    "objectID": "lectures/lecture-08-content.html",
    "href": "lectures/lecture-08-content.html",
    "title": "Work on Assignment 1 in Class",
    "section": "",
    "text": "Work on Assignment 1 in Class\nYou may complete the assignment individually or in groups of 2-3. If working collaboratively, each member must submit their own assignment with the group’s names listed in the author metadata above.\nSteps to Complete the Assignment:\n\nCreate a free or student account ($5/month) with PositCloud.\nCreate a new workspace.\nIn the workspace, create a new project from a Github repository using this link: https://github.com/mclevey/3040-2025-assignment-1-posit.git.\nOpen assignment-1.qmd and update the author metadata at the top of the file.\nFollow the instructions in the notebook, executing the code cells as you go. Note that one of the instructions asks you to insert a citation to the assigned reading. Citation information is already entered into the refs.bib file.\n“Knit” your document to an HTML file. Make sure it executes without errors and preview the result.\nSave the HTML file and upload it to the Dropbox on Brightspace."
  },
  {
    "objectID": "deliverables/assignment-2.html",
    "href": "deliverables/assignment-2.html",
    "title": " Data Stories 2",
    "section": "",
    "text": "Assignment information will be finalized by the end of the week.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 2"
    ]
  },
  {
    "objectID": "deliverables/assignment-2.html#assignment-instructions",
    "href": "deliverables/assignment-2.html#assignment-instructions",
    "title": " Data Stories 2",
    "section": "",
    "text": "Assignment information will be finalized by the end of the week.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 2"
    ]
  },
  {
    "objectID": "deliverables/assignment-4.html",
    "href": "deliverables/assignment-4.html",
    "title": " Data Stories 4",
    "section": "",
    "text": "Assignment information will be finalized by the end of the week.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories  4"
    ]
  },
  {
    "objectID": "deliverables/assignment-4.html#assignment-instructions",
    "href": "deliverables/assignment-4.html#assignment-instructions",
    "title": " Data Stories 4",
    "section": "",
    "text": "Assignment information will be finalized by the end of the week.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories  4"
    ]
  },
  {
    "objectID": "lectures/lecture-04-slides.html#references",
    "href": "lectures/lecture-04-slides.html#references",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-06-slides.html#references",
    "href": "lectures/lecture-06-slides.html#references",
    "title": "Reproducible Workflows with R and RStudio",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-08-slides.html#references",
    "href": "lectures/lecture-08-slides.html#references",
    "title": "Writing and Developing Research Questions",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-10-slides.html#references",
    "href": "lectures/lecture-10-slides.html#references",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#references",
    "href": "lectures/lecture-12-slides.html#references",
    "title": "Measurement, Censuses, and Sampling",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#references",
    "href": "lectures/lecture-14-slides.html#references",
    "title": "APIs, Scraping, and Parsing",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#references",
    "href": "lectures/lecture-16-slides.html#references",
    "title": "Experiments and Surveys",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#references",
    "href": "lectures/lecture-18-slides.html#references",
    "title": "Cleaning, Preparing, and Testing",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#references",
    "href": "lectures/lecture-20-slides.html#references",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#references",
    "href": "lectures/lecture-22-slides.html#references",
    "title": "Linear Models",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/lecture-24-slides.html#references",
    "href": "lectures/lecture-24-slides.html#references",
    "title": "Generalized Linear Models (GLMs)",
    "section": "1 References",
    "text": "1 References"
  },
  {
    "objectID": "lectures/list-lectures.html",
    "href": "lectures/list-lectures.html",
    "title": " Class & Lab Notes",
    "section": "",
    "text": "Introduction + Telling Stories with Data\n\n\n\nTuesday, January 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTelling Stories with Data + R & the Tidyverse\n\n\n\nThursday, January 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis Workflow – The Firehose\n\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis Workflow – The Firehose\n\n\n\nThursday, January 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Workflows with R & RStudio\n\n\n\nTuesday, January 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Workflows with R & RStudio\n\n\n\nThursday, January 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWriting and Developing Research Questions\n\n\n\nTuesday, January 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWriting and Developing Research Questions\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Graphs, Tables, & Maps\n\n\n\nTuesday, February 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Graphs, Tables, & Maps\n\n\n\nThursday, February 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement, Censuses, and Sampling\n\n\n\nTuesday, February 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement, Censuses, and Sampling\n\n\n\nThursday, February 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs, Scraping, and Parsing\n\n\n\nTuesday, February 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs, Scraping, and Parsing\n\n\n\nThursday, February 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments and Surveys\n\n\n\nTuesday, March 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments and Surveys\n\n\n\nThursday, March 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning, Preparing, and Testing\n\n\n\nTuesday, March 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning, Preparing, and Testing\n\n\n\nThursday, March 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nTuesday, March 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nThursday, March 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models\n\n\n\nTuesday, March 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models\n\n\n\nThursday, March 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models (GLMs)\n\n\n\nTuesday, April 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models (GLMs)\n\n\n\nThursday, April 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProject Work\n\n\n\nTuesday, April 8, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus/computing.html",
    "href": "syllabus/computing.html",
    "title": " Computing",
    "section": "",
    "text": "R and Quarto 😁 – No Experience Necessary  You will learn to use R (a free/open source programming language and environment for statistical computing and graphics) and Quarto (a free/open source publishing system for creating dynamic and reproducible manuscripts, reports, websites, and presentations) in this course. Note that I assume no prior experience with R or Quarto. Everything you need to know about both will be introduced in the course. While some prior programming experience is an asset, it is by no means necessary.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/schedule.html",
    "href": "syllabus/schedule.html",
    "title": " Course Schedule",
    "section": "",
    "text": "In the table below,\n\n refers to Alexander’s (2023) Telling Stories with Data (required)\n refers to Healy’s (2019) Data Visualization: A Practical Introduction (supplementary / optional)\n refers to McElreath’s (2023) Statistical Rethinking 2023 Lectures (supplementary / optional)\n\nEach file icon links directly to the assigned or supplementary materials.\n\n\n\n\n\n\n\n\n\nNo.\n\n\nClass Date\n\n\nClass & Lab Notes + Slides\n\n\n Required\n\n\n Recommended\n\n\nDue by 5:00 pm\n\n\n\n\n\n\n01\n\n\n1/7/25\n\n\nIntroduction + Telling Stories with Data\n\n\nBrowse this site\n\n\n \n\n\n \n\n\n\n\n02\n\n\n1/9/25\n\n\nTelling Stories with Data + R & the Tidyverse\n\n\n  Ch 1\n\n\n  Appendix A\n\n\n \n\n\n\n\n03\n\n\n1/14/25\n\n\nData Analysis Workflow – The Firehose\n\n\n  Ch 2\n\n\n  Ch 1\n\n\n \n\n\n\n\n04\n\n\n1/16/25\n\n\nData Analysis Workflow – The Firehose\n\n\n  Ch 2\n\n\n  Ch 1\n\n\n \n\n\n\n\n05\n\n\n1/21/25\n\n\nReproducible Workflows with R & RStudio\n\n\n  Ch 3\n\n\n  Ch 2\n\n\n \n\n\n\n\n06\n\n\n1/23/25\n\n\nReproducible Workflows with R & RStudio\n\n\n  Ch 3\n\n\n  Ch 2\n\n\n \n\n\n\n\n07\n\n\n1/28/25\n\n\nWriting and Developing Research Questions\n\n\n  Ch 4\n\n\n  Ch 3\n\n\n \n\n\n\n\n08\n\n\n1/30/25\n\n\nWriting and Developing Research Questions\n\n\n  Ch 4\n\n\n  Ch 3\n\n\nData Stories 1\n\n\n\n\n09\n\n\n2/4/25\n\n\nCreating Graphs, Tables, & Maps\n\n\n  Ch 5\n\n\n  Ch 4\n\n\n \n\n\n\n\n10\n\n\n2/6/25\n\n\nCreating Graphs, Tables, & Maps\n\n\n  Ch 5\n\n\n  Ch 4\n\n\n \n\n\n\n\n11\n\n\n2/11/25\n\n\nMeasurement, Censuses, and Sampling\n\n\n  Ch 6\n\n\n  Ch 5\n\n\n \n\n\n\n\n12\n\n\n2/13/25\n\n\nMeasurement, Censuses, and Sampling\n\n\n  Ch 6\n\n\n  Ch 5\n\n\n \n\n\n\n\n13\n\n\n2/18/25\n\n\nAPIs, Scraping, and Parsing\n\n\n  Ch 7\n\n\n  Ch 7\n\n\n \n\n\n\n\n14\n\n\n2/20/25\n\n\nAPIs, Scraping, and Parsing\n\n\n  Ch 7\n\n\n  Ch 7\n\n\nData Stories 2\n\n\n\n\n15\n\n\n3/4/25\n\n\nExperiments and Surveys\n\n\n  Ch 8\n\n\n  Ch 8\n\n\n \n\n\n\n\n16\n\n\n3/6/25\n\n\nExperiments and Surveys\n\n\n  Ch 8\n\n\n  Ch 8\n\n\n \n\n\n\n\n17\n\n\n3/11/25\n\n\nCleaning, Preparing, and Testing\n\n\n  Ch 9\n\n\n  Statistical Golems\n\n\n \n\n\n\n\n18\n\n\n3/13/25\n\n\nCleaning, Preparing, and Testing\n\n\n  Ch 9\n\n\n  Statistical Golems\n\n\n \n\n\n\n\n19\n\n\n3/18/25\n\n\nExploratory Data Analysis (EDA)\n\n\n  Ch 11\n\n\n  The Garden of Forking Data\n\n\n \n\n\n\n\n20\n\n\n3/20/25\n\n\nExploratory Data Analysis (EDA)\n\n\n  Ch 11\n\n\n  The Garden of Forking Data\n\n\nData Stories 3\n\n\n\n\n21\n\n\n3/25/25\n\n\nLinear Models\n\n\n  Ch 12\n\n\n  Geocentric Models\n\n\n \n\n\n\n\n22\n\n\n3/27/25\n\n\nLinear Models\n\n\n  Ch 12\n\n\n  Geocentric Models\n\n\n \n\n\n\n\n23\n\n\n4/1/25\n\n\nGeneralized Linear Models (GLMs)\n\n\n  Ch 13\n\n\n  Ch 6\n\n\n \n\n\n\n\n24\n\n\n4/3/25\n\n\nGeneralized Linear Models (GLMs)\n\n\n  Ch 13\n\n\n  Ch 6\n\n\n \n\n\n\n\n25\n\n\n4/8/25\n\n\nProject Work\n\n\n \n\n\n \n\n\nData Stories 4\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nReferences\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nHealy, Kieran. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nMcElreath, Richard. 2023. “Statistical Rethinking Lectures.” YouTube. https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Course Schedule"
    ]
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-2.html",
    "href": "deliverables/submissions/assignment-1/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nPraesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\nProin sodales neque erat, varius cursus diam tincidunt sit amet. Etiam scelerisque fringilla nisl eu venenatis. Donec sem ipsum, scelerisque ac venenatis quis, hendrerit vel mauris. Praesent semper erat sit amet purus condimentum, sit amet auctor mi feugiat. In hac habitasse platea dictumst. Nunc ac mauris in massa feugiat bibendum id in dui. Praesent accumsan urna at lacinia aliquet. Proin ultricies eu est quis pellentesque. In vel lorem at nisl rhoncus cursus eu quis mi. In eu rutrum ante, quis placerat justo. Etiam euismod nibh nibh, sed elementum nunc imperdiet in. Praesent gravida nunc vel odio lacinia, at tempus nisl placerat. Aenean id ipsum sed est sagittis hendrerit non in tortor."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-4.html",
    "href": "deliverables/submissions/assignment-1/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nPraesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\nProin sodales neque erat, varius cursus diam tincidunt sit amet. Etiam scelerisque fringilla nisl eu venenatis. Donec sem ipsum, scelerisque ac venenatis quis, hendrerit vel mauris. Praesent semper erat sit amet purus condimentum, sit amet auctor mi feugiat. In hac habitasse platea dictumst. Nunc ac mauris in massa feugiat bibendum id in dui. Praesent accumsan urna at lacinia aliquet. Proin ultricies eu est quis pellentesque. In vel lorem at nisl rhoncus cursus eu quis mi. In eu rutrum ante, quis placerat justo. Etiam euismod nibh nibh, sed elementum nunc imperdiet in. Praesent gravida nunc vel odio lacinia, at tempus nisl placerat. Aenean id ipsum sed est sagittis hendrerit non in tortor."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-1.html",
    "href": "deliverables/submissions/assignment-2/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nVestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-3.html",
    "href": "deliverables/submissions/assignment-2/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-5.html",
    "href": "deliverables/submissions/assignment-2/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-2.html",
    "href": "deliverables/submissions/assignment-3/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-4.html",
    "href": "deliverables/submissions/assignment-3/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula.\nPraesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-1.html",
    "href": "deliverables/submissions/assignment-4/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-3.html",
    "href": "deliverables/submissions/assignment-4/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-5.html",
    "href": "deliverables/submissions/assignment-4/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula."
  },
  {
    "objectID": "lectures/lecture-09-notes.html",
    "href": "lectures/lecture-09-notes.html",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required:   Ch 5\nRecommended:   Ch 4\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#reading-assignment",
    "href": "lectures/lecture-09-notes.html#reading-assignment",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required:   Ch 5\nRecommended:   Ch 4"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#lecture-slides",
    "href": "lectures/lecture-09-notes.html#lecture-slides",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-slides.html#welcome-to-soci-3040",
    "title": "Introduction + Telling Stories with Data",
    "section": "👋 Welcome to SOCI 3040!",
    "text": "👋 Welcome to SOCI 3040!\n\nMy name is [John, Professor McLevey, Dr. McLevey] (he/him).\nProfessor & Head of Sociology New to Memorial after 11 years at University of Waterloo"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#agenda.",
    "href": "lectures/lecture-01-slides.html#agenda.",
    "title": "Introduction + Telling Stories with Data",
    "section": "agenda.",
    "text": "agenda.\n\nWho are you? Background? Expectations?\nWhat is this course about?\nWhat will we do? Where is everything?"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#section",
    "href": "lectures/lecture-01-slides.html#section",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "Who are you? Do you have any previous quant courses / experience? What are your expectations for this course?"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#calendar-description",
    "href": "lectures/lecture-01-slides.html#calendar-description",
    "title": "Introduction + Telling Stories with Data",
    "section": "3040 Calendar Description",
    "text": "3040 Calendar Description\n\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#this-section-001",
    "href": "lectures/lecture-01-slides.html#this-section-001",
    "title": "Introduction + Telling Stories with Data",
    "section": "This Section (001)",
    "text": "This Section (001)\n\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s (2023) Telling Stories with Data, you will learn how to:\n\nplan an analysis and sketch your data and endpoint simulate some data to “force you into the details” acquire, assess, and prepare empirical data for analysis explore and analyze data by creating visualizations and fitting models share the results of your work with the world!\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#this-section-001-1",
    "href": "lectures/lecture-01-slides.html#this-section-001-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "This Section (001)",
    "text": "This Section (001)\n\n“A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.” (Alexander 2023)\n\n\nCore foundation of quantitative research methods Bridge between analysis and understanding Essential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#this-section-001-2",
    "href": "lectures/lecture-01-slides.html#this-section-001-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "This Section (001)",
    "text": "This Section (001)\n\n\n\n\n\n\n\n You will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear and generalized linear models. Reproducibility and research ethics are considered throughout the workflow, and the entire course."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#common-concerns-key-questions",
    "href": "lectures/lecture-01-slides.html#common-concerns-key-questions",
    "title": "Introduction + Telling Stories with Data",
    "section": "Common Concerns & Key Questions",
    "text": "Common Concerns & Key Questions\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What’s missing or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data? Have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components",
    "href": "lectures/lecture-01-slides.html#core-workflow-components",
    "title": "Introduction + Telling Stories with Data",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\nPlan and Sketch\ndeliberate, reasoned decisions purposeful adjustments even 10 minutes of planning is valuable\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn’t require extensive time—often just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components-1",
    "href": "lectures/lecture-01-slides.html#core-workflow-components-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\nSimulate Data\nForces detailed thinking Clarifies expected data structure and distributions. Helps with cleaning and preparation Identifies potential issues beforehand. Provides clear testing framework Ensures data meets expectations. “Almost free” with modern computing Provides “an intimate feeling for the situation” (Hamming [1997] 2020)\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides “an intimate feeling for the situation” and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components-2",
    "href": "lectures/lecture-01-slides.html#core-workflow-components-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\nAcquire and Prepare\nOften overlooked but crucial stage Many difficult decisions required: data sources, formats, permissions. Can significantly affect statistical results (Huntington-Klein et al. 2021) Common challenges: quantity (too little or too much data) and quality\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed—either by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components-3",
    "href": "lectures/lecture-01-slides.html#core-workflow-components-3",
    "title": "Introduction + Telling Stories with Data",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\nExplore and Understand\nBegin with descriptive statistics Move to statistical models\nRemember: Models are tools, not truth, and they reflect our previous decisions, data acquisition choices, and cleaning procedures.\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It’s important to remember that statistical models are tools—not absolute truths—and they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components-4",
    "href": "lectures/lecture-01-slides.html#core-workflow-components-4",
    "title": "Introduction + Telling Stories with Data",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\nShare Findings\nHigh-fidelity communication is essential Document all decisions Build credibility through transparency\n\nInclude:\nWhat was done Why it was done What was found Weaknesses of the approach\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it’s essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process—from data acquisition to analysis—builds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#quantitative-research-essentials-1",
    "href": "lectures/lecture-01-slides.html#quantitative-research-essentials-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "Quantitative Research Essentials",
    "text": "Quantitative Research Essentials\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#communication-most-important",
    "href": "lectures/lecture-01-slides.html#communication-most-important",
    "title": "Introduction + Telling Stories with Data",
    "section": "Communication (Most Important)",
    "text": "Communication (Most Important)\n\n“Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly.” (Alexander 2023)\n\n\n“One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.” (Alexander 2023)\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience’s perspective"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#reproducibility",
    "href": "lectures/lecture-01-slides.html#reproducibility",
    "title": "Introduction + Telling Stories with Data",
    "section": "Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\n\nRequirements:\nOpen access to code Data availability or simulation Automated testing Clear documentation Aim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#ethics",
    "href": "lectures/lecture-01-slides.html#ethics",
    "title": "Introduction + Telling Stories with Data",
    "section": "Ethics",
    "text": "Ethics\n\n“This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?” (Alexander 2023)\n\nConsider the full context of the dataset (D’Ignazio and Klein 2020) Acknowledge the social, cultural, and political forces (Crawford 2021) Use data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#questions",
    "href": "lectures/lecture-01-slides.html#questions",
    "title": "Introduction + Telling Stories with Data",
    "section": "Questions",
    "text": "Questions\nQuestions evolve through understanding Challenge of operationalizing variables Curiosity is essential, drives deeper exploration Value of “hybrid” knowledge that combines multiple disciplines Comfort with asking “dumb” questions\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a “hybrid”—cultivating knowledge across various disciplines—and being comfortable with asking seemingly simple or “dumb” questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#measurement",
    "href": "lectures/lecture-01-slides.html#measurement",
    "title": "Introduction + Telling Stories with Data",
    "section": "Measurement",
    "text": "Measurement\n\n“The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.” (Alexander 2023)\n\n\n\nMeasuring even simple things is challenging (e.g., measuring height: Shoes on or off? Time of day affects height. Different tools yield different results). More complex measurements are even harder. How do we measure happiness or pain?\n Measurement requires decisions and is not value-free. Context and purpose guide all measurement choices.\n\n\n\n\nPicasso’s dog and the challenges of reduction.\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person’s height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso’s minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we’re measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#data-collection-cleaning",
    "href": "lectures/lecture-01-slides.html#data-collection-cleaning",
    "title": "Introduction + Telling Stories with Data",
    "section": "Data Collection & Cleaning",
    "text": "Data Collection & Cleaning\n\n“Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.” (Alexander 2023)\n\nCollection determines possibilities What and how we measure matters.\nCleaning requires many decisions E.g., Handling “prefer not to say” and open-text responses.\nDocument every step To ensure transparency and reproducibility.\nConsider implications of choices E.g., ethics, representation.\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Decisions such as whether to exclude “prefer not to say” responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents’ specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it’s vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#exploratory-data-analysis-eda",
    "href": "lectures/lecture-01-slides.html#exploratory-data-analysis-eda",
    "title": "Introduction + Telling Stories with Data",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nIterative process Never truly complete Shapes understanding\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there’s always more to explore. Although it’s challenging to delineate where EDA ends and formal statistical modeling begins—since our beliefs and understanding evolve continuously—EDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it’s crucial that all steps taken during EDA are recorded and shared."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#modeling",
    "href": "lectures/lecture-01-slides.html#modeling",
    "title": "Introduction + Telling Stories with Data",
    "section": "Modeling",
    "text": "Modeling\nTool for understanding Not a recipe to follow Just one representation of reality Statistical significance \\(\\neq\\) scientific significance Statistical models help us explore the shape of the data; are like echolocation\n\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it’s a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference—the process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#scaling",
    "href": "lectures/lecture-01-slides.html#scaling",
    "title": "Introduction + Telling Stories with Data",
    "section": "Scaling",
    "text": "Scaling\nUsing programming languages like R and Python\nHandle large datasets efficiently Automate repetitive tasks Share work widely and quickly Outputs can reach many people easily APIs can make analyses accessible in real-time\n\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it’s essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly—whether it’s one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction 👨‍🍳\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data—plausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it’s crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment—much like a chef monitoring a reduction to achieve the desired consistency without overcooking—to determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors—from selecting what to measure to deciding on the methods and accuracy required. Advances in instruments—from telescopes in astronomy to real-time internet data collection—have expanded our ability to gather data, much like new culinary techniques enhance a chef’s ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our “reduction” process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#embracing-the-challenge-1",
    "href": "lectures/lecture-01-slides.html#embracing-the-challenge-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\n\n“Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.” (Alexander 2023)\n\nTelling good stories with data is difficult but rewarding.\nDevelop resilience and intrinsic motivation. Accept that failure is part of the process. Consider possibilities and probabilities. Learn to make trade-offs. No perfect analysis exists. Aim for transparency and continuous improvement."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#key-takeaways",
    "href": "lectures/lecture-01-slides.html#key-takeaways",
    "title": "Introduction + Telling Stories with Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#section-1",
    "href": "lectures/lecture-01-slides.html#section-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "Brightspace Course materials website: johnmclevey.com/SOCI3040/"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#next-class",
    "href": "lectures/lecture-01-slides.html#next-class",
    "title": "Introduction + Telling Stories with Data",
    "section": "Next class",
    "text": "Next class\n\nBefore class: Complete the assigned reading In class: Introduction to R and RStudio"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#references",
    "href": "lectures/lecture-01-slides.html#references",
    "title": "Introduction + Telling Stories with Data",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992."
  },
  {
    "objectID": "lectures/lecture-07-notes.html",
    "href": "lectures/lecture-07-notes.html",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required:   Ch 4\nRecommended:   Ch 3"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#reading-assignment",
    "href": "lectures/lecture-07-notes.html#reading-assignment",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required:   Ch 4\nRecommended:   Ch 3"
  },
  {
    "objectID": "lectures/lecture-01-notes.html",
    "href": "lectures/lecture-01-notes.html",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "Required: Browse this site\nRecommended: \n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#reading-assignment",
    "href": "lectures/lecture-01-notes.html#reading-assignment",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "Required: Browse this site\nRecommended:"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#lecture-slides",
    "href": "lectures/lecture-01-notes.html#lecture-slides",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-notes.html#welcome-to-soci-3040",
    "title": "Introduction + Telling Stories with Data",
    "section": "0.1 👋 Welcome to SOCI 3040!",
    "text": "0.1 👋 Welcome to SOCI 3040!\n\nMy name is [John, Professor McLevey, Dr. McLevey] (he/him).\nProfessor & Head of Sociology New to Memorial after 11 years at University of Waterloo"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#agenda.",
    "href": "lectures/lecture-01-notes.html#agenda.",
    "title": "Introduction + Telling Stories with Data",
    "section": "0.2 agenda.",
    "text": "0.2 agenda.\n\nWho are you? Background? Expectations?\nWhat is this course about?\nWhat will we do? Where is everything?"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#section",
    "href": "lectures/lecture-01-notes.html#section",
    "title": "Introduction + Telling Stories with Data",
    "section": "1.1 ",
    "text": "1.1 \n\nWho are you? Do you have any previous quant courses / experience? What are your expectations for this course?"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#calendar-description",
    "href": "lectures/lecture-01-notes.html#calendar-description",
    "title": "Introduction + Telling Stories with Data",
    "section": "2.1 3040 Calendar Description",
    "text": "2.1 3040 Calendar Description\n\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#this-section-001",
    "href": "lectures/lecture-01-notes.html#this-section-001",
    "title": "Introduction + Telling Stories with Data",
    "section": "2.2 This Section (001)",
    "text": "2.2 This Section (001)\n\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s (2023) Telling Stories with Data, you will learn how to:\n\nplan an analysis and sketch your data and endpoint simulate some data to “force you into the details” acquire, assess, and prepare empirical data for analysis explore and analyze data by creating visualizations and fitting models share the results of your work with the world!\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#this-section-001-1",
    "href": "lectures/lecture-01-notes.html#this-section-001-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "2.3 This Section (001)",
    "text": "2.3 This Section (001)\n\n“A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.” (Alexander 2023)\n\n\nCore foundation of quantitative research methods Bridge between analysis and understanding Essential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#this-section-001-2",
    "href": "lectures/lecture-01-notes.html#this-section-001-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "2.4 This Section (001)",
    "text": "2.4 This Section (001)\n\n\n\n\n\n\n\n You will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear and generalized linear models. Reproducibility and research ethics are considered throughout the workflow, and the entire course."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#common-concerns-key-questions",
    "href": "lectures/lecture-01-notes.html#common-concerns-key-questions",
    "title": "Introduction + Telling Stories with Data",
    "section": "2.5 Common Concerns & Key Questions",
    "text": "2.5 Common Concerns & Key Questions\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What’s missing or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data? Have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components",
    "href": "lectures/lecture-01-notes.html#core-workflow-components",
    "title": "Introduction + Telling Stories with Data",
    "section": "3.1 Core Workflow Components",
    "text": "3.1 Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\n3.1.1 Plan and Sketch\ndeliberate, reasoned decisions purposeful adjustments even 10 minutes of planning is valuable\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn’t require extensive time—often just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components-1",
    "href": "lectures/lecture-01-notes.html#core-workflow-components-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "3.2 Core Workflow Components",
    "text": "3.2 Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\n3.2.1 Simulate Data\nForces detailed thinking Clarifies expected data structure and distributions. Helps with cleaning and preparation Identifies potential issues beforehand. Provides clear testing framework Ensures data meets expectations. “Almost free” with modern computing Provides “an intimate feeling for the situation” (Hamming [1997] 2020)\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides “an intimate feeling for the situation” and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components-2",
    "href": "lectures/lecture-01-notes.html#core-workflow-components-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "3.3 Core Workflow Components",
    "text": "3.3 Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\n3.3.1 Acquire and Prepare\nOften overlooked but crucial stage Many difficult decisions required: data sources, formats, permissions. Can significantly affect statistical results (Huntington-Klein et al. 2021) Common challenges: quantity (too little or too much data) and quality\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed—either by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components-3",
    "href": "lectures/lecture-01-notes.html#core-workflow-components-3",
    "title": "Introduction + Telling Stories with Data",
    "section": "3.4 Core Workflow Components",
    "text": "3.4 Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\n3.4.1 Explore and Understand\nBegin with descriptive statistics Move to statistical models\nRemember: Models are tools, not truth, and they reflect our previous decisions, data acquisition choices, and cleaning procedures.\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It’s important to remember that statistical models are tools—not absolute truths—and they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components-4",
    "href": "lectures/lecture-01-notes.html#core-workflow-components-4",
    "title": "Introduction + Telling Stories with Data",
    "section": "3.5 Core Workflow Components",
    "text": "3.5 Core Workflow Components\nPlan, Simulate, Acquire, Explore / Analyze, Share\n\n\n3.5.1 Share Findings\nHigh-fidelity communication is essential Document all decisions Build credibility through transparency\n\nInclude:\nWhat was done Why it was done What was found Weaknesses of the approach\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it’s essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process—from data acquisition to analysis—builds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#quantitative-research-essentials-1",
    "href": "lectures/lecture-01-notes.html#quantitative-research-essentials-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.1 Quantitative Research Essentials",
    "text": "4.1 Quantitative Research Essentials\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#communication-most-important",
    "href": "lectures/lecture-01-notes.html#communication-most-important",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.2 Communication (Most Important)",
    "text": "4.2 Communication (Most Important)\n\n“Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly.” (Alexander 2023)\n\n\n“One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.” (Alexander 2023)\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience’s perspective"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#reproducibility",
    "href": "lectures/lecture-01-notes.html#reproducibility",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.3 Reproducibility",
    "text": "4.3 Reproducibility\nEverything must be independently repeatable.\n\nRequirements:\nOpen access to code Data availability or simulation Automated testing Clear documentation Aim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#ethics",
    "href": "lectures/lecture-01-notes.html#ethics",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.4 Ethics",
    "text": "4.4 Ethics\n\n“This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?” (Alexander 2023)\n\nConsider the full context of the dataset (D’Ignazio and Klein 2020) Acknowledge the social, cultural, and political forces (Crawford 2021) Use data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#questions",
    "href": "lectures/lecture-01-notes.html#questions",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.5 Questions",
    "text": "4.5 Questions\nQuestions evolve through understanding Challenge of operationalizing variables Curiosity is essential, drives deeper exploration Value of “hybrid” knowledge that combines multiple disciplines Comfort with asking “dumb” questions\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a “hybrid”—cultivating knowledge across various disciplines—and being comfortable with asking seemingly simple or “dumb” questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#measurement",
    "href": "lectures/lecture-01-notes.html#measurement",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.6 Measurement",
    "text": "4.6 Measurement\n\n“The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.” (Alexander 2023)\n\n\n\nMeasuring even simple things is challenging (e.g., measuring height: Shoes on or off? Time of day affects height. Different tools yield different results). More complex measurements are even harder. How do we measure happiness or pain?\n Measurement requires decisions and is not value-free. Context and purpose guide all measurement choices.\n\n\n\n\nPicasso’s dog and the challenges of reduction.\n\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person’s height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso’s minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we’re measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#data-collection-cleaning",
    "href": "lectures/lecture-01-notes.html#data-collection-cleaning",
    "title": "Introduction + Telling Stories with Data",
    "section": "4.7 Data Collection & Cleaning",
    "text": "4.7 Data Collection & Cleaning\n\n“Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.” (Alexander 2023)\n\nCollection determines possibilities What and how we measure matters.\nCleaning requires many decisions E.g., Handling “prefer not to say” and open-text responses.\nDocument every step To ensure transparency and reproducibility.\nConsider implications of choices E.g., ethics, representation.\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Decisions such as whether to exclude “prefer not to say” responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents’ specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it’s vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#exploratory-data-analysis-eda",
    "href": "lectures/lecture-01-notes.html#exploratory-data-analysis-eda",
    "title": "Introduction + Telling Stories with Data",
    "section": "5.1 Exploratory Data Analysis (EDA)",
    "text": "5.1 Exploratory Data Analysis (EDA)\nIterative process Never truly complete Shapes understanding\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there’s always more to explore. Although it’s challenging to delineate where EDA ends and formal statistical modeling begins—since our beliefs and understanding evolve continuously—EDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it’s crucial that all steps taken during EDA are recorded and shared."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#modeling",
    "href": "lectures/lecture-01-notes.html#modeling",
    "title": "Introduction + Telling Stories with Data",
    "section": "5.2 Modeling",
    "text": "5.2 Modeling\nTool for understanding Not a recipe to follow Just one representation of reality Statistical significance \\(\\neq\\) scientific significance Statistical models help us explore the shape of the data; are like echolocation\n\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it’s a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference—the process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#scaling",
    "href": "lectures/lecture-01-notes.html#scaling",
    "title": "Introduction + Telling Stories with Data",
    "section": "5.3 Scaling",
    "text": "5.3 Scaling\nUsing programming languages like R and Python\nHandle large datasets efficiently Automate repetitive tasks Share work widely and quickly Outputs can reach many people easily APIs can make analyses accessible in real-time\n\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it’s essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly—whether it’s one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "6.1 How Do Our Worlds Become Data?",
    "text": "6.1 How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "6.2 How Do Our Worlds Become Data?",
    "text": "6.2 How Do Our Worlds Become Data?\n Through skillfulreduction 👨‍🍳\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data—plausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it’s crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment—much like a chef monitoring a reduction to achieve the desired consistency without overcooking—to determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors—from selecting what to measure to deciding on the methods and accuracy required. Advances in instruments—from telescopes in astronomy to real-time internet data collection—have expanded our ability to gather data, much like new culinary techniques enhance a chef’s ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our “reduction” process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#embracing-the-challenge-1",
    "href": "lectures/lecture-01-notes.html#embracing-the-challenge-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "7.1 Embracing the Challenge",
    "text": "7.1 Embracing the Challenge\n\n“Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.” (Alexander 2023)\n\nTelling good stories with data is difficult but rewarding.\nDevelop resilience and intrinsic motivation. Accept that failure is part of the process. Consider possibilities and probabilities. Learn to make trade-offs. No perfect analysis exists. Aim for transparency and continuous improvement."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#key-takeaways",
    "href": "lectures/lecture-01-notes.html#key-takeaways",
    "title": "Introduction + Telling Stories with Data",
    "section": "7.2 Key Takeaways",
    "text": "7.2 Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#section-1",
    "href": "lectures/lecture-01-notes.html#section-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "8.1 ",
    "text": "8.1 \n\nBrightspace Course materials website: johnmclevey.com/SOCI3040/"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#next-class",
    "href": "lectures/lecture-01-notes.html#next-class",
    "title": "Introduction + Telling Stories with Data",
    "section": "8.2 Next class",
    "text": "8.2 Next class\n\nBefore class: Complete the assigned reading In class: Introduction to R and RStudio"
  },
  {
    "objectID": "lectures/lecture-06-notes.html",
    "href": "lectures/lecture-06-notes.html",
    "title": "Reproducible Workflows with R & RStudio",
    "section": "",
    "text": "Required:   Ch 3\nRecommended:   Ch 2\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-06-notes.html#reading-assignment",
    "href": "lectures/lecture-06-notes.html#reading-assignment",
    "title": "Reproducible Workflows with R & RStudio",
    "section": "",
    "text": "Required:   Ch 3\nRecommended:   Ch 2"
  },
  {
    "objectID": "lectures/lecture-06-notes.html#lecture-slides",
    "href": "lectures/lecture-06-notes.html#lecture-slides",
    "title": "Reproducible Workflows with R & RStudio",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-11-notes.html",
    "href": "lectures/lecture-11-notes.html",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required:   Ch 6\nRecommended:   Ch 5\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#reading-assignment",
    "href": "lectures/lecture-11-notes.html#reading-assignment",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required:   Ch 6\nRecommended:   Ch 5"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#lecture-slides",
    "href": "lectures/lecture-11-notes.html#lecture-slides",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-13-notes.html",
    "href": "lectures/lecture-13-notes.html",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required:   Ch 7\nRecommended:   Ch 7\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#reading-assignment",
    "href": "lectures/lecture-13-notes.html#reading-assignment",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required:   Ch 7\nRecommended:   Ch 7"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#lecture-slides",
    "href": "lectures/lecture-13-notes.html#lecture-slides",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-15-notes.html",
    "href": "lectures/lecture-15-notes.html",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required:   Ch 8\nRecommended:   Ch 8\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#reading-assignment",
    "href": "lectures/lecture-15-notes.html#reading-assignment",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required:   Ch 8\nRecommended:   Ch 8"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#lecture-slides",
    "href": "lectures/lecture-15-notes.html#lecture-slides",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-17-notes.html",
    "href": "lectures/lecture-17-notes.html",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:   Ch 9\nRecommended:   Statistical Golems\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#reading-assignment",
    "href": "lectures/lecture-17-notes.html#reading-assignment",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:   Ch 9\nRecommended:   Statistical Golems"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#lecture-slides",
    "href": "lectures/lecture-17-notes.html#lecture-slides",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-19-notes.html",
    "href": "lectures/lecture-19-notes.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:   Ch 11\nRecommended:   The Garden of Forking Data\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#reading-assignment",
    "href": "lectures/lecture-19-notes.html#reading-assignment",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:   Ch 11\nRecommended:   The Garden of Forking Data"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#lecture-slides",
    "href": "lectures/lecture-19-notes.html#lecture-slides",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-21-notes.html",
    "href": "lectures/lecture-21-notes.html",
    "title": "Linear Models",
    "section": "",
    "text": "Required:   Ch 12\nRecommended:   Geocentric Models\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#reading-assignment",
    "href": "lectures/lecture-21-notes.html#reading-assignment",
    "title": "Linear Models",
    "section": "",
    "text": "Required:   Ch 12\nRecommended:   Geocentric Models"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#lecture-slides",
    "href": "lectures/lecture-21-notes.html#lecture-slides",
    "title": "Linear Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-23-notes.html",
    "href": "lectures/lecture-23-notes.html",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required:   Ch 13\nRecommended:   Ch 6\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#reading-assignment",
    "href": "lectures/lecture-23-notes.html#reading-assignment",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required:   Ch 13\nRecommended:   Ch 6"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#lecture-slides",
    "href": "lectures/lecture-23-notes.html#lecture-slides",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-25-notes.html",
    "href": "lectures/lecture-25-notes.html",
    "title": "Project Work",
    "section": "",
    "text": "Required: ?meta:assigned_reading\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#reading-assignment",
    "href": "lectures/lecture-25-notes.html#reading-assignment",
    "title": "Project Work",
    "section": "",
    "text": "Required: ?meta:assigned_reading\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#lecture-slides",
    "href": "lectures/lecture-25-notes.html#lecture-slides",
    "title": "Project Work",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-03-notes.html",
    "href": "lectures/lecture-03-notes.html",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "",
    "text": "Lecture Slides\nView slides in full screen\nClass Notes"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#the-firehose",
    "href": "lectures/lecture-03-notes.html#the-firehose",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.1 the firehose",
    "text": "1.1 the firehose\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s (2023) Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#the-firehose-1",
    "href": "lectures/lecture-03-notes.html#the-firehose-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.2 the firehose",
    "text": "1.2 the firehose\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by Barrett (2021)\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan Alexander (2023)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#import-libraries",
    "href": "lectures/lecture-03-notes.html#import-libraries",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "1.3 import libraries",
    "text": "1.3 import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#plan-1",
    "href": "lectures/lecture-03-notes.html#plan-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "2.1 plan",
    "text": "2.1 plan\n\n\n2.1.1 Australian Elections\n\n\n\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#plan-2",
    "href": "lectures/lecture-03-notes.html#plan-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "2.2 plan",
    "text": "2.2 plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure 1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a “division” in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-1",
    "href": "lectures/lecture-03-notes.html#simulate-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.1 simulate",
    "text": "3.1 simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-2",
    "href": "lectures/lecture-03-notes.html#simulate-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.2 simulate",
    "text": "3.2 simulate\n\nWe’ll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-3",
    "href": "lectures/lecture-03-notes.html#simulate-3",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.3 simulate",
    "text": "3.3 simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we’re creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named “Division”. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it’s used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options — “Liberal”, “Labor”, “National”, “Green”, and “Other” — into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the “Division” column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we’re picking “with replacement” (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn’t match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-4",
    "href": "lectures/lecture-03-notes.html#simulate-4",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "3.4 simulate",
    "text": "3.4 simulate\n🤘 We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Liberal \n 2        2 Green   \n 3        3 Other   \n 4        4 National\n 5        5 Green   \n 6        6 National\n 7        7 Labor   \n 8        8 National\n 9        9 National\n10       10 Other   \n# ℹ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-1",
    "href": "lectures/lecture-03-notes.html#acquire-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.1 acquire",
    "text": "4.1 acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-2",
    "href": "lectures/lecture-03-notes.html#acquire-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.2 acquire",
    "text": "4.2 acquire\n\nWe’ll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n\n\n\n✌️ R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief “Ode to the here package,” “here here,” which you can read… here."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-3",
    "href": "lectures/lecture-03-notes.html#acquire-3",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.3 acquire",
    "text": "4.3 acquire\n🤘 We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 × 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA…\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather… KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge… ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE…\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ℹ 141 more rows\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-4",
    "href": "lectures/lecture-03-notes.html#acquire-4",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.4 acquire",
    "text": "4.4 acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA…\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri… KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-5",
    "href": "lectures/lecture-03-notes.html#acquire-5",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.5 acquire",
    "text": "4.5 acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma… STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO…\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-6",
    "href": "lectures/lecture-03-notes.html#acquire-6",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.6 acquire",
    "text": "4.6 acquire\n\n“We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.” (Alexander 2023)\n\n\nLet’s clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-7",
    "href": "lectures/lecture-03-notes.html#acquire-7",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.7 acquire",
    "text": "4.7 acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let’s look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ℹ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-8",
    "href": "lectures/lecture-03-notes.html#acquire-8",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.8 acquire",
    "text": "4.8 acquire\n\n\n\n\n\n\n✌️ R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-9",
    "href": "lectures/lecture-03-notes.html#acquire-9",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.9 acquire",
    "text": "4.9 acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-10",
    "href": "lectures/lecture-03-notes.html#acquire-10",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.10 acquire",
    "text": "4.10 acquire\n\n\n\n\n\n\n\n✌️ R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet’s rename."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-11",
    "href": "lectures/lecture-03-notes.html#acquire-11",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.11 acquire",
    "text": "4.11 acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-12",
    "href": "lectures/lecture-03-notes.html#acquire-12",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.12 acquire",
    "text": "4.12 acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let’s simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-13",
    "href": "lectures/lecture-03-notes.html#acquire-13",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.13 acquire",
    "text": "4.13 acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-14",
    "href": "lectures/lecture-03-notes.html#acquire-14",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.14 acquire",
    "text": "4.14 acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! 😎"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#aus_elections_clean_path",
    "href": "lectures/lecture-03-notes.html#aus_elections_clean_path",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "4.15 acquire",
    "text": "4.15 acquire\n\nLet’s save the cleaned data so that we can start with it data in the next stage. We’ll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-1",
    "href": "lectures/lecture-03-notes.html#explore-understand-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.1 explore / understand",
    "text": "5.1 explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-2",
    "href": "lectures/lecture-03-notes.html#explore-understand-2",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.2 explore / understand",
    "text": "5.2 explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nI’m using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won’t work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-3",
    "href": "lectures/lecture-03-notes.html#explore-understand-3",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.3 explore / understand",
    "text": "5.3 explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n😎"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-4",
    "href": "lectures/lecture-03-notes.html#explore-understand-4",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.4 explore / understand",
    "text": "5.4 explore / understand\n\n\n\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-5",
    "href": "lectures/lecture-03-notes.html#explore-understand-5",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.5 explore / understand",
    "text": "5.5 explore / understand\n\n\n\n\n\n\nRemember, we’re trying to make something like this.\n\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer’s understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer’s eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-6",
    "href": "lectures/lecture-03-notes.html#explore-understand-6",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.6 explore / understand",
    "text": "5.6 explore / understand\n\nLet’s visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it’s cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\n\n\n\n\n\n\nFigure 2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-7",
    "href": "lectures/lecture-03-notes.html#explore-understand-7",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.7 explore / understand",
    "text": "5.7 explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels\n\n\n\n\n\n\n\nFigure 3: Number of seats won, by political party, at the 2022 Australian Federal Election. 😎"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#section",
    "href": "lectures/lecture-03-notes.html#section",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "5.8 ",
    "text": "5.8 \ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure 4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#share-1",
    "href": "lectures/lecture-03-notes.html#share-1",
    "title": "Data Analysis Workflow – The Firehose",
    "section": "6.1 share",
    "text": "6.1 share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure 3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "lectures/lecture-03-content.html",
    "href": "lectures/lecture-03-content.html",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "flowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)\n\n\n\n\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by @citeBarrett\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan @alexander2023telling\n\n\n\n\n\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-content.html#the-firehose",
    "href": "lectures/lecture-03-content.html#the-firehose",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "flowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#the-firehose-1",
    "href": "lectures/lecture-03-content.html#the-firehose-1",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "Whenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by @citeBarrett\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan @alexander2023telling"
  },
  {
    "objectID": "lectures/lecture-03-content.html#import-libraries",
    "href": "lectures/lecture-03-content.html#import-libraries",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "library(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-content.html#plan-1",
    "href": "lectures/lecture-03-content.html#plan-1",
    "title": "Drinking From the Firehose",
    "section": "plan",
    "text": "plan\n\n\nAustralian Elections\n\n\n\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-03-content.html#plan-2",
    "href": "lectures/lecture-03-content.html#plan-2",
    "title": "Drinking From the Firehose",
    "section": "plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure 1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a “division” in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-1",
    "href": "lectures/lecture-03-content.html#simulate-1",
    "title": "Drinking From the Firehose",
    "section": "simulate",
    "text": "simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-2",
    "href": "lectures/lecture-03-content.html#simulate-2",
    "title": "Drinking From the Firehose",
    "section": "simulate",
    "text": "simulate\n\nWe’ll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-3",
    "href": "lectures/lecture-03-content.html#simulate-3",
    "title": "Drinking From the Firehose",
    "section": "simulate",
    "text": "simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we’re creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named “Division”. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it’s used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options — “Liberal”, “Labor”, “National”, “Green”, and “Other” — into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the “Division” column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we’re picking “with replacement” (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn’t match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-4",
    "href": "lectures/lecture-03-content.html#simulate-4",
    "title": "Drinking From the Firehose",
    "section": "simulate",
    "text": "simulate\n🤘 We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Green   \n 2        2 National\n 3        3 Other   \n 4        4 Liberal \n 5        5 Green   \n 6        6 Other   \n 7        7 Other   \n 8        8 Other   \n 9        9 National\n10       10 Green   \n# ℹ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-1",
    "href": "lectures/lecture-03-content.html#acquire-1",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-2",
    "href": "lectures/lecture-03-content.html#acquire-2",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\nWe’ll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n\n\n\n✌️ R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief “Ode to the here package,” “here here,” which you can read… here."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-3",
    "href": "lectures/lecture-03-content.html#acquire-3",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n🤘 We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 × 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA…\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather… KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge… ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE…\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ℹ 141 more rows\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-4",
    "href": "lectures/lecture-03-content.html#acquire-4",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA…\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri… KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-5",
    "href": "lectures/lecture-03-content.html#acquire-5",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma… STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO…\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-6",
    "href": "lectures/lecture-03-content.html#acquire-6",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n“We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.” [@alexander2023telling]\n\n\nLet’s clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-7",
    "href": "lectures/lecture-03-content.html#acquire-7",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let’s look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ℹ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-8",
    "href": "lectures/lecture-03-content.html#acquire-8",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n\n\n\n\n\n✌️ R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-9",
    "href": "lectures/lecture-03-content.html#acquire-9",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-10",
    "href": "lectures/lecture-03-content.html#acquire-10",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n\n\n\n\n\n\n✌️ R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet’s rename."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-11",
    "href": "lectures/lecture-03-content.html#acquire-11",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-12",
    "href": "lectures/lecture-03-content.html#acquire-12",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let’s simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-13",
    "href": "lectures/lecture-03-content.html#acquire-13",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-14",
    "href": "lectures/lecture-03-content.html#acquire-14",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! 😎"
  },
  {
    "objectID": "lectures/lecture-03-content.html#aus_elections_clean_path",
    "href": "lectures/lecture-03-content.html#aus_elections_clean_path",
    "title": "Drinking From the Firehose",
    "section": "acquire",
    "text": "acquire\n\nLet’s save the cleaned data so that we can start with it data in the next stage. We’ll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-1",
    "href": "lectures/lecture-03-content.html#explore-understand-1",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-2",
    "href": "lectures/lecture-03-content.html#explore-understand-2",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nI’m using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won’t work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-3",
    "href": "lectures/lecture-03-content.html#explore-understand-3",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n😎"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-4",
    "href": "lectures/lecture-03-content.html#explore-understand-4",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-5",
    "href": "lectures/lecture-03-content.html#explore-understand-5",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nRemember, we’re trying to make something like this.\n\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer’s understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer’s eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-6",
    "href": "lectures/lecture-03-content.html#explore-understand-6",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\nLet’s visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it’s cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\n\n\n\n\n\n\nFigure 2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-7",
    "href": "lectures/lecture-03-content.html#explore-understand-7",
    "title": "Drinking From the Firehose",
    "section": "explore / understand",
    "text": "explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels\n\n\n\n\n\n\n\nFigure 3: Number of seats won, by political party, at the 2022 Australian Federal Election. 😎"
  },
  {
    "objectID": "lectures/lecture-03-content.html#section",
    "href": "lectures/lecture-03-content.html#section",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "cleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure 4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-03-content.html#share-1",
    "href": "lectures/lecture-03-content.html#share-1",
    "title": "Drinking From the Firehose",
    "section": "share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R [@citeR] including the tidyverse [@tidyverse] and janitor [@janitor]. We then created a graph of the number of seats that each political party won (Figure 3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, @biasbehindbars and @bronnerftw."
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": " Quantitative Research Methods",
    "section": "",
    "text": "Course Instructor John McLevey (he/him) Professor, Department of Sociology Memorial University\n\n  mclevey@mun.ca Note: I do not check or respond to email in the evenings or on weekends.\n\n\nGraduate Assistant Felix Morrow, PhD Student Department of Sociology, Memorial University fpcmorrow@mun.ca\n\nWhere is class? CP-2003 (Chemistry-Physics, Computer Lab) When is class? Tuesdays & Thursdays, 1:30 - 2:50 pm Office Hours: A4054, Tuesdays & Thursdays, 3:00 - 4:00 pm\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s (2023) Telling Stories with Data, you will learn how to:\nYou will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear and generalized linear models. Reproducibility and research ethics are considered throughout the workflow, and the entire course.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#alexander2023telling-telling-stories-with-data",
    "href": "syllabus/syllabus.html#alexander2023telling-telling-stories-with-data",
    "title": " Quantitative Research Methods",
    "section": " Alexander (2023) Telling Stories with Data",
    "text": "Alexander (2023) Telling Stories with Data\n\n“This is not another statistics book. It is much better than that. It is a book about doing quantitative research, about scientific justification, about quality control, about communication and epistemic humility. It’s a valuable supplement to any methods curriculum, and useful for self-learners as well.” – Richard McElreath\n\n\n\n\n\n“This clean and fun book covers a wide range of topics on statistical communication, programming, and modeling in a way that should be a useful supplement to any statistics course or self-learning program. I absolutely love this book!” – Andrew Gelman\n\n\n“Every data analyst has to tell stories with data, and yet traditional textbooks focus on statistical methods alone. Telling Stories with Data teaches the entire data science workflow, including data acquisition, communication, and reproducibility. I highly recommend this unique book!” – Kosuke Imai\n\n\n“Telling (true) Stories with Data requires more than fancy statistical models and big data. With a series of fascinating case studies, Rohan Alexander teaches us how to ask good questions, acquire data, estimate models, and communicate our results. This holistic approach is explained with crisp and engaging prose. The pages are filled with detailed R examples, which emphasize the importance of transparency and reproducibility. I absolutely love this book and recommend it to all my students.” – Vincent Arel-Bundock\n\n\n“An excellent book. Communication and reproducibility are of increasing concern in statistics, and this book covers these topics and more in a practical, appealing, and truly unique way.” – Daniela Witten",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#recommended-optional",
    "href": "syllabus/syllabus.html#recommended-optional",
    "title": " Quantitative Research Methods",
    "section": "Recommended / Optional",
    "text": "Recommended / Optional\nYou may also wish to consult Kieran Healy’s (2019) Data Visualization: A Practical Introduction, which is also available for free online, in full.\n\n Healy (2019) Data Visualization\n\n“Finally! A data visualization guide that is simultaneously practical and elegant. … Data Visualization is brimming with insights into how quantitative analysts can use visualization as a tool for understanding and communication. A must-read for anyone who works with data.” – Elizabeth Bruch\n\n\n\n\n\n“Healy’s fun and readable book is unusual in covering the ‘why do’ as well as the ‘how to’ of data visualization, demonstrating how dataviz is a key step in all stages of social science – from theory construction to measurement to modeling and interpretation of analyses―and giving readers the tools to integrate visualization into their own work.” – Andrew Gelman\n\n\n“Data Visualization is a brilliant book that not only teaches the reader how to visualize data but also carefully considers why data visualization is essential for good social science. The book is broadly relevant, beautifully rendered, and engagingly written. It is easily accessible for students at any level and will be an incredible teaching resource for courses on research methods, statistics, and data visualization. It is packed full of clear-headed and sage insights.” – Becky Pettit\n\n\n“Kieran Healy has written a wonderful book that fills an important niche in an increasingly crowded landscape of materials about software in R. Data Visualization is clear, beautifully formatted, and full of careful insights.” – Brandon Stewart",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  }
]