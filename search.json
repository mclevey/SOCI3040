[
  {
    "objectID": "lectures/lecture-09-content.html",
    "href": "lectures/lecture-09-content.html",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Visualization is one way to get a sense of our data and to communicate this to the reader. Plotting the observations in a dataset is important.\nWe need to be comfortable with a variety of graph types, including: bar charts, scatterplots, line plots, and histograms. We can even consider a map to be a type of graph, especially after geocoding our data.\nWe should also summarize data using tables. Typical use cases for this include showing part of a dataset, summary statistics, and regression results.\n\n\n\n\n\n\n\nBase R [@citeR]\ncarData [@carData]\ndatasauRus [@citedatasauRus]\nggmap [@KahleWickham2013]\njanitor [@janitor]\nknitr [@citeknitr]\nmaps [@citemaps]\nmapproj [@mapproj]\nmodelsummary [@citemodelsummary]\nopendatatoronto [@citeSharla]\npatchwork [@citepatchwork]\ntidygeocoder [@tidygeocoder]\ntidyverse [@tidyverse]\ntroopdata [@troopdata]\nWDI [@WDI]\n\n\n\n\n\nlibrary(carData)\nlibrary(datasauRus)\nlibrary(ggmap)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(modelsummary)\nlibrary(opendatatoronto)\nlibrary(patchwork)\nlibrary(tidygeocoder)\nlibrary(tidyverse)\nlibrary(troopdata)\nlibrary(WDI)\n\n\n\n\n\n\nWhen telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.\nTry to show the observations that underpin our analysis. For instance, if your dataset consists of 2,500 responses to a survey, then at some point in the paper you should have a plot/s that contains each of the 2,500 observations, for every variable of interest. To do this we build graphs using ggplot2 which is part of the core tidyverse and so does not have to be installed or loaded separately. In this chapter we go through a variety of different options including bar charts, scatterplots, line plots, and histograms.\nIn contrast to the role of graphs, which is to show each observation, the role of tables is typically to show an extract of the dataset or to convey various summary statistics, or regression results. We will build tables primarily using knitr. Later we will use modelsummary to build tables related to regression output.\nFinally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using ggmap after having obtained geocoded data using tidygeocoder.\n\n\n\nGraphs are a critical aspect of compelling data stories. They allow us to see both broad patterns and details [@elementsofgraphingdata, p. 5]. Graphs enable a familiarity with our data that is hard to get from any other method. Every variable of interest should be graphed.\nThe most important objective of a graph is to convey as much of the actual data, and its context, as possible. In a way, graphing is an information encoding process where we construct a deliberate representation to convey information to our audience. The audience must decode that representation. The success of our graph depends on how much information is lost in this process so the decoding is a critical aspect [@elementsofgraphingdata, p. 221]. This means that we must focus on creating effective graphs that are suitable for our specific audience.\n\n\n\nTo see why graphing the actual data is important, after installing and loading datasauRus consider the datasaurus_dozen dataset.\n\ndatasaurus_dozen\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# ℹ 1,836 more rows\n\n\nThe dataset consists of values for “x” and “y”, which should be plotted on the x-axis and y-axis, respectively. There are 13 different values in the variable “dataset” including: “dino”, “star”, “away”, and “bullseye”. We focus on those four and generate summary statistics for each (Table 1).\n\n# Based on: https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  summarise(across(c(x, y), list(mean = mean, sd = sd)),\n            .by = dataset) |&gt;\n  kable(col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n        booktabs = TRUE, digits = 1)\n\n\n\nTable 1: Mean and standard deviation for four datasauRus datasets\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\ndino\n54.3\n16.8\n47.8\n26.9\n\n\naway\n54.3\n16.8\n47.8\n26.9\n\n\nstar\n54.3\n16.8\n47.8\n26.9\n\n\nbullseye\n54.3\n16.8\n47.8\n26.9\n\n\n\n\n\n\n\n\n\n\n\nNotice that the summary statistics are similar (Table 1). Despite this it turns out that the different datasets are actually very different beasts. This becomes clear when we plot the data (Figure 1).\n\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  ggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(color = \"Dataset\")\n\n\n\n\n\n\n\nFigure 1: Graph of four datasauRus datasets\n\n\n\n\n\n\n\n\nWe get a similar lesson—always plot your data—from “Anscombe’s Quartet”, created by the twentieth century statistician Frank Anscombe. The key takeaway is that it is important to plot the actual data and not rely solely on summary statistics.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n\n\n\n\n\nAnscombe’s Quartet consists of eleven observations for four different datasets, with x and y values for each observation. We need to manipulate this dataset with pivot_longer() to get it into the “tidy” format discussed in ?@sec-r-essentials.\n\n# From: https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# And the pivot_longer() vignette.\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(\n    everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )\n\n\n\n\nWe can first create summary statistics (Table 2) and then plot the data (Figure 2). This again illustrates the importance of graphing the actual data, rather than relying on summary statistics.\n\ntidy_anscombe |&gt;\n  summarise(\n    across(c(x, y), list(mean = mean, sd = sd)),\n    .by = set\n    ) |&gt;\n  kable(\n    col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n    digits = 1, booktabs = TRUE\n  )\n\n\n\nTable 2: Mean and standard deviation for Anscombe’s quartet\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\n1\n9\n3.3\n7.5\n2\n\n\n2\n9\n3.3\n7.5\n2\n\n\n3\n9\n3.3\n7.5\n2\n\n\n4\n9\n3.3\n7.5\n2\n\n\n\n\n\n\n\n\n\n\n\n\ntidy_anscombe |&gt;\n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Recreation of Anscombe’s Quartet\n\n\n\n\n\n\n\n\nWe typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in ?@sec-fire-hose when we constructed a graph of the number of occupied beds. The geometric object—a “geom”—that we primarily use is geom_bar(), but there are many variants to cater for specific situations. To illustrate the use of bar charts, we use a dataset from the 1997-2001 British Election Panel Study that was put together by @fox2006effect and made available with BEPS, after installing and loading carData.\n\nbeps &lt;- \n  BEPS |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  select(age, vote, gender, political_knowledge)\n\n\n\n\nThe dataset consists of which party the respondent supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondent. We begin by creating age-groups from the ages, and making a bar chart showing the frequency of each age-group using geom_bar() (Figure 3 (a)).\n\nbeps &lt;-\n  beps |&gt;\n  mutate(\n    age_group =\n      case_when(\n        age &lt; 35 ~ \"&lt;35\",\n        age &lt; 50 ~ \"35-49\",\n        age &lt; 65 ~ \"50-64\",\n        age &lt; 80 ~ \"65-79\",\n        age &lt; 100 ~ \"80-99\"\n      ),\n    age_group = \n      factor(age_group, levels = c(\"&lt;35\", \"35-49\", \"50-64\", \"65-79\", \"80-99\"))\n  )\n\n\n\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\nbeps |&gt; \n  count(age_group) |&gt; \n  ggplot(mapping = aes(x = age_group, y = n)) +\n  geom_col() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using count() and geom_col()\n\n\n\n\n\n\n\nFigure 3: Distribution of age-groups in the 1997-2001 British Election Panel Study\n\n\n\n\n\n\nThe default axis label used by ggplot2 is the name of the relevant variable, so it is often useful to add more detail. We do this using labs() by specifying a variable and a name. In the case of Figure 3 (a) we have specified labels for the x-axis and y-axis.\nBy default, geom_bar() creates a count of the number of times each age-group appears in the dataset. It does this because the default statistical transformation—a “stat”—for geom_bar() is “count”, which saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with beps |&gt; count(age_group)), then we could specify a variable for the y-axis and then use geom_col() (Figure 3 (b)).\n\n\n\nWe may also like to consider various groupings of the data to get a different insight. For instance, we can use color to look at which party the respondent supports, by age-group (Figure 4 (a)).\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge2\") +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using geom_bar() with dodge2\n\n\n\n\n\n\n\nFigure 4: Distribution of age-group, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\nBy default, these different groups are stacked, but they can be placed side by side with position = \"dodge2\" (Figure 4 (b)). (Using “dodge2” rather than “dodge” adds a little space between the bars.)\n\n\n\nAt this point, we may like to address the general look of the graph. There are various themes that are built into ggplot2. These include: theme_bw(), theme_classic(), theme_dark(), and theme_minimal(). A full list is available in the ggplot2 cheat sheet. We can use these themes by adding them as a layer (Figure 5). We could also install more themes from other packages, including ggthemes [@ggthemes], and hrbrthemes [@hrbrthemes]. We could even build our own!\n\ntheme_bw &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\n\n\n\n\n\n\n\nFigure 5: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes and the use of patchwork\n\n\n\n\n\nIn Figure 5 we use patchwork to bring together multiple graphs. To do this, after installing and loading the package, we assign the graph to a variable. We then use “+” to signal which should be next to each other, “/” to signal which should be on top, and use brackets to indicate precedence\n\n\n\nWe use facets to show variation, based on one or more variables [@grammarofgraphics, p. 219]. Facets are especially useful when we have already used color to highlight variation in some other variable. For instance, we may be interested to explain vote, by age and gender (Figure 6). We rotate the x-axis with guides(x = guide_axis(angle = 90)) to avoid overlapping. We also change the position of the legend with theme(legend.position = \"bottom\").\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 6: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\n\n\n\n\nWe could change facet_wrap() to wrap vertically instead of horizontally with dir = \"v\". Alternatively, we could specify a few rows, say nrow = 2, or a number of columns, say ncol = 2.\nBy default, both facets will have the same x-axis and y-axis. We could enable both facets to have different scales with scales = \"free\", or just the x-axis with scales = \"free_x\", or just the y-axis with scales = \"free_y\" (Figure 7).\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote), scales = \"free\") +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 7: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\n\n\n\n\nFinally, we can change the labels of the facets using labeller() (Figure 8).\n\nnew_labels &lt;- \n  c(\"0\" = \"No knowledge\", \"1\" = \"Low knowledge\",\n    \"2\" = \"Moderate knowledge\", \"3\" = \"High knowledge\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(\n    vars(political_knowledge),\n    scales = \"free\",\n    labeller = labeller(political_knowledge = new_labels)\n  ) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 8: Distribution of age-group by political knowledge, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\n\nWe now have three ways to combine multiple graphs: sub-figures, facets, and patchwork. They are useful in different circumstances:\n\nsub-figures—which we covered in ?@sec-reproducible-workflows—for when we are considering different variables;\nfacets for when we are considering a categorical variable; and\npatchwork for when we are interested in bringing together entirely different graphs.\n\n\n\n\nWe now turn to the colors used in the graph. There are a variety of different ways to change the colors. The many palettes available from RColorBrewer [@RColorBrewer] can be specified using scale_fill_brewer(). In the case of viridis [@viridis] we can specify the palettes using scale_fill_viridis_d(). Additionally, viridis is particularly focused on color-blind palettes (Figure 9). Neither RColorBrewer nor viridis need to be explicitly installed or loaded because ggplot2, which is part of the tidyverse, takes care of that for us.\n\n\n\n# Panel (a)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Blues\")\n\n# Panel (b)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Panel (c)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d()\n\n# Panel (d)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 9: Distribution of age-group and vote preference, in the 1997-2001 British Election Panel Study, illustrating different colors\n\n\n\n\n\n\nWe are often interested in the relationship between two numeric or continuous variables. We can use scatterplots to show this. A scatterplot may not always be the best choice, but it is rarely a bad one [@weissgerber2015beyond]. Some consider it the most versatile and useful graph option [@historyofdataviz, p. 121]. To illustrate scatterplots, we install and load WDI and then use that to download some economic indicators from the World Bank. In particular, we use WDIsearch() to find the unique key that we need to pass to WDI() to facilitate the download.\n\n\n\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nFrom @EssentialMacroAggregates [p. 15] Gross Domestic Product (GDP) “combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country’s economic territory.” The modern concept was developed by the twentieth century economist Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the economic activity of a country. It is useful and informative that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and disaggregated differences can be important [@Moyer2020Measuring]. It highlights short term economic progress over longer term improvements. And “the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable” [@NationalIncomeAndItsComposition, p. xxvi]. Summary measures of economic performance shows only one side of a country’s economy. While there are many strengths there are also well-known areas where GDP is weak.\n\n\n\n\n\n\nWDIsearch(\"gdp growth\")\nWDIsearch(\"inflation\")\nWDIsearch(\"population, total\")\nWDIsearch(\"Unemployment, total\")\n\n\n\n\n\nworld_bank_data &lt;-\n  WDI(\n    indicator =\n      c(\"FP.CPI.TOTL.ZG\", \"NY.GDP.MKTP.KD.ZG\", \"SP.POP.TOTL\",\"SL.UEM.TOTL.NE.ZS\"),\n    country = c(\"AU\", \"ET\", \"IN\", \"US\")\n  )\n\n\n\n\nWe may like to change the variable names to be more meaningful, and only keep those that we need.\n\nworld_bank_data &lt;-\n  world_bank_data |&gt;\n  rename(\n    inflation = FP.CPI.TOTL.ZG,\n    gdp_growth = NY.GDP.MKTP.KD.ZG,\n    population = SP.POP.TOTL,\n    unem_rate = SL.UEM.TOTL.NE.ZS\n  ) |&gt;\n  select(country, year, inflation, gdp_growth, population, unem_rate)\n\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unem_rate\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia  1960     3.73       NA      10276477        NA\n2 Australia  1961     2.29        2.48   10483000        NA\n3 Australia  1962    -0.319       1.29   10742000        NA\n4 Australia  1963     0.641       6.22   10950000        NA\n5 Australia  1964     2.87        6.98   11167000        NA\n6 Australia  1965     3.41        5.98   11388000        NA\n\n\n\n\n\nTo get started we can use geom_point() to make a scatterplot showing GDP growth and inflation, by country (Figure 10 (a)).\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default settings\n\n\n\n\n\n\n\n\n\n\n\n(b) With the addition of a theme and labels\n\n\n\n\n\n\n\nFigure 10: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nAs with bar charts, we can change the theme, and update the labels (Figure 10 (b)).\n\n\n\nFor scatterplots we use “color” instead of “fill”, as we did for bar charts, because they use dots rather than bars. This also then slightly affects how we change the palette (Figure 11). That said, with particular types of dots, for instance shape = 21, it is possible to have both fill and color aesthetics.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Blues\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d()\n\n# Panel (d)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 11: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nThe points of a scatterplot sometimes overlap. We can address this situation in a variety of ways (Figure 12):\n\nAdding a degree of transparency to our dots with “alpha” (Figure 12 (a)). The value for “alpha” can vary between 0, which is fully transparent, and 1, which is completely opaque.\nAdding a small amount of noise, which slightly moves the points, using geom_jitter() (Figure 12 (b)). By default, the movement is uniform in both directions, but we can specify which direction movement occurs with “width” or “height”. The decision between these two options turns on the degree to which accuracy matters, and the number of points: it is often useful to use geom_jitter() when you want to highlight the relative density of points and not necessarily the exact value of individual points. When using geom_jitter() it is a good idea to set a seed, as introduced in ?@sec-fire-hose, for reproducibility.\n\n\n\n\nset.seed(853)\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country )) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter(width = 1, height = 1) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Changing the alpha setting\n\n\n\n\n\n\n\n\n\n\n\n(b) Using jitter\n\n\n\n\n\n\n\nFigure 12: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nWe often use scatterplots to illustrate a relationship between two continuous variables. It can be useful to add a “summary” line using geom_smooth() (Figure 13). We can specify the relationship using “method”, change the color with “color”, and add or remove standard errors with “se”. A commonly used “method” is lm, which computes and plots a simple linear regression line similar to using the lm() function. Using geom_smooth() adds a layer to the graph, and so it inherits aesthetics from ggplot(). For instance, that is why we have one line for each country in Figure 13 (a) and Figure 13 (b). We could overwrite that by specifying a particular color (Figure 13 (c)). There are situation where other types of fitted lines such as splines might be preferred.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default line of best fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Specifying a linear relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Specifying only one color\n\n\n\n\n\n\n\nFigure 13: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nWe can use a line plot when we have variables that should be joined together, for instance, an economic time series. We will continue with the dataset from the World Bank and focus on GDP growth in the United States using geom_line() (Figure 14 (a)). The source of the data can be added to the graph using “caption” within labs().\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using a line plot\n\n\n\n\n\n\n\n\n\n\n\n(b) Using a stairstep line plot\n\n\n\n\n\n\n\nFigure 14: United States GDP growth (1961-2020)\n\n\n\n\n\n\nWe can use geom_step(), a slight variant of geom_line(), to focus attention on the change from year to year (Figure 14 (b)).\nThe Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the United Kingdom between 1861 and 1957 [@phillips1958relation]. We have a variety of ways to investigate this relationship in our data, including:\n\nAdding a second line to our graph. For instance, we could add inflation (Figure 15 (a)). This requires us to use pivot_longer(), which is discussed in ?@sec-r-essentials, to ensure that the data are in a tidy format.\nUsing geom_path() to link values in the order they appear in the dataset. In Figure 15 (b) we show a Phillips curve for the United States between 1960 and 2020. Figure 15 (b) does not appear to show any clear relationship between unemployment and inflation.\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  select(-population, -gdp_growth) |&gt;\n  pivot_longer(\n    cols = c(\"inflation\", \"unem_rate\"),\n    names_to = \"series\",\n    values_to = \"value\"\n  ) |&gt;\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Year\", y = \"Value\", color = \"Economic indicator\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = unem_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(\n    x = \"Unemployment rate\", y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparing the two time series over time\n\n\n\n\n\n\n\n\n\n\n\n(b) Plotting the two time series against each other\n\n\n\n\n\n\n\nFigure 15: Unemployment and inflation for the United States (1960-2020)\n\n\n\n\n\n\nA histogram is useful to show the shape of the distribution of a continuous variable. The full range of the data values is split into intervals called “bins” and the histogram counts how many observations fall into which bin. In Figure 16 we examine the distribution of GDP in Ethiopia.\n\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\nFigure 16: Distribution of GDP growth in Ethiopia (1960-2020)\n\n\n\n\n\n\n\n\nThe key component that determines the shape of a histogram is the number of bins. This can be specified in one of two ways (Figure 17):\n\nspecifying the number of “bins” to include; or\nspecifying their “binwidth”.\n\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (d)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Five bins\n\n\n\n\n\n\n\n\n\n\n\n(b) 20 bins\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Binwidth of two\n\n\n\n\n\n\n\n\n\n\n\n(d) Binwidth of five\n\n\n\n\n\n\n\nFigure 17: Distribution of GDP growth in Ethiopia (1960-2020)\n\n\n\n\n\n\nHistograms can be thought of as locally averaging data, and the number of bins affects how much of this occurs. When there are only two bins then there is considerable smoothing, but we lose a lot of accuracy. Too few bins results in more bias, while too many bins results in more variance [@wasserman, p. 303]. Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal [@elementsofgraphingdata, p. 135]. This is one of the reasons that @Denby2009 consider histograms to be especially valuable as exploratory tools.\n\n\n\nFinally, while we can use “fill” to distinguish between different types of observations, it can get quite messy. It is usually better to:\n\ntrace the outline of the distribution with geom_freqpoly() (Figure 18 (a))\nbuild stack of dots with geom_dotplot() (Figure 18 (b)); or\nadd transparency, especially if the differences are more stark (Figure 18 (c)).\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = \"histodot\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country %in% c(\"India\", \"United States\")) |&gt;\n  ggplot(mapping = aes(x = gdp_growth, fill = country)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tracing the outline\n\n\n\n\n\n\n\n\n\n\n\n(b) Using dots\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding transparency\n\n\n\n\n\n\n\nFigure 18: Distribution of GDP growth across various countries (1960-2020)\n\n\n\n\n\n\nAn interesting alternative to a histogram is the empirical cumulative distribution function (ECDF). The choice between this and a histogram is tends to be audience-specific. It may not appropriate for less-sophisticated audiences, but if the audience is quantitatively comfortable, then it can be a great choice because it does less smoothing than a histogram. We can build an ECDF with stat_ecdf(). For instance, Figure 19 shows an ECDF equivalent to Figure 16.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  stat_ecdf(geom = \"point\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Proportion\", color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 19: Distribution of GDP growth in four countries (1960-2020)\n\n\n\n\n\n\n\n\nA boxplot typically shows five aspects: 1) the median, 2) the 25th, and 3) 75th percentiles. The fourth and fifth elements differ depending on specifics. One option is the minimum and maximum values. Another option is to determine the difference between the 75th and 25th percentiles, which is the interquartile range (IQR). The fourth and fifth elements are then the extreme observations within \\(1.5\\times\\mbox{IQR}\\) from the 25th and 75th percentiles. That latter approach is used, by default, in geom_boxplot from ggplot2. @chartingstatistics [p. 166] introduced the notion of a chart that focused on the range and various summary statistics including the median and the range, while @tukeyeda focused on which summary statistics and popularized it [@anotherhadleyreferencelol].\nOne reason for using graphs is that they help us understand and embrace how complex our data are, rather than trying to hide and smooth it away [@armstrongembracecomplexity]. One appropriate use case for boxplots is to compare the summary statistics of many variables at once, such as in @Bethlehem2022. But boxplots alone are rarely the best choice because they hide the distribution of data, rather than show it. The same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. The first contains draws from two beta distributions: one that is right skewed and another that is left skewed. The second contains draws from a beta distribution with no skew, noting that \\(\\mbox{Beta}(1, 1)\\) is equivalent to \\(\\mbox{Uniform}(0, 1)\\).\n\n\n\n\nset.seed(853)\n\nnumber_of_draws &lt;- 10000\n\nboth_left_and_right_skew &lt;-\n  c(\n    rbeta(number_of_draws / 2, 5, 2),\n    rbeta(number_of_draws / 2, 2, 5)\n  )\n\nno_skew &lt;-\n  rbeta(number_of_draws, 1, 1)\n\nbeta_distributions &lt;-\n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(\n      rep(\"Left and right skew\", number_of_draws),\n      rep(\"No skew\", number_of_draws)\n    )\n  )\n\n\n\n\nWe can first compare the boxplots of the two series (Figure 20 (a)). But if we plot the actual data then we can see how different they are (Figure 20 (b)).\nbeta_distributions |&gt;\n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\n\nbeta_distributions |&gt;\n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Illustrated with a boxplot\n\n\n\n\n\n\n\n\n\n\n\n(b) Actual data\n\n\n\n\n\n\n\nFigure 20: Data drawn from beta distributions with different parameters\n\n\n\n\n\n\nOne way forward, if a boxplot is to be used, is to include the actual data as a layer on top of the boxplot. For instance, in Figure 21 we show the distribution of inflation across the four countries. The reason that this works well is that it shows the actual observations, as well as the summary statistics.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Country\",\n    y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\nFigure 21: Distribution of inflation data for four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#key-concepts-and-skills",
    "href": "lectures/lecture-09-content.html#key-concepts-and-skills",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Visualization is one way to get a sense of our data and to communicate this to the reader. Plotting the observations in a dataset is important.\nWe need to be comfortable with a variety of graph types, including: bar charts, scatterplots, line plots, and histograms. We can even consider a map to be a type of graph, especially after geocoding our data.\nWe should also summarize data using tables. Typical use cases for this include showing part of a dataset, summary statistics, and regression results."
  },
  {
    "objectID": "lectures/lecture-09-content.html#software-and-packages",
    "href": "lectures/lecture-09-content.html#software-and-packages",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Base R [@citeR]\ncarData [@carData]\ndatasauRus [@citedatasauRus]\nggmap [@KahleWickham2013]\njanitor [@janitor]\nknitr [@citeknitr]\nmaps [@citemaps]\nmapproj [@mapproj]\nmodelsummary [@citemodelsummary]\nopendatatoronto [@citeSharla]\npatchwork [@citepatchwork]\ntidygeocoder [@tidygeocoder]\ntidyverse [@tidyverse]\ntroopdata [@troopdata]\nWDI [@WDI]\n\n\n\n\n\nlibrary(carData)\nlibrary(datasauRus)\nlibrary(ggmap)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(modelsummary)\nlibrary(opendatatoronto)\nlibrary(patchwork)\nlibrary(tidygeocoder)\nlibrary(tidyverse)\nlibrary(troopdata)\nlibrary(WDI)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#introduction",
    "href": "lectures/lecture-09-content.html#introduction",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "When telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.\nTry to show the observations that underpin our analysis. For instance, if your dataset consists of 2,500 responses to a survey, then at some point in the paper you should have a plot/s that contains each of the 2,500 observations, for every variable of interest. To do this we build graphs using ggplot2 which is part of the core tidyverse and so does not have to be installed or loaded separately. In this chapter we go through a variety of different options including bar charts, scatterplots, line plots, and histograms.\nIn contrast to the role of graphs, which is to show each observation, the role of tables is typically to show an extract of the dataset or to convey various summary statistics, or regression results. We will build tables primarily using knitr. Later we will use modelsummary to build tables related to regression output.\nFinally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using ggmap after having obtained geocoded data using tidygeocoder."
  },
  {
    "objectID": "lectures/lecture-09-content.html#graphs",
    "href": "lectures/lecture-09-content.html#graphs",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Graphs are a critical aspect of compelling data stories. They allow us to see both broad patterns and details [@elementsofgraphingdata, p. 5]. Graphs enable a familiarity with our data that is hard to get from any other method. Every variable of interest should be graphed.\nThe most important objective of a graph is to convey as much of the actual data, and its context, as possible. In a way, graphing is an information encoding process where we construct a deliberate representation to convey information to our audience. The audience must decode that representation. The success of our graph depends on how much information is lost in this process so the decoding is a critical aspect [@elementsofgraphingdata, p. 221]. This means that we must focus on creating effective graphs that are suitable for our specific audience."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section",
    "href": "lectures/lecture-09-content.html#section",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "To see why graphing the actual data is important, after installing and loading datasauRus consider the datasaurus_dozen dataset.\n\ndatasaurus_dozen\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# ℹ 1,836 more rows\n\n\nThe dataset consists of values for “x” and “y”, which should be plotted on the x-axis and y-axis, respectively. There are 13 different values in the variable “dataset” including: “dino”, “star”, “away”, and “bullseye”. We focus on those four and generate summary statistics for each (Table 1).\n\n# Based on: https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  summarise(across(c(x, y), list(mean = mean, sd = sd)),\n            .by = dataset) |&gt;\n  kable(col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n        booktabs = TRUE, digits = 1)\n\n\n\nTable 1: Mean and standard deviation for four datasauRus datasets\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\ndino\n54.3\n16.8\n47.8\n26.9\n\n\naway\n54.3\n16.8\n47.8\n26.9\n\n\nstar\n54.3\n16.8\n47.8\n26.9\n\n\nbullseye\n54.3\n16.8\n47.8\n26.9"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-1",
    "href": "lectures/lecture-09-content.html#section-1",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Notice that the summary statistics are similar (Table 1). Despite this it turns out that the different datasets are actually very different beasts. This becomes clear when we plot the data (Figure 1).\n\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  ggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(color = \"Dataset\")\n\n\n\n\n\n\n\nFigure 1: Graph of four datasauRus datasets"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-2",
    "href": "lectures/lecture-09-content.html#section-2",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We get a similar lesson—always plot your data—from “Anscombe’s Quartet”, created by the twentieth century statistician Frank Anscombe. The key takeaway is that it is important to plot the actual data and not rely solely on summary statistics.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-3",
    "href": "lectures/lecture-09-content.html#section-3",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Anscombe’s Quartet consists of eleven observations for four different datasets, with x and y values for each observation. We need to manipulate this dataset with pivot_longer() to get it into the “tidy” format discussed in ?@sec-r-essentials.\n\n# From: https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# And the pivot_longer() vignette.\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(\n    everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-4",
    "href": "lectures/lecture-09-content.html#section-4",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We can first create summary statistics (Table 2) and then plot the data (Figure 2). This again illustrates the importance of graphing the actual data, rather than relying on summary statistics.\n\ntidy_anscombe |&gt;\n  summarise(\n    across(c(x, y), list(mean = mean, sd = sd)),\n    .by = set\n    ) |&gt;\n  kable(\n    col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n    digits = 1, booktabs = TRUE\n  )\n\n\n\nTable 2: Mean and standard deviation for Anscombe’s quartet\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\n1\n9\n3.3\n7.5\n2\n\n\n2\n9\n3.3\n7.5\n2\n\n\n3\n9\n3.3\n7.5\n2\n\n\n4\n9\n3.3\n7.5\n2"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-5",
    "href": "lectures/lecture-09-content.html#section-5",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "tidy_anscombe |&gt;\n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Recreation of Anscombe’s Quartet"
  },
  {
    "objectID": "lectures/lecture-09-content.html#bar-charts",
    "href": "lectures/lecture-09-content.html#bar-charts",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in ?@sec-fire-hose when we constructed a graph of the number of occupied beds. The geometric object—a “geom”—that we primarily use is geom_bar(), but there are many variants to cater for specific situations. To illustrate the use of bar charts, we use a dataset from the 1997-2001 British Election Panel Study that was put together by @fox2006effect and made available with BEPS, after installing and loading carData.\n\nbeps &lt;- \n  BEPS |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  select(age, vote, gender, political_knowledge)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-6",
    "href": "lectures/lecture-09-content.html#section-6",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "The dataset consists of which party the respondent supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondent. We begin by creating age-groups from the ages, and making a bar chart showing the frequency of each age-group using geom_bar() (Figure 3 (a)).\n\nbeps &lt;-\n  beps |&gt;\n  mutate(\n    age_group =\n      case_when(\n        age &lt; 35 ~ \"&lt;35\",\n        age &lt; 50 ~ \"35-49\",\n        age &lt; 65 ~ \"50-64\",\n        age &lt; 80 ~ \"65-79\",\n        age &lt; 100 ~ \"80-99\"\n      ),\n    age_group = \n      factor(age_group, levels = c(\"&lt;35\", \"35-49\", \"50-64\", \"65-79\", \"80-99\"))\n  )"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-7",
    "href": "lectures/lecture-09-content.html#section-7",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\nbeps |&gt; \n  count(age_group) |&gt; \n  ggplot(mapping = aes(x = age_group, y = n)) +\n  geom_col() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using count() and geom_col()\n\n\n\n\n\n\n\nFigure 3: Distribution of age-groups in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-8",
    "href": "lectures/lecture-09-content.html#section-8",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "The default axis label used by ggplot2 is the name of the relevant variable, so it is often useful to add more detail. We do this using labs() by specifying a variable and a name. In the case of Figure 3 (a) we have specified labels for the x-axis and y-axis.\nBy default, geom_bar() creates a count of the number of times each age-group appears in the dataset. It does this because the default statistical transformation—a “stat”—for geom_bar() is “count”, which saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with beps |&gt; count(age_group)), then we could specify a variable for the y-axis and then use geom_col() (Figure 3 (b))."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-9",
    "href": "lectures/lecture-09-content.html#section-9",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We may also like to consider various groupings of the data to get a different insight. For instance, we can use color to look at which party the respondent supports, by age-group (Figure 4 (a)).\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge2\") +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using geom_bar() with dodge2\n\n\n\n\n\n\n\nFigure 4: Distribution of age-group, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\nBy default, these different groups are stacked, but they can be placed side by side with position = \"dodge2\" (Figure 4 (b)). (Using “dodge2” rather than “dodge” adds a little space between the bars.)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#themes",
    "href": "lectures/lecture-09-content.html#themes",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "At this point, we may like to address the general look of the graph. There are various themes that are built into ggplot2. These include: theme_bw(), theme_classic(), theme_dark(), and theme_minimal(). A full list is available in the ggplot2 cheat sheet. We can use these themes by adding them as a layer (Figure 5). We could also install more themes from other packages, including ggthemes [@ggthemes], and hrbrthemes [@hrbrthemes]. We could even build our own!\n\ntheme_bw &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\n\n\n\n\n\n\n\nFigure 5: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes and the use of patchwork\n\n\n\n\n\nIn Figure 5 we use patchwork to bring together multiple graphs. To do this, after installing and loading the package, we assign the graph to a variable. We then use “+” to signal which should be next to each other, “/” to signal which should be on top, and use brackets to indicate precedence"
  },
  {
    "objectID": "lectures/lecture-09-content.html#facets",
    "href": "lectures/lecture-09-content.html#facets",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We use facets to show variation, based on one or more variables [@grammarofgraphics, p. 219]. Facets are especially useful when we have already used color to highlight variation in some other variable. For instance, we may be interested to explain vote, by age and gender (Figure 6). We rotate the x-axis with guides(x = guide_axis(angle = 90)) to avoid overlapping. We also change the position of the legend with theme(legend.position = \"bottom\").\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 6: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-10",
    "href": "lectures/lecture-09-content.html#section-10",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We could change facet_wrap() to wrap vertically instead of horizontally with dir = \"v\". Alternatively, we could specify a few rows, say nrow = 2, or a number of columns, say ncol = 2.\nBy default, both facets will have the same x-axis and y-axis. We could enable both facets to have different scales with scales = \"free\", or just the x-axis with scales = \"free_x\", or just the y-axis with scales = \"free_y\" (Figure 7).\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote), scales = \"free\") +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 7: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-11",
    "href": "lectures/lecture-09-content.html#section-11",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Finally, we can change the labels of the facets using labeller() (Figure 8).\n\nnew_labels &lt;- \n  c(\"0\" = \"No knowledge\", \"1\" = \"Low knowledge\",\n    \"2\" = \"Moderate knowledge\", \"3\" = \"High knowledge\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(\n    vars(political_knowledge),\n    scales = \"free\",\n    labeller = labeller(political_knowledge = new_labels)\n  ) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 8: Distribution of age-group by political knowledge, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\n\nWe now have three ways to combine multiple graphs: sub-figures, facets, and patchwork. They are useful in different circumstances:\n\nsub-figures—which we covered in ?@sec-reproducible-workflows—for when we are considering different variables;\nfacets for when we are considering a categorical variable; and\npatchwork for when we are interested in bringing together entirely different graphs."
  },
  {
    "objectID": "lectures/lecture-09-content.html#colors",
    "href": "lectures/lecture-09-content.html#colors",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We now turn to the colors used in the graph. There are a variety of different ways to change the colors. The many palettes available from RColorBrewer [@RColorBrewer] can be specified using scale_fill_brewer(). In the case of viridis [@viridis] we can specify the palettes using scale_fill_viridis_d(). Additionally, viridis is particularly focused on color-blind palettes (Figure 9). Neither RColorBrewer nor viridis need to be explicitly installed or loaded because ggplot2, which is part of the tidyverse, takes care of that for us."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-12",
    "href": "lectures/lecture-09-content.html#section-12",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "# Panel (a)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Blues\")\n\n# Panel (b)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Panel (c)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d()\n\n# Panel (d)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 9: Distribution of age-group and vote preference, in the 1997-2001 British Election Panel Study, illustrating different colors"
  },
  {
    "objectID": "lectures/lecture-09-content.html#scatterplots",
    "href": "lectures/lecture-09-content.html#scatterplots",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We are often interested in the relationship between two numeric or continuous variables. We can use scatterplots to show this. A scatterplot may not always be the best choice, but it is rarely a bad one [@weissgerber2015beyond]. Some consider it the most versatile and useful graph option [@historyofdataviz, p. 121]. To illustrate scatterplots, we install and load WDI and then use that to download some economic indicators from the World Bank. In particular, we use WDIsearch() to find the unique key that we need to pass to WDI() to facilitate the download."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-13",
    "href": "lectures/lecture-09-content.html#section-13",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Oh, you think we have good data on that!\n\n\n\nFrom @EssentialMacroAggregates [p. 15] Gross Domestic Product (GDP) “combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country’s economic territory.” The modern concept was developed by the twentieth century economist Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the economic activity of a country. It is useful and informative that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and disaggregated differences can be important [@Moyer2020Measuring]. It highlights short term economic progress over longer term improvements. And “the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable” [@NationalIncomeAndItsComposition, p. xxvi]. Summary measures of economic performance shows only one side of a country’s economy. While there are many strengths there are also well-known areas where GDP is weak."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-14",
    "href": "lectures/lecture-09-content.html#section-14",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "WDIsearch(\"gdp growth\")\nWDIsearch(\"inflation\")\nWDIsearch(\"population, total\")\nWDIsearch(\"Unemployment, total\")"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-15",
    "href": "lectures/lecture-09-content.html#section-15",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "world_bank_data &lt;-\n  WDI(\n    indicator =\n      c(\"FP.CPI.TOTL.ZG\", \"NY.GDP.MKTP.KD.ZG\", \"SP.POP.TOTL\",\"SL.UEM.TOTL.NE.ZS\"),\n    country = c(\"AU\", \"ET\", \"IN\", \"US\")\n  )"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-16",
    "href": "lectures/lecture-09-content.html#section-16",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We may like to change the variable names to be more meaningful, and only keep those that we need.\n\nworld_bank_data &lt;-\n  world_bank_data |&gt;\n  rename(\n    inflation = FP.CPI.TOTL.ZG,\n    gdp_growth = NY.GDP.MKTP.KD.ZG,\n    population = SP.POP.TOTL,\n    unem_rate = SL.UEM.TOTL.NE.ZS\n  ) |&gt;\n  select(country, year, inflation, gdp_growth, population, unem_rate)\n\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unem_rate\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia  1960     3.73       NA      10276477        NA\n2 Australia  1961     2.29        2.48   10483000        NA\n3 Australia  1962    -0.319       1.29   10742000        NA\n4 Australia  1963     0.641       6.22   10950000        NA\n5 Australia  1964     2.87        6.98   11167000        NA\n6 Australia  1965     3.41        5.98   11388000        NA"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-17",
    "href": "lectures/lecture-09-content.html#section-17",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "To get started we can use geom_point() to make a scatterplot showing GDP growth and inflation, by country (Figure 10 (a)).\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default settings\n\n\n\n\n\n\n\n\n\n\n\n(b) With the addition of a theme and labels\n\n\n\n\n\n\n\nFigure 10: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nAs with bar charts, we can change the theme, and update the labels (Figure 10 (b))."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-18",
    "href": "lectures/lecture-09-content.html#section-18",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "For scatterplots we use “color” instead of “fill”, as we did for bar charts, because they use dots rather than bars. This also then slightly affects how we change the palette (Figure 11). That said, with particular types of dots, for instance shape = 21, it is possible to have both fill and color aesthetics.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Blues\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d()\n\n# Panel (d)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 11: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-19",
    "href": "lectures/lecture-09-content.html#section-19",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "The points of a scatterplot sometimes overlap. We can address this situation in a variety of ways (Figure 12):\n\nAdding a degree of transparency to our dots with “alpha” (Figure 12 (a)). The value for “alpha” can vary between 0, which is fully transparent, and 1, which is completely opaque.\nAdding a small amount of noise, which slightly moves the points, using geom_jitter() (Figure 12 (b)). By default, the movement is uniform in both directions, but we can specify which direction movement occurs with “width” or “height”. The decision between these two options turns on the degree to which accuracy matters, and the number of points: it is often useful to use geom_jitter() when you want to highlight the relative density of points and not necessarily the exact value of individual points. When using geom_jitter() it is a good idea to set a seed, as introduced in ?@sec-fire-hose, for reproducibility."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-20",
    "href": "lectures/lecture-09-content.html#section-20",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "set.seed(853)\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country )) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter(width = 1, height = 1) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Changing the alpha setting\n\n\n\n\n\n\n\n\n\n\n\n(b) Using jitter\n\n\n\n\n\n\n\nFigure 12: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-21",
    "href": "lectures/lecture-09-content.html#section-21",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We often use scatterplots to illustrate a relationship between two continuous variables. It can be useful to add a “summary” line using geom_smooth() (Figure 13). We can specify the relationship using “method”, change the color with “color”, and add or remove standard errors with “se”. A commonly used “method” is lm, which computes and plots a simple linear regression line similar to using the lm() function. Using geom_smooth() adds a layer to the graph, and so it inherits aesthetics from ggplot(). For instance, that is why we have one line for each country in Figure 13 (a) and Figure 13 (b). We could overwrite that by specifying a particular color (Figure 13 (c)). There are situation where other types of fitted lines such as splines might be preferred.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default line of best fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Specifying a linear relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Specifying only one color\n\n\n\n\n\n\n\nFigure 13: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-content.html#line-plots",
    "href": "lectures/lecture-09-content.html#line-plots",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We can use a line plot when we have variables that should be joined together, for instance, an economic time series. We will continue with the dataset from the World Bank and focus on GDP growth in the United States using geom_line() (Figure 14 (a)). The source of the data can be added to the graph using “caption” within labs().\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using a line plot\n\n\n\n\n\n\n\n\n\n\n\n(b) Using a stairstep line plot\n\n\n\n\n\n\n\nFigure 14: United States GDP growth (1961-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-22",
    "href": "lectures/lecture-09-content.html#section-22",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We can use geom_step(), a slight variant of geom_line(), to focus attention on the change from year to year (Figure 14 (b)).\nThe Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the United Kingdom between 1861 and 1957 [@phillips1958relation]. We have a variety of ways to investigate this relationship in our data, including:\n\nAdding a second line to our graph. For instance, we could add inflation (Figure 15 (a)). This requires us to use pivot_longer(), which is discussed in ?@sec-r-essentials, to ensure that the data are in a tidy format.\nUsing geom_path() to link values in the order they appear in the dataset. In Figure 15 (b) we show a Phillips curve for the United States between 1960 and 2020. Figure 15 (b) does not appear to show any clear relationship between unemployment and inflation.\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  select(-population, -gdp_growth) |&gt;\n  pivot_longer(\n    cols = c(\"inflation\", \"unem_rate\"),\n    names_to = \"series\",\n    values_to = \"value\"\n  ) |&gt;\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Year\", y = \"Value\", color = \"Economic indicator\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = unem_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(\n    x = \"Unemployment rate\", y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparing the two time series over time\n\n\n\n\n\n\n\n\n\n\n\n(b) Plotting the two time series against each other\n\n\n\n\n\n\n\nFigure 15: Unemployment and inflation for the United States (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#histograms",
    "href": "lectures/lecture-09-content.html#histograms",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "A histogram is useful to show the shape of the distribution of a continuous variable. The full range of the data values is split into intervals called “bins” and the histogram counts how many observations fall into which bin. In Figure 16 we examine the distribution of GDP in Ethiopia.\n\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\nFigure 16: Distribution of GDP growth in Ethiopia (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-23",
    "href": "lectures/lecture-09-content.html#section-23",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "The key component that determines the shape of a histogram is the number of bins. This can be specified in one of two ways (Figure 17):\n\nspecifying the number of “bins” to include; or\nspecifying their “binwidth”.\n\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (d)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Five bins\n\n\n\n\n\n\n\n\n\n\n\n(b) 20 bins\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Binwidth of two\n\n\n\n\n\n\n\n\n\n\n\n(d) Binwidth of five\n\n\n\n\n\n\n\nFigure 17: Distribution of GDP growth in Ethiopia (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-24",
    "href": "lectures/lecture-09-content.html#section-24",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Histograms can be thought of as locally averaging data, and the number of bins affects how much of this occurs. When there are only two bins then there is considerable smoothing, but we lose a lot of accuracy. Too few bins results in more bias, while too many bins results in more variance [@wasserman, p. 303]. Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal [@elementsofgraphingdata, p. 135]. This is one of the reasons that @Denby2009 consider histograms to be especially valuable as exploratory tools."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-25",
    "href": "lectures/lecture-09-content.html#section-25",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Finally, while we can use “fill” to distinguish between different types of observations, it can get quite messy. It is usually better to:\n\ntrace the outline of the distribution with geom_freqpoly() (Figure 18 (a))\nbuild stack of dots with geom_dotplot() (Figure 18 (b)); or\nadd transparency, especially if the differences are more stark (Figure 18 (c)).\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = \"histodot\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country %in% c(\"India\", \"United States\")) |&gt;\n  ggplot(mapping = aes(x = gdp_growth, fill = country)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tracing the outline\n\n\n\n\n\n\n\n\n\n\n\n(b) Using dots\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding transparency\n\n\n\n\n\n\n\nFigure 18: Distribution of GDP growth across various countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-26",
    "href": "lectures/lecture-09-content.html#section-26",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "An interesting alternative to a histogram is the empirical cumulative distribution function (ECDF). The choice between this and a histogram is tends to be audience-specific. It may not appropriate for less-sophisticated audiences, but if the audience is quantitatively comfortable, then it can be a great choice because it does less smoothing than a histogram. We can build an ECDF with stat_ecdf(). For instance, Figure 19 shows an ECDF equivalent to Figure 16.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  stat_ecdf(geom = \"point\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Proportion\", color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 19: Distribution of GDP growth in four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#boxplots",
    "href": "lectures/lecture-09-content.html#boxplots",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "A boxplot typically shows five aspects: 1) the median, 2) the 25th, and 3) 75th percentiles. The fourth and fifth elements differ depending on specifics. One option is the minimum and maximum values. Another option is to determine the difference between the 75th and 25th percentiles, which is the interquartile range (IQR). The fourth and fifth elements are then the extreme observations within \\(1.5\\times\\mbox{IQR}\\) from the 25th and 75th percentiles. That latter approach is used, by default, in geom_boxplot from ggplot2. @chartingstatistics [p. 166] introduced the notion of a chart that focused on the range and various summary statistics including the median and the range, while @tukeyeda focused on which summary statistics and popularized it [@anotherhadleyreferencelol].\nOne reason for using graphs is that they help us understand and embrace how complex our data are, rather than trying to hide and smooth it away [@armstrongembracecomplexity]. One appropriate use case for boxplots is to compare the summary statistics of many variables at once, such as in @Bethlehem2022. But boxplots alone are rarely the best choice because they hide the distribution of data, rather than show it. The same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. The first contains draws from two beta distributions: one that is right skewed and another that is left skewed. The second contains draws from a beta distribution with no skew, noting that \\(\\mbox{Beta}(1, 1)\\) is equivalent to \\(\\mbox{Uniform}(0, 1)\\)."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-27",
    "href": "lectures/lecture-09-content.html#section-27",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "set.seed(853)\n\nnumber_of_draws &lt;- 10000\n\nboth_left_and_right_skew &lt;-\n  c(\n    rbeta(number_of_draws / 2, 5, 2),\n    rbeta(number_of_draws / 2, 2, 5)\n  )\n\nno_skew &lt;-\n  rbeta(number_of_draws, 1, 1)\n\nbeta_distributions &lt;-\n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(\n      rep(\"Left and right skew\", number_of_draws),\n      rep(\"No skew\", number_of_draws)\n    )\n  )"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-28",
    "href": "lectures/lecture-09-content.html#section-28",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We can first compare the boxplots of the two series (Figure 20 (a)). But if we plot the actual data then we can see how different they are (Figure 20 (b)).\nbeta_distributions |&gt;\n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\n\nbeta_distributions |&gt;\n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Illustrated with a boxplot\n\n\n\n\n\n\n\n\n\n\n\n(b) Actual data\n\n\n\n\n\n\n\nFigure 20: Data drawn from beta distributions with different parameters"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-29",
    "href": "lectures/lecture-09-content.html#section-29",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "One way forward, if a boxplot is to be used, is to include the actual data as a layer on top of the boxplot. For instance, in Figure 21 we show the distribution of inflation across the four countries. The reason that this works well is that it shows the actual observations, as well as the summary statistics.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Country\",\n    y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\nFigure 21: Distribution of inflation data for four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-30",
    "href": "lectures/lecture-09-content.html#section-30",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Tables are an important part of telling a compelling story. Tables can communicate less information than a graph, but they do so at a high fidelity. They are especially useful to highlight a few specific values [@andersen2021presenting]. In this book, we primarily use tables in three ways:\n\nTo show an extract of the dataset.\nTo communicate summary statistics.\nTo display regression results."
  },
  {
    "objectID": "lectures/lecture-09-content.html#showing-part-of-a-dataset",
    "href": "lectures/lecture-09-content.html#showing-part-of-a-dataset",
    "title": "Graphs, Tables, & Maps",
    "section": "Showing part of a dataset",
    "text": "Showing part of a dataset\nWe illustrate showing part of a dataset using kable() from knitr. We use the World Bank dataset that we downloaded earlier and focus on inflation, GDP growth, and population as unemployment data are not available for every year for every country.\n\nworld_bank_data &lt;- \n  world_bank_data |&gt; \n  select(-unem_rate)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-31",
    "href": "lectures/lecture-09-content.html#section-31",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "To begin, after installing and loading knitr, we can display the first ten rows with the default kable() settings.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable()\n\n\n\n\ncountry\nyear\ninflation\ngdp_growth\npopulation\n\n\n\n\nAustralia\n1960\n3.7288136\nNA\n10276477\n\n\nAustralia\n1961\n2.2875817\n2.482656\n10483000\n\n\nAustralia\n1962\n-0.3194888\n1.294611\n10742000\n\n\nAustralia\n1963\n0.6410256\n6.216107\n10950000\n\n\nAustralia\n1964\n2.8662420\n6.980061\n11167000\n\n\nAustralia\n1965\n3.4055728\n5.980438\n11388000\n\n\nAustralia\n1966\n3.2934132\n2.379040\n11651000\n\n\nAustralia\n1967\n3.4782609\n6.304945\n11799000\n\n\nAustralia\n1968\n2.5210084\n5.094034\n12009000\n\n\nAustralia\n1969\n3.2786885\n7.045584\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-32",
    "href": "lectures/lecture-09-content.html#section-32",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "To be able to cross-reference a table in the text, we need to add a table caption and label to the R chunk as shown in ?@sec-quartocrossreferences of ?@sec-reproducible-workflows. We can also make the column names more informative with “col.names” and specify the number of digits to be displayed (Table 3).\n\n```{r}\n#| label: tbl-gdpfirst\n#| message: false\n#| tbl-cap: \"A dataset of economic indicators for four countries\"\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1\n  )\n```\n\n\n\nTable 3: A dataset of economic indicators for four countries\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-33",
    "href": "lectures/lecture-09-content.html#section-33",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Improving the formatting\nWhen producing PDFs, the “booktabs” option makes a host of small changes to the default display and results in tables that look better (Table 4). (This should not have an effect for HTML output.) By default a small space will be added every five lines. We can additionally specify “linesep” to stop that.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 4: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-34",
    "href": "lectures/lecture-09-content.html#section-34",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We can specify the alignment of the columns using a character vector of “l” (left), “c” (center), and “r” (right) (Table 5). Additionally, we can change the formatting. For instance, we could specify groupings for numbers that are at least 1,000 using format.args = list(big.mark = \",\").\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  mutate(year = as.factor(year)) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\",\n    align = c(\"l\", \"l\", \"c\", \"c\", \"r\", \"r\"),\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 5: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10,276,477\n\n\nAustralia\n1961\n2.3\n2.5\n10,483,000\n\n\nAustralia\n1962\n-0.3\n1.3\n10,742,000\n\n\nAustralia\n1963\n0.6\n6.2\n10,950,000\n\n\nAustralia\n1964\n2.9\n7.0\n11,167,000\n\n\nAustralia\n1965\n3.4\n6.0\n11,388,000\n\n\nAustralia\n1966\n3.3\n2.4\n11,651,000\n\n\nAustralia\n1967\n3.5\n6.3\n11,799,000\n\n\nAustralia\n1968\n2.5\n5.1\n12,009,000\n\n\nAustralia\n1969\n3.3\n7.0\n12,263,000"
  },
  {
    "objectID": "lectures/lecture-09-content.html#communicating-summary-statistics",
    "href": "lectures/lecture-09-content.html#communicating-summary-statistics",
    "title": "Graphs, Tables, & Maps",
    "section": "Communicating summary statistics",
    "text": "Communicating summary statistics\nAfter installing and loading modelsummary we can use datasummary_skim() to create tables of summary statistics from our dataset.\nWe can use this to get a table such as Table 6. That might be useful for exploratory data analysis, which we cover in ?@sec-exploratory-data-analysis. (Here we remove population to save space and do not include a histogram of each variable.)\n\nworld_bank_data |&gt;\n  select(-population) |&gt; \n  datasummary_skim(histogram = FALSE)\n\n\n\nTable 6: Summary of economic indicator variables for four countries\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n              \n        \n        \n        \n                \n                  year\n                  62\n                  0\n                  1990.5\n                  17.9\n                  1960.0\n                  1990.5\n                  2021.0\n                \n                \n                  inflation\n                  243\n                  2\n                  6.1\n                  6.5\n                  -9.8\n                  4.3\n                  44.4\n                \n                \n                  gdp_growth\n                  224\n                  10\n                  4.2\n                  3.7\n                  -11.1\n                  3.9\n                  13.9\n                \n                \n                  country\n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Australia\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Ethiopia\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  India\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  United States\n                  62\n                  25.0"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-35",
    "href": "lectures/lecture-09-content.html#section-35",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "By default, datasummary_skim() summarizes the numeric variables, but we can ask for the categorical variables (Table 7). Additionally we can add cross-references in the same way as kable(), that is, include a “tbl-cap” entry and then cross-reference the name of the R chunk.\n\nworld_bank_data |&gt;\n  datasummary_skim(type = \"categorical\")\n\n\n\nTable 7: Summary of categorical economic indicator variables for four countries\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                country\n                N\n                %\n              \n        \n        \n        \n                \n                  Australia    \n                  62\n                  25.0\n                \n                \n                  Ethiopia     \n                  62\n                  25.0\n                \n                \n                  India        \n                  62\n                  25.0\n                \n                \n                  United States\n                  62\n                  25.0"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-36",
    "href": "lectures/lecture-09-content.html#section-36",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We can create a table that shows the correlation between variables using datasummary_correlation() (Table 8).\n\nworld_bank_data |&gt;\n  datasummary_correlation()\n\n\n\nTable 8: Correlation between the economic indicator variables for four countries (Australia, Ethiopia, India, and the United States)\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                year\n                inflation\n                gdp_growth\n                population\n              \n        \n        \n        \n                \n                  year      \n                  1  \n                  .  \n                  .  \n                  .\n                \n                \n                  inflation \n                  .03\n                  1  \n                  .  \n                  .\n                \n                \n                  gdp_growth\n                  .11\n                  .01\n                  1  \n                  .\n                \n                \n                  population\n                  .25\n                  .06\n                  .16\n                  1"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-37",
    "href": "lectures/lecture-09-content.html#section-37",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We typically need a table of descriptive statistics that we could add to our paper (Table 9). This contrasts with Table 7 which would likely not be included in the main section of a paper, and is more to help us understand the data. We can add a note about the source of the data using notes.\n\ndatasummary_balance(\n  formula = ~country,\n  data = world_bank_data |&gt; \n    filter(country %in% c(\"Australia\", \"Ethiopia\")),\n  dinm = FALSE,\n  notes = \"Data source: World Bank.\"\n)\n\n\n\nTable 9: Descriptive statistics for the inflation and GDP dataset\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nAustralia (N=62)\nEthiopia (N=62)\n\n        \n              \n                 \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n              \n        \n        Data source: World Bank.\n        \n                \n                  year      \n                  1990.5    \n                  18.0     \n                  1990.5    \n                  18.0      \n                \n                \n                  inflation \n                  4.7       \n                  3.8      \n                  9.1       \n                  10.6      \n                \n                \n                  gdp_growth\n                  3.4       \n                  1.8      \n                  5.9       \n                  6.4       \n                \n                \n                  population\n                  17351313.1\n                  4407899.0\n                  57185292.0\n                  29328845.8"
  },
  {
    "objectID": "lectures/lecture-09-content.html#display-regression-results",
    "href": "lectures/lecture-09-content.html#display-regression-results",
    "title": "Graphs, Tables, & Maps",
    "section": "Display regression results",
    "text": "Display regression results\nWe can report regression results using modelsummary() from modelsummary. For instance, we could display the estimates from a few different models (Table 10).\n\nfirst_model &lt;- lm(\n  formula = gdp_growth ~ inflation,\n  data = world_bank_data\n)\n\nsecond_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country,\n  data = world_bank_data\n)\n\nthird_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country + population,\n  data = world_bank_data\n)\n\nmodelsummary(list(first_model, second_model, third_model))\n\n\n\nTable 10: Explaining GDP as a function of inflation\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  4.147   \n                  3.676   \n                  3.611   \n                \n                \n                                      \n                  (0.343) \n                  (0.484) \n                  (0.482) \n                \n                \n                  inflation           \n                  0.006   \n                  -0.068  \n                  -0.065  \n                \n                \n                                      \n                  (0.039) \n                  (0.040) \n                  (0.039) \n                \n                \n                  countryEthiopia     \n                          \n                  2.896   \n                  2.716   \n                \n                \n                                      \n                          \n                  (0.740) \n                  (0.740) \n                \n                \n                  countryIndia        \n                          \n                  1.916   \n                  -0.730  \n                \n                \n                                      \n                          \n                  (0.642) \n                  (1.465) \n                \n                \n                  countryUnited States\n                          \n                  -0.436  \n                  -1.145  \n                \n                \n                                      \n                          \n                  (0.633) \n                  (0.722) \n                \n                \n                  population          \n                          \n                          \n                  0.000   \n                \n                \n                                      \n                          \n                          \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  223     \n                  223     \n                  223     \n                \n                \n                  R2                  \n                  0.000   \n                  0.111   \n                  0.127   \n                \n                \n                  R2 Adj.             \n                  -0.004  \n                  0.095   \n                  0.107   \n                \n                \n                  AIC                 \n                  1217.7  \n                  1197.5  \n                  1195.4  \n                \n                \n                  BIC                 \n                  1227.9  \n                  1217.9  \n                  1219.3  \n                \n                \n                  Log.Lik.            \n                  -605.861\n                  -592.752\n                  -590.704\n                \n                \n                  F                   \n                  0.024   \n                  6.806   \n                          \n                \n                \n                  RMSE                \n                  3.66    \n                  3.45    \n                  3.42    \n                \n        \n      \n    \n\n\n\n\n\n\nThe number of significant digits can be adjusted with “fmt” (Table 11). To help establish credibility you should generally not add as many significant digits as possible [@howes2022representing]. Instead, you should think carefully about the data-generating process and adjust based on that."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-38",
    "href": "lectures/lecture-09-content.html#section-38",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "modelsummary(\n  list(first_model, second_model, third_model),\n  fmt = 1\n)\n\n\n\nTable 11: Three models of GDP as a function of inflation\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  4.1     \n                  3.7     \n                  3.6     \n                \n                \n                                      \n                  (0.3)   \n                  (0.5)   \n                  (0.5)   \n                \n                \n                  inflation           \n                  0.0     \n                  -0.1    \n                  -0.1    \n                \n                \n                                      \n                  (0.0)   \n                  (0.0)   \n                  (0.0)   \n                \n                \n                  countryEthiopia     \n                          \n                  2.9     \n                  2.7     \n                \n                \n                                      \n                          \n                  (0.7)   \n                  (0.7)   \n                \n                \n                  countryIndia        \n                          \n                  1.9     \n                  -0.7    \n                \n                \n                                      \n                          \n                  (0.6)   \n                  (1.5)   \n                \n                \n                  countryUnited States\n                          \n                  -0.4    \n                  -1.1    \n                \n                \n                                      \n                          \n                  (0.6)   \n                  (0.7)   \n                \n                \n                  population          \n                          \n                          \n                  0.0     \n                \n                \n                                      \n                          \n                          \n                  (0.0)   \n                \n                \n                  Num.Obs.            \n                  223     \n                  223     \n                  223     \n                \n                \n                  R2                  \n                  0.000   \n                  0.111   \n                  0.127   \n                \n                \n                  R2 Adj.             \n                  -0.004  \n                  0.095   \n                  0.107   \n                \n                \n                  AIC                 \n                  1217.7  \n                  1197.5  \n                  1195.4  \n                \n                \n                  BIC                 \n                  1227.9  \n                  1217.9  \n                  1219.3  \n                \n                \n                  Log.Lik.            \n                  -605.861\n                  -592.752\n                  -590.704\n                \n                \n                  F                   \n                  0.024   \n                  6.806   \n                          \n                \n                \n                  RMSE                \n                  3.66    \n                  3.45    \n                  3.42"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-39",
    "href": "lectures/lecture-09-content.html#section-39",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "In many ways maps can be thought of as another type of graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or background image. It is possible that they are the oldest and best understood type of chart [@karsetn, p. 1]. We can generate a map in a straight-forward manner. That said, it is not to be taken lightly; things quickly get complicated!\nThe first step is to get some data. There is some geographic data built into ggplot2 that we can access with map_data(). There are additional variables in the world.cities dataset from maps.\n\nfrance &lt;- map_data(map = \"france\")\n\nhead(france)\n\n      long      lat group order region subregion\n1 2.557093 51.09752     1     1   Nord      &lt;NA&gt;\n2 2.579995 51.00298     1     2   Nord      &lt;NA&gt;\n3 2.609101 50.98545     1     3   Nord      &lt;NA&gt;\n4 2.630782 50.95073     1     4   Nord      &lt;NA&gt;\n5 2.625894 50.94116     1     5   Nord      &lt;NA&gt;\n6 2.597699 50.91967     1     6   Nord      &lt;NA&gt;\n\nfrench_cities &lt;-\n  world.cities |&gt;\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n\n             name country.etc    pop   lat long capital\n1       Abbeville      France  26656 50.12 1.83       0\n2         Acheres      France  23219 48.97 2.06       0\n3            Agde      France  23477 43.33 3.46       0\n4            Agen      France  34742 44.20 0.62       0\n5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n6 Aix-en-Provence      France 148622 43.53 5.44       0"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-40",
    "href": "lectures/lecture-09-content.html#section-40",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Using that information you can create a map of France that shows the larger cities (Figure 22). Use geom_polygon() from ggplot2 to draw shapes by connecting points within groups. And coord_map() adjusts for the fact that we are making a 2D map to represent a world that is 3D.\n\nggplot() +\n  geom_polygon(\n    data = france,\n    aes(x = long, y = lat, group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map() +\n  geom_point(\n    aes(x = french_cities$long, y = french_cities$lat),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  theme_minimal() +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n\n\n\n\n\nFigure 22: Map of France showing the largest cities"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-41",
    "href": "lectures/lecture-09-content.html#section-41",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "As is often the case with R, there are many ways to get started creating static maps. We have seen how they can be built using only ggplot2, but ggmap brings additional functionality.\nThere are two essential components to a map:\n\na border or background image (sometimes called a tile); and\nsomething of interest within that border, or on top of that tile.\n\nIn ggmap, we use an open-source option for our tile, Stamen Maps. And we use plot points based on latitude and longitude."
  },
  {
    "objectID": "lectures/lecture-09-content.html#australian-polling-places",
    "href": "lectures/lecture-09-content.html#australian-polling-places",
    "title": "Graphs, Tables, & Maps",
    "section": "Australian polling places",
    "text": "Australian polling places\nIn Australia, people have to go to “booths” in order to vote. Because the booths have coordinates (latitude and longitude), we can plot them. One reason we may like to do that is to notice spatial voting patterns.\nTo get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap. The main argument to this function is to specify a bounding box. A bounding box is the coordinates of the edges that you are interested in. This requires two latitudes and two longitudes."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-42",
    "href": "lectures/lecture-09-content.html#section-42",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "It can be useful to use Google Maps, or other mapping platform, to find the coordinate values that you need. In this case we have provided it with coordinates such that it will be centered around Australia’s capital Canberra.\n\nbbox &lt;- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-43",
    "href": "lectures/lecture-09-content.html#section-43",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "It is free, but we need to register in order to get a map. To do this go to https://client.stadiamaps.com/signup/ and create an account. Then create a new property, then “Add API Key”. Copy the key and run (replacing PUT-KEY-HERE with the key) register_stadiamaps(key = \"PUT-KEY-HERE\", write = TRUE). Then once you have defined the bounding box, the function get_stadiamap() will get the tiles in that area (?@fig-heyitscanberra). The number of tiles that it needs depends on the zoom, and the type of tiles that it gets depends on the type of map. We have used “toner-lite”, which is black and white, but there are others including: “terrain”, “toner”, and “toner-lines”. We pass the tiles to ggmap() which will plot it. An internet connection is needed for this to work as get_stadiamap() downloads the tiles.\n\ncanberra_stamen_map &lt;- get_stadiamap(bbox, zoom = 11, maptype = \"stamen_toner_lite\")\n\nggmap(canberra_stamen_map)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-44",
    "href": "lectures/lecture-09-content.html#section-44",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Once we have a map then we can use ggmap() to plot it. Now we want to get some data that we plot on top of our tiles. We will plot the location of the polling place based on its “division”. This is available from the Australian Electoral Commission (AEC).\n\nbooths &lt;-\n  read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-45",
    "href": "lectures/lecture-09-content.html#section-45",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "This dataset is for the whole of Australia, but as we are only plotting the area around Canberra, we will filter the data to only booths with a geography close to Canberra.\n\nbooths_reduced &lt;-\n    booths |&gt;\n    filter(State == \"ACT\") |&gt;\n    select(PollingPlaceID, DivisionNm, Latitude, Longitude) |&gt;\n    filter(!is.na(Longitude)) |&gt; # Remove rows without geography\n    filter(Longitude &lt; 165) # Remove Norfolk Island\n\nNow we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest.\n\nggmap(canberra_stamen_map, extent = \"normal\", maprange = FALSE) +\n    geom_point(\n        data = booths_reduced,\n        aes(x = Longitude, y = Latitude, colour = DivisionNm),\n        alpha = 0.7\n    ) +\n    scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n    coord_map(\n        projection = \"mercator\",\n        xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n        ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n    ) +\n    labs(\n        x = \"Longitude\",\n        y = \"Latitude\"\n    ) +\n    theme_minimal() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()```\n\n##\n\nWe may like to save the map so that we do not have to create it every time, and we can do that in the same way as any other graph, using `ggsave()`.\n\n\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")\n\nFinally, the reason that we used Stamen Maps and OpenStreetMap is because they are open source, but we could have also used Google Maps. This requires you to first register a credit card with Google, and specify a key, but with low usage the service should be free. Using Google Maps—by using get_googlemap() within ggmap—brings some advantages over get_stadiamap(). For instance it will attempt to find a place name rather than needing to specify a bounding box."
  },
  {
    "objectID": "lectures/lecture-09-content.html#united-states-military-bases",
    "href": "lectures/lecture-09-content.html#united-states-military-bases",
    "title": "Graphs, Tables, & Maps",
    "section": "United States military bases",
    "text": "United States military bases\nTo see another example of a static map we will plot some United States military bases after installing and loading troopdata. We can access data about United States overseas military bases back to the start of the Cold War using get_basedata().\n\nbases &lt;- get_basedata()\n\nhead(bases)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-46",
    "href": "lectures/lecture-09-content.html#section-46",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "We will look at the locations of United States military bases in Germany, Japan, and Australia. The troopdata dataset already has the latitude and longitude of each base, and we will use that as our item of interest. The first step is to define a bounding box for each country.\n\n# Use: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany &lt;- c(left = 5.867, bottom = 45.967, right = 15.033, top = 55.133)\n\nbbox_japan &lt;- c(left = 127, bottom = 30, right = 146, top = 45)\n\nbbox_australia &lt;- c(left = 112.467, bottom = -45, right = 155, top = -9.133)"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-47",
    "href": "lectures/lecture-09-content.html#section-47",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Then we need to get the tiles using get_stadiamap() from ggmap.\n\ngerman_stamen_map &lt;- get_stadiamap(bbox_germany, zoom = 6, maptype = \"stamen_toner_lite\")\n\njapan_stamen_map &lt;- get_stadiamap(bbox_japan, zoom = 6, maptype = \"stamen_toner_lite\")\n\naus_stamen_map &lt;- get_stadiamap(bbox_australia, zoom = 5, maptype = \"stamen_toner_lite\")"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-48",
    "href": "lectures/lecture-09-content.html#section-48",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "And finally, we can bring it all together with maps showing United States military bases in Germany (?@fig-mapbasesin-1), Japan (?@fig-mapbasesin-2), and Australia (?@fig-mapbasesin-3)."
  },
  {
    "objectID": "lectures/lecture-09-content.html#geocoding",
    "href": "lectures/lecture-09-content.html#geocoding",
    "title": "Graphs, Tables, & Maps",
    "section": "Geocoding",
    "text": "Geocoding\nSo far we have assumed that we already have geocoded data. This means that we have latitude and longitude coordinates for each place. But sometimes we only have place names, such as “Sydney, Australia”, “Toronto, Canada”, “Accra, Ghana”, and “Guayaquil, Ecuador”. Before we can plot them, we need to get the latitude and longitude coordinates for each case. The process of going from names to coordinates is called geocoding."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-49",
    "href": "lectures/lecture-09-content.html#section-49",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "Oh, you think we have good data on that!\n\n\n\nWhile you almost surely know where you live, it can be surprisingly difficult to specifically define the boundaries of many places. And this is made especially difficult when different levels of government have different definitions. @bronnerquantediting illustrates this in the case of Atlanta, Georgia, where there are (at least) three official different definitions:\n\nthe metropolitan statistical area;\nthe urbanized area; and\nthe census place.\n\nWhich definition is used can have a substantial effect on the analysis, or even the data that are available, even though they are all “Atlanta”."
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-50",
    "href": "lectures/lecture-09-content.html#section-50",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "There are a range of options to geocode data in R, but tidygeocoder is especially useful. We first need a dataframe of locations.\n\nplace_names &lt;-\n    tibble(\n        city = c(\"Sydney\", \"Toronto\", \"Accra\", \"Guayaquil\"),\n        country = c(\"Australia\", \"Canada\", \"Ghana\", \"Ecuador\")\n    )\n\nplace_names\n\n\nplace_names &lt;-\n    geo(\n        city = place_names$city,\n        country = place_names$country,\n        method = \"osm\"\n    )\n\nplace_names"
  },
  {
    "objectID": "lectures/lecture-09-content.html#section-51",
    "href": "lectures/lecture-09-content.html#section-51",
    "title": "Graphs, Tables, & Maps",
    "section": "",
    "text": "And we can now plot and label these cities (?@fig-mynicemap).\n\nworld &lt;- map_data(map = \"world\")\n\nggplot() +\n    geom_polygon(\n        data = world,\n        aes(x = long, y = lat, group = group),\n        fill = \"white\",\n        colour = \"grey\"\n    ) +\n    geom_point(\n        aes(x = place_names$long, y = place_names$lat),\n        color = \"black\"\n    ) +\n    geom_text(\n        aes(x = place_names$long, y = place_names$lat, label = place_names$city),\n        nudge_y = -5\n    ) +\n    theme_minimal() +\n    labs(\n        x = \"Longitude\",\n        y = \"Latitude\"\n    )"
  },
  {
    "objectID": "lectures/lecture-09-content.html#concluding-remarks",
    "href": "lectures/lecture-09-content.html#concluding-remarks",
    "title": "Graphs, Tables, & Maps",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nIn this chapter we considered many ways of communicating data. We spent substantial time on graphs, because of their ability to convey a large amount of information in an efficient way. We then turned to tables because of how they can specifically convey information. Finally, we discussed maps, which allow us to display geographic information. The most important task is to show the observations to the full extent possible."
  },
  {
    "objectID": "lectures/lecture-09-content.html#references",
    "href": "lectures/lecture-09-content.html#references",
    "title": "Graphs, Tables, & Maps",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-07-content.html",
    "href": "lectures/lecture-07-content.html",
    "title": "Introduction",
    "section": "",
    "text": "Writing is a critical skill—perhaps the most important—of all the skills required to analyze data. The only way to get better at writing is to write, ideally every day.\nWhen we write, although the benefits typically accrue to ourselves, we must nonetheless write for the reader. This means having one main message that we want to communicate, and thinking about where they are, rather than where we are.\nWe want to get to a first draft as quickly as possible. Even if it is horrible, the difference between a first draft existing and not is enormous. At that point we start to rewrite. When doing so we aim to maximize clarity, often by removing unnecessary words.\nWe typically begin with some area of interest and then develop research questions, datasets, and analysis in an iterative way. Through this process we come to a better understanding of what we are doing.\n\n\n\n\n\nknitr [@citeknitr]\ntidyverse [@tidyverse]\n\n\nlibrary(knitr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lectures/lecture-07-content.html#key-concepts-and-skills",
    "href": "lectures/lecture-07-content.html#key-concepts-and-skills",
    "title": "Introduction",
    "section": "",
    "text": "Writing is a critical skill—perhaps the most important—of all the skills required to analyze data. The only way to get better at writing is to write, ideally every day.\nWhen we write, although the benefits typically accrue to ourselves, we must nonetheless write for the reader. This means having one main message that we want to communicate, and thinking about where they are, rather than where we are.\nWe want to get to a first draft as quickly as possible. Even if it is horrible, the difference between a first draft existing and not is enormous. At that point we start to rewrite. When doing so we aim to maximize clarity, often by removing unnecessary words.\nWe typically begin with some area of interest and then develop research questions, datasets, and analysis in an iterative way. Through this process we come to a better understanding of what we are doing."
  },
  {
    "objectID": "lectures/lecture-07-content.html#software-and-packages",
    "href": "lectures/lecture-07-content.html#software-and-packages",
    "title": "Introduction",
    "section": "",
    "text": "knitr [@citeknitr]\ntidyverse [@tidyverse]\n\n\nlibrary(knitr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lectures/lecture-07-content.html#writing-1",
    "href": "lectures/lecture-07-content.html#writing-1",
    "title": "Introduction",
    "section": "Writing",
    "text": "Writing\n\nThe way to do a piece of writing is three or four times over, never once. For me, the hardest part comes first, getting something—anything—out in front of me. Sometimes in a nervous frenzy I just fling words as if I were flinging mud at a wall. Blurt out, heave out, babble out something—anything—as a first draft.\n@draftnumberfour [p. 159]\n\nThe process of writing is a process of rewriting. The critical task is to get to a first draft as quickly as possible. Until that complete first draft exists, it is useful to try to not to delete, or even revise, anything that was written, regardless of how bad it may seem. Just write. (This advice is directed at less-experienced writers. As you get more experience, you may find that your approach changes.)\nOne of the most intimidating stages is a blank page, and we deal with this by immediately adding headings such as: “Introduction”, “Data”, “Model”, “Results”, and “Discussion”. And then adding fields in the top matter for the various bits and pieces that are needed, such as “title”, “date”, “author”, and “abstract”. This creates a generic outline, which will play the role of mise en place for the paper. By way of background, mise en place is a preparatory phase in a professional kitchen when ingredients are sorted, prepared, and arranged for easy access. This ensures that everything that is needed is available without unnecessary delay. Putting together an outline plays the same role when writing quantitative papers, and is akin to placing on the counter, the ingredients that we will use to prepare dinner [@draftnumberfour]."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section",
    "href": "lectures/lecture-07-content.html#section",
    "title": "Introduction",
    "section": "",
    "text": "Having established this generic outline, we need to develop an understanding of what we are exploring through thinking deeply about our research question. In theory, we develop a research question, answer it, and then do all the writing; but that rarely actually happens [@franklin2005exploratory]. Instead, we typically have some idea of the question and the shape of an answer, and these become less vague as we write. This is because it is through the process of writing that we refine our thinking [@stephenking, p. 131]. Having put down some thoughts about the research question, we can start to add dot points in each of the sections, adding sub-sections with informative sub-headings as needed. We then go back and expand those dot points into paragraphs. While we do this our thinking is influenced by a web of other researchers, but also other aspects such as our circumstances and environment [@latour].\nWhile writing the first draft you should ignore the feeling that you are not good enough, or that it is impossible. Just write. You need words on paper, even if they are bad, and the first draft is when you accomplish this. Remove distractions and focus on writing. Perfectionism is the enemy, and should be set aside. Sometimes this can be accomplished by getting up very early to write, by creating a deadline, or forming a writing group. Creating a sense of urgency can be useful and one option is to not bother with adding proper citations as you go, which could slow you down, and instead just add something like “[TODO: CITE R HERE]”. Do similar with graphs and tables. That is, include textual descriptions such as “[TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE]” instead of actual graphs and tables. Focus on adding content, even if it is bad. When this is all done, a first draft exists."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-1",
    "href": "lectures/lecture-07-content.html#section-1",
    "title": "Introduction",
    "section": "",
    "text": "This first draft will be poorly written and far from great. But it is by writing a bad first draft that you can get to a good second draft, a great third draft, and eventually excellence [@birdbybird, p. 20]. That first draft will be too long, it will not make sense, it will contain claims that cannot be supported, and some claims that should not be. If you are not embarrassed by your first draft, then you have not written it quickly enough.\nUse the “delete” key extensively, as well as “cut” and “paste”, to turn that first draft into a second. Print the draft and using a red pen to move or remove words, sentences, and entire paragraphs, is especially helpful. The process of going from a first draft to a second draft is best done in one sitting, to help with the flow and consistency of the story. One aspect of this first rewrite is enhancing the story that we want to tell. Another aspect is taking out everything that is not the story [@stephenking, p. 57].\nIt can be painful to remove work that seems good even if it does not quite fit into what the draft is becoming. One way to make this less painful is to make a temporary document, perhaps named “debris.qmd”, to save these unwanted paragraphs instead of immediately deleting them. Another strategy is to comment out the paragraphs. That way you can still look at the raw file and notice aspects that could be useful."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-2",
    "href": "lectures/lecture-07-content.html#section-2",
    "title": "Introduction",
    "section": "",
    "text": "As you go through what was written in each of the sections try to bring some sense to it with special consideration to how it supports the story that is developing. This revision process is the essence of writing [@draftnumberfour, p. 160]. You should also fix the references, and add the real graphs and tables. As part of this rewriting process, the paper’s central message tends to develop, and the answers to the research questions tend to become clearer. At this point, aspects such as the introduction can be returned to and, finally, the abstract. Typos and other issues affect the credibility of the work. So these should be fixed as part of the second draft."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-3",
    "href": "lectures/lecture-07-content.html#section-3",
    "title": "Introduction",
    "section": "",
    "text": "At this point the draft is starting to become sensible. The job is to now make it brilliant. Print it and again go through it on paper. Try to remove everything that does not contribute to the story. At about this stage, you may start to get too close to the paper. This is a great opportunity to give it to someone else for their comments. Ask for feedback about what is weak about the story. After addressing these, it can be helpful to go through the paper once more, this time reading it aloud. A paper is never “done” and it is more that at a certain point you either run out of time or become sick of the sight of it."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-4",
    "href": "lectures/lecture-07-content.html#section-4",
    "title": "Introduction",
    "section": "",
    "text": "Consider two examples:\n\n@ahstonanderson examine eight billion unique listening events from 100,000 Spotify users to understand how users explore content. They find a clear relationship between age and behavior, with younger users exploring unknown content less than older users, despite having more diverse consumption. While it is clear that research questions around discovery and exploration drive this paper, it would not have been possible without access to this dataset. There likely would have been an iterative process where potential research questions and potential datasets were considered, before the ultimate match.\nThink of wanting to explore the neonatal mortality rate (NMR), which was introduced in ?@sec-fire-hose. One might be interested in what NMR could look like in Sub-Saharan Africa in 20 years. This would be question-first. But within this, there could be: theory-driven aspects, such as what do we expect based on biological relationships with other quantities; or data-driven aspects such as collecting as much data as possible to make forecasts. An alternative, purely data-driven approach would be having access to the NMR and then working out what is possible."
  },
  {
    "objectID": "lectures/lecture-07-content.html#data-first",
    "href": "lectures/lecture-07-content.html#data-first",
    "title": "Introduction",
    "section": "Data-first",
    "text": "Data-first\nWhen being data-first, the main issue is working out the questions that can be reasonably answered with the available data. When deciding what these are, it is useful to consider:\n\nTheory: Is there a reasonable expectation that there is something causal that could be determined? For instance, Mark Christensen used to joke that if the question involved charting the stock market, then it might be better to hark back to The Odyssey and read bull entrails on a fire, because at least that way you would have something to eat at the end of the day. Questions usually need to have some plausible theoretical underpinning to help avoid spurious relationships. One way to develop theory, given data, is to consider “of what is this an instance?” [@ofwhatisthisaninstance, p. 7]. Following that approach, one tries to generalize beyond the specific setting. For instance, thinking of some particular civil war as an instance of all civil wars. The benefit of this is it focuses attention on the general attributes needed for building theory.\nImportance: There are plenty of trivial questions that can be answered, but it is important to not waste our time or that of the reader. Having an important question can also help with motivation when we find ourselves in, say, the fourth straight week of cleaning data and debugging code. In industry it can also make it easier to attract talented employees and funding. That said, a balance is needed; the question needs to have a decent chance of being answered. Attacking a generation-defining question might be best broken into chunks.\nAvailability: Is there a reasonable expectation of additional data being available in the future? This could allow us to answer related questions and turn one paper into a research agenda.\nIteration: Is this something that could be run multiple times, or is it a once-off analysis? If it is the former, then it becomes possible to start answering specific research questions and then iterate. But if we can only get access to the data once then we need to think about broader questions."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-5",
    "href": "lectures/lecture-07-content.html#section-5",
    "title": "Introduction",
    "section": "",
    "text": "There is a saying, sometimes attributed to Xiao-Li Meng, that all of statistics is a missing data problem. And so paradoxically, another way to ask data-first questions is to think about the data we do not have. For instance, returning to the neonatal and maternal mortality examples discussed earlier one problem is that we do not have complete cause of death data. If we did, then we could count the number of relevant deaths. (@Castro2023 remind us that this simplistic hypothetical would be complicated in reality because there are sometimes causes of death that are not independent of other causes.) Having established some missing data problem, we can take a data-driven approach. We look at the data we do have, and then ask research questions that speak to the extent that we can use that to approximate our hypothetical dataset.\nOne way that some researchers are data-first is that they develop a particular expertise in the data of some geographical or historical circumstance. For instance, they may be especially knowledgeable about, say, the present-day United Kingdom, or late nineteenth century Japan. They then look at the questions that other researchers are asking in other circumstances, and bring their data to that question. For instance, it is common to see a particular question initially asked for the United States, and then a host of researchers answer that same question for the United Kingdom, Canada, Australia, and many other countries.\nThere are a number of negatives to data-first research, including the fact that it can be especially uncertain. It can also struggle for external validity because there is always a worry about a selection effect.\nA variant of data-driven research is model-driven research. Here a researcher becomes an expert on some particular statistical approach and then applies that approach to appropriate contexts."
  },
  {
    "objectID": "lectures/lecture-07-content.html#question-first",
    "href": "lectures/lecture-07-content.html#question-first",
    "title": "Introduction",
    "section": "Question-first",
    "text": "Question-first\nWhen trying to be question-first, there is the inverse issue of being concerned about data availability. The “FINER framework” is used in medicine to help guide the development of research questions. It recommends asking questions that are: Feasible, Interesting, Novel, Ethical, and Relevant [@hulley2007designing]. @farrugia2010research build on FINER with PICOT, which recommends additional considerations: Population, Intervention, Comparison group, Outcome of interest, and Time."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-6",
    "href": "lectures/lecture-07-content.html#section-6",
    "title": "Introduction",
    "section": "",
    "text": "It can feel overwhelming trying to write out a question. One way to go about it is to ask a very specific question. Another is to decide whether we are interested in descriptive, predictive, inferential, or causal analysis. These then lead to different types of questions. For instance:\n\ndescriptive analysis: “What does \\(x\\) look like?”;\npredictive analysis: “What will happen to \\(x\\)?”;\ninferential: “How can we explain \\(x\\)?”; and\ncausal: “What impact does \\(x\\) have on \\(y\\)?”.\n\nEach of these have a role to play. Since the credibility revolution [@angrist2010credibility], causal questions answered with a particular approach have been predominant. This has brought some benefit, but not without cost. Descriptive analysis can be just as, indeed sometimes more, illuminating, and is critical [@Sen1980]. The nature of the question being asked matters less than being genuinely interested in answering it."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-7",
    "href": "lectures/lecture-07-content.html#section-7",
    "title": "Introduction",
    "section": "",
    "text": "Time will often be constrained, possibly in an interesting way and this can guide the specifics of the research question. If we are interested in the effect of a celebrity’s announcements on the stock market, then that can be done by looking at stock prices before and after the announcement. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we must either wait a while, or we need to look at people who were treated twenty years ago. We then have selection effects and different circumstances compared to if we were to administer the drug today. Often the only reasonable thing to do is to build a statistical model, but that brings other issues."
  },
  {
    "objectID": "lectures/lecture-07-content.html#counterfactuals-and-bias",
    "href": "lectures/lecture-07-content.html#counterfactuals-and-bias",
    "title": "Introduction",
    "section": "Counterfactuals and bias",
    "text": "Counterfactuals and bias\nThe creation of a counterfactual is often crucial when answering questions. A counterfactual is an if-then statement in which the “if” is false. Consider the example of Humpty Dumpty in Through the Looking-Glass by Lewis Carroll:\n\n“What tremendously easy riddles you ask!” Humpty Dumpty growled out. “Of course I don’t think so! Why, if ever I did fall off—which there’s no chance of—but if I did—” Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. “If I did fall,” he went on, “The King has promised me—with his very own mouth-to-to-” “To send all his horses and all his men,” Alice interrupted, rather unwisely.\n@throughthelookingglass\n\nHumpty is satisfied with what would happen if he were to fall off, even though he is convinced that this would never happen. It is this comparison group that often determines the answer to a question. For instance, in ?@sec-causality-from-observational-data we consider the effect of VO2 max on a cyclist’s chance of winning a race. If we compare over the general population then it is an important variable. But if we only compare over well-trained athletes, then it is less important, because of selection.\n\nSelection bias and measurement bias\nTwo aspects of the data to be especially aware of when deciding on a research question are selection bias and measurement bias.\nSelection bias occurs when the results depend on who is in the sample. One of the pernicious aspects of selection bias is that we need to know about its existence in order to do anything about it. But many default diagnostics will not identify selection bias. In A/B testing, which we discuss in ?@sec-hunt-data, A/A testing is a slight variant where we create groups and compare them before imposing a treatment (hence the A/A nomenclature). This effort to check whether the groups are initially the same, can help to identify selection bias. More generally, comparing the properties of the sample, such as age-group, gender, and education, with characteristics of the population can assist as well. But the fundamental problem with selection bias and observational data is that we know people about whom we have data are different in at least one way to those about whom we do not! But we do not know in what other ways they may be different.\nSelection bias can pervade many aspects of our analysis. Even a sample that is initially representative may become biased over time. For instance, survey panels, that we discuss in ?@sec-farm-data, need to be updated from time to time because the people who do not get anything out of it stop responding.\nAnother bias to be aware of is measurement bias, which occurs when the results are affected by how the data were collected. A common example of this is if we were to ask respondents their income, then we may get different answers in-person compared with an online survey."
  },
  {
    "objectID": "lectures/lecture-07-content.html#estimands",
    "href": "lectures/lecture-07-content.html#estimands",
    "title": "Introduction",
    "section": "Estimands",
    "text": "Estimands\nWe will typically be interested in using data to answer our question and it is important that we are clear about specifics. For instance, we might be interested in the effect of smoking on life expectancy. In that case, there is some true effect, which we can never know, and that true effect is called the “estimand” [@Little2021]. Defining the estimand at some point in the paper, ideally in the introduction, is critical [@Lundberg2021]. This is because it is easy to slightly change some specific aspect of the analysis plan and end up accidentally estimating something different [@becarefulaboutyourestimand]. They are beginning to be required by some medicine regulators [@kahan2024]. For an estimand we are looking for a clear description of what the effect represents [@Kahan2023]. An “estimator” is a process by which we use the data that we have available to generate an “estimate” of the “estimand”. @steinsparadox provide a discussion of estimators and related concerns.\n@de2021thinking [p. 94] describe the relationship between an estimate and an estimand as:\n\\[\n\\mbox{Estimate = Estimand + Bias + Noise}\n\\]"
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-8",
    "href": "lectures/lecture-07-content.html#section-8",
    "title": "Introduction",
    "section": "",
    "text": "Bias refers to issues with an estimator systematically providing estimates that are different from the estimand, while noise refers to non-systematic differences. For instance, consider a standard Normal distribution. We might be interested in understanding the average, which would be our estimand. We know (in a way that we can never with real data) that the estimand is zero. Let us draw ten times from that distribution. One estimator we could use to produce an estimate is: sum the draws and divide by the number of draws. Another is to order the draws and find the middle observation. To be more specific, we will simulate this situation (Table 1).\n\nset.seed(853)\n\ntibble(\n    num_draws = c(\n        rep(10, times = 10),\n        rep(100, times = 100),\n        rep(1000, times = 1000),\n        rep(10000, times = 10000)\n    ),\n    draw = rnorm(\n        n = length(num_draws),\n        mean = 0,\n        sd = 1\n    )\n) |&gt;\n    summarise(\n        estimator_one = sum(draw) / unique(num_draws),\n        estimator_two = sort(draw)[round(unique(num_draws) / 2, 0)],\n        .by = num_draws\n    ) |&gt;\n    kable(\n        col.names = c(\"Number of draws\", \"Estimator one\", \"Estimator two\"),\n        digits = 2,\n        format.args = list(big.mark = \",\")\n    )\n\n\n\nTable 1: Comparing two estimators of the average of random draws as the number of draws increases\n\n\n\n\n\n\nNumber of draws\nEstimator one\nEstimator two\n\n\n\n\n10\n-0.58\n-0.82\n\n\n100\n-0.06\n-0.07\n\n\n1,000\n0.06\n0.04\n\n\n10,000\n-0.01\n-0.01\n\n\n\n\n\n\n\n\nAs the number of draws increases, the effect of noise is removed, and our estimates illustrate the bias of our estimators. In this example, we know what the truth is, but when considering real data it can be more difficult to know what to do. Hence the importance of being clear about what the estimand is, before turning to generating estimates."
  },
  {
    "objectID": "lectures/lecture-07-content.html#directed-acyclic-graphs",
    "href": "lectures/lecture-07-content.html#directed-acyclic-graphs",
    "title": "Introduction",
    "section": "Directed Acyclic Graphs",
    "text": "Directed Acyclic Graphs\nWhen we are thinking about the variables we will use to answer our question, it can help to be specific about what we mean. It is easy to get caught up in observational data and trick ourselves. We should think hard, and to use all the tools available to us. One framework that can help with thinking hard about our data is the use of directed acyclic graphs (DAG). DAGs are a fancy name for a flow diagram and involve drawing arrows and lines between the variables to indicate the relationship between them.\nTo construct them we use Graphviz, which is an open-source package for graph visualization and is built into Quarto. The code needs to be wrapped in a “dot” chunk rather than “R”, and the chunk options are set with “//|” instead of “#|”. Alternatives that do not require this include the use of DiagrammeR [@citeDiagrammeR] and ggdag [@citeggdag]. We provide the whole chunk for the first DAG, but then, only provide the code for the others.\n\n```{dot}\n//| label: fig-dot-firstdag-quarto\n//| fig-cap: \"We expect a causal relationship between x and y, where x influences y\"\n//| fig-width: 4\ndigraph D {\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  {rank=same x y};\n  \n  x -&gt; y;\n}\n```\n\n\n\n\n\n\n\nD\n\n\n\nx\nx\n\n\n\ny\ny\n\n\n\nx-&gt;y\n\n\n\n\n\n\n\n\nFigure 1: We expect a causal relationship between x and y, where x influences y\n\n\n\n\n\nIn Figure 1, we are saying that we think x causes y."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-12",
    "href": "lectures/lecture-07-content.html#section-12",
    "title": "Introduction",
    "section": "",
    "text": "We could build another DAG where the situation is less clear. To make the examples a little easier to follow, we will switch to thinking about a hypothetical relationship between income and happiness, with consideration of variables that could affect that relationship. In this first one we consider the relationship between income and happiness, along with education (Figure 2).\n\ndigraph D {\n  \n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Education\"];\n  \n  { rank=same a b};\n  \n  a-&gt;b;\n  c-&gt;{a, b};\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nEducation\n\n\n\nc-&gt;a\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\nFigure 2: Education is a confounder that affects the relationship between income and happiness\n\n\n\n\n\nIn Figure 2, we think income causes happiness. But we also think that education causes happiness, and that education also causes income. That relationship is a “backdoor path”, and failing to adjust for education in a regression could overstate the extent of the relationship, or even create a spurious relationship, between income and happiness in our analysis. That is, we may think that changes in income are causing changes in happiness, but it could be that education is changing them both. That variable, in this case, education, is called a “confounder”.\n@hernanrobins2020 [p. 83] discuss an interesting case where a researcher was interested in whether one person looking up at the sky makes others look up at the sky also. There was a clear relationship between the responses of both people. But it was also the case that there was noise in the sky. It was unclear whether the second person looked up because the first person looked up, or they both looked up because of the noise. When using experimental data, randomization allows us to avoid this concern, but with observational data we cannot rely on that. It is also not the case that bigger data necessarily get around this problem for us. Instead, we should think carefully about the situation, and DAGs can help with that.\nIf there are confounders, but we are still interested in causal effects, then we need to adjust for them. One way is to include them in the regression. But the validity of this requires several assumptions. In particular, @gelmanandhill [p. 169] warn that our estimate will only correspond to the average causal effect in the sample if we include all of the confounders and have the right model. Putting the second requirement to one side, and focusing only on the first, if we do not think about and observe a confounder, then it can be difficult to adjust for it. And this is an area where both domain expertise and theory can bring considerable weight to an analysis.\nIn Figure 3 we again consider that income causes happiness. But, if income also causes children, and children also cause happiness, then we have a situation where it would be tricky to understand the effect of income on happiness.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Children\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b, c};\n  c-&gt;b;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nChildren\n\n\n\na-&gt;c\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\nFigure 3: Children as a mediator between income and happiness\n\n\n\n\n\nIn Figure 3, children is called a “mediator” and we would not adjust for it if we were interested in the effect of income on happiness. If we were to adjust for it, then some of what we are attributing to income, would be due to children.\nFinally, in Figure 4 we have yet another similar situation, where we think that income causes happiness. But this time both income and happiness also cause exercise. For instance, if you have more money then it may be easier to exercise, but also it may be easier to exercise if you are happier.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Exercise\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b c};\n  b-&gt;c;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nExercise\n\n\n\na-&gt;c\n\n\n\n\n\nb-&gt;c\n\n\n\n\n\n\n\n\nFigure 4: Exercise as a collider affecting the relationship between income and happiness\n\n\n\n\n\nIn this case, exercise is called a “collider” and if we were to condition on it, then we would create a misleading relationship. Income influences exercise, but a person’s happiness also affects this. Exercise is a collider because both the predictor and outcome variable of interest influence it."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-13",
    "href": "lectures/lecture-07-content.html#section-13",
    "title": "Introduction",
    "section": "",
    "text": "We will be clear about this: we must create the DAG ourselves, in the same way that we must put together the model ourselves. There is nothing that will create it for us. This means that we need to think carefully about the situation. Because it is one thing to see something in the DAG and then do something about it, but it is another to not even know that it is there. @citemcelreath [p. 180] describes these as haunted DAGs. DAGs are helpful, but they are just a tool to help us think deeply about our situation.\nWhen we are building models, it can be tempting to include as many predictor variables as possible. DAGs show clearly why we need to be more thoughtful. For instance, if a variable is a confounder, then we would want to adjust for it, whereas if a variable was a collider then we would not. We can never know the truth, and we are informed by aspects such as theory, what we are interested in, research design, limitations of the data, or our own limitations as researchers, to name a few. Knowing the limits is as important as reporting the model. Data and models with flaws are still useful, if you acknowledge those flaws. The work of thinking about a situation is never done, and relies on others, which is why we need to make all our work as reproducible as possible."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-14",
    "href": "lectures/lecture-07-content.html#section-14",
    "title": "Introduction",
    "section": "",
    "text": "We discuss the following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, and technical terms.1 Throughout the paper try to be as brief and specific as possible. Most readers will not get past the title. Almost no one will read more than the abstract. Section and sub-section headings, as well as graph and table captions should work on their own, without the surrounding text, because that type of skimming is how many people read papers [@Keshav2007]."
  },
  {
    "objectID": "lectures/lecture-07-content.html#title",
    "href": "lectures/lecture-07-content.html#title",
    "title": "Introduction",
    "section": "Title",
    "text": "Title\nA title is the first opportunity that we have to engage our reader in our story. Ideally, we are able to tell our reader exactly what we found. Effective titles are critical because otherwise papers could be ignored by readers. While a title does not have to be “cute”, it does need to be meaningful. This means it needs to make the story clear.\nOne example of a title that is good enough is “On the 2016 Brexit referendum”. This title is useful because the reader knows what the paper is about. But it is not particularly informative or enticing. A slightly better title could be “On the Vote Leave outcome in the 2016 Brexit referendum”. This variant adds informative specificity. We argue the best title would be something like “Vote Leave outperforms in rural areas in the 2016 Brexit referendum: Evidence from a Bayesian hierarchical model”. Here the reader knows the approach of the paper and also the main take-away.\nWe will consider a few examples of particularly effective titles. @hug2019national use “National, regional, and global levels and trends in neonatal mortality between 1990 and 2017, with scenario-based projections to 2030: a systematic analysis”. Here it is clear what the paper is about and the methods that are used. @Alexander2020 use “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018”. The main finding is, along with a good deal of information about what the content will be, clear from the title. @alexander2018trends use “Trends in Black and White Opioid Mortality in the United States, 1979–2015”; @Frei2022 use “How the closure of a US tax loophole may affect investor portfolios”. Possibly one of the best titles ever is @bickel1975sex “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation”, which we return to in ?@sec-causality-from-observational-data.\nA title is often among the last aspects of a paper to be finalized. While getting through the first draft, we typically use a working title that gets the job done. We then refine it over the course of redrafting. The title needs to reflect the final story of the paper, and this is not usually something that we know at the start. We must strike a balance between getting our reader interested enough to read the paper, and conveying enough of the content so as to be useful [@hayotacademicstyle]. Two excellent examples are The History of England from the Accession of James the Second by Thomas Babington Macaulay, and A History of the English-Speaking Peoples by Winston Churchill. Both are clear about what the content is, and, for their target audience, spark interest.\nOne specific approach is the form: “Exciting content: Specific content”, for instance, “Returning to their roots: Examining the performance of Vote Leave in the 2016 Brexit referendum”. @kennedy2020know provide a particularly nice example of this approach with “Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample”, as does @craiu2019hiring with “The Hiring Gambit: In Search of the Twofer Data Scientist”. A close variant of this is “A question? And an approach”. For instance, @cahill2020increase with “What increase in modern contraceptive use is needed in FP2020 countries to reach 75% demand satisfied by 2030? An assessment using the Accelerated Transition Method and Family Planning Estimation Model”. As you gain experience with this variant, it becomes possible to know when it is appropriate to drop the answer part yet remain effective, such as @briggs2021does with “Why Does Aid Not Target the Poorest?”. Another specific approach is “Specific content then broad content” or the inverse. For instance, “Rurality, elites, and support for Vote Leave in the 2016 Brexit referendum” or “Support for Vote Leave in the 2016 Brexit referendum, rurality and elites”. This approach is used by @tolley2021gender with “Gender, municipal party politics, and Montreal’s first woman mayor”.\nSometimes it is possible to include a subtitle. When this is possible, a great way to take advantage of this is to use it to include some detail of the main quantitative result that you found. Getting the right level of detail and abstraction about that result is difficult and will require re-writing and getting other’s opinions."
  },
  {
    "objectID": "lectures/lecture-07-content.html#abstract",
    "href": "lectures/lecture-07-content.html#abstract",
    "title": "Introduction",
    "section": "Abstract",
    "text": "Abstract\nFor a ten-to-fifteen-page paper, a good abstract is a three-to-five sentence paragraph. For a longer paper the abstract can be slightly longer. The abstract needs to specify the story of the paper. It must also convey what was done and why it matters. To do so, an abstract typically touches on the context of the work, its objectives, approach, and findings.\nMore specifically, a good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.\nWe see this pattern in a variety of abstracts. For instance, @tolley2021gender draw in the reader with their first sentence by mentioning the election of the first woman mayor in 400 years. The second sentence is clear about what is done in the paper. The third sentence tells the reader how it is done i.e. a survey, and the fourth sentence adds some detail. The fifth and final sentence makes the main take-away clear.\n\nIn 2017, Montreal elected Valérie Plante, the first woman mayor in the city’s 400-year history. Using this election as a case study, we show how gender did and did not influence the outcome. A survey of Montreal electors suggests that gender was not a salient factor in vote choice. Although gender did not matter much for voters, it did shape the organization of the campaign and party. We argue that Plante’s victory can be explained in part by a strategy that showcased a less leader-centric party and a degendered campaign that helped counteract stereotypes about women’s unsuitability for positions of political leadership.\n\nSimilarly, @beauregard2021antiwomen make the broader environment clear within the first two sentences, and the specific contribution of this paper to that environment. The third and fourth sentences make the data source and main findings clear. The fifth and sixth sentences add specificity that would be of interest to likely readers of this abstract i.e. academic political scientists. In the final sentence, the position of the authors is made clear.\n\nPrevious research on support for gender quotas focuses on attitudes toward gender equality and government intervention as explanations. We argue the role of attitudes toward women in understanding support for policies aiming to increase the presence of women in politics is ambivalent—both hostile and benevolent forms of sexism contribute in understanding support, albeit in different ways. Using original data from a survey conducted on a probability-based sample of Australian respondents, our findings demonstrate that hostile sexists are more likely to oppose increasing of women’s presence in politics through the adoption of gender quotas. Benevolent sexists, on the other hand, are more likely to support these policies than respondents exhibiting low levels of benevolent sexism. We argue this is because benevolent sexism holds that women are pure and need protection; they do not have what it takes to succeed in politics without the assistance of quotas. Finally, we show that while women are more likely to support quotas, ambivalent sexism has the same relationship with support among both women and men. These findings suggest that aggregate levels of public support for gender quotas do not necessarily represent greater acceptance of gender equality generally."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-15",
    "href": "lectures/lecture-07-content.html#section-15",
    "title": "Introduction",
    "section": "",
    "text": "Another excellent example of an abstract is @sidesvavreckwarshaw. In just five sentences, they make it clear what they do, how they do it, what they find, and why it is important.\n\nWe provide a comprehensive assessment of the influence of television advertising on United States election outcomes from 2000–2018. We expand on previous research by including presidential, Senate, House, gubernatorial, Attorney General, and state Treasurer elections and using both difference-in-differences and border-discontinuity research designs to help identify the causal effect of advertising. We find that televised broadcast campaign advertising matters up and down the ballot, but it has much larger effects in down-ballot elections than in presidential elections. Using survey and voter registration data from multiple election cycles, we also show that the primary mechanism for ad effects is persuasion, not the mobilization of partisans. Our results have implications for the study of campaigns and elections as well as voter decision making and information processing."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-16",
    "href": "lectures/lecture-07-content.html#section-16",
    "title": "Introduction",
    "section": "",
    "text": "The best abstracts will have such a high content to words ratio that they may even feel a little terse. For instance, in the abstract of @touvron, there is not a word that is wasted and they communicate a large amount of information in only four sentences.\n\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\n\n@kasymatching provide an excellent example of a more statistical abstract. They clearly identify what they do and why it is important.\n\nWe consider an experimental setting in which a matching of resources to participants has to be chosen repeatedly and returns from the individual chosen matches are unknown but can be learned. Our setting covers two-sided and one-sided matching with (potentially complex) capacity constraints, such as refugee resettlement, social housing allocation, and foster care. We propose a variant of the Thompson sampling algorithm to solve such adaptive combinatorial allocation problems. We give a tight, prior-independent, finite-sample bound on the expected regret for this algorithm. Although the number of allocations grows exponentially in the number of matches, our bound does not. In simulations based on refugee resettlement data using a Bayesian hierarchical model, we find that the algorithm achieves half of the employment gains (relative to the status quo) that could be obtained in an optimal matching based on perfect knowledge of employment probabilities."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-17",
    "href": "lectures/lecture-07-content.html#section-17",
    "title": "Introduction",
    "section": "",
    "text": "Finally, @briggs2021does begins with a claim that seems unquestionably true. In the second sentence he then says that it is false! The third sentence specifies the extent of this claim, and the fourth sentence details how he comes to this position, before providing more detail. The final two sentences speak broader implications and importance.\n\nForeign-aid projects typically have local effects, so they need to be placed close to the poor if they are to reduce poverty. I show that, conditional on local population levels, World Bank (WB) project aid targets richer parts of countries. This relationship holds over time and across world regions. I test five donor-side explanations for pro-rich targeting using a pre-registered conjoint experiment on WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments as most interested in targeting aid politically and controlling implementation. They also believe that aid works better in poorer or more remote areas, but that implementation in these areas is uniquely difficult. These results speak to debates in distributive politics, international bargaining over aid, and principal-agent issues in international organizations. The results also suggest that tweaks to WB incentive structures to make ease of project implementation less important may encourage aid to flow to poorer parts of countries."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-18",
    "href": "lectures/lecture-07-content.html#section-18",
    "title": "Introduction",
    "section": "",
    "text": "Nature, a scientific journal, provides a guide for constructing an abstract. They recommend a structure that results in an abstract of six parts and adds up to around 200 words:\n\nAn introductory sentence that is comprehensible to a wide audience.\nA more detailed background sentence that is relevant to likely readers.\nA sentence that states the general problem.\nSentences that summarize and then explain the main results.\nA sentence about general context.\nAnd finally, a sentence about the broader perspective.\n\nThe first sentence of an abstract should not be vacuous. Assuming the reader continued past the title, this first sentence is the next opportunity that we have to implore them to keep reading our paper. And then the second sentence of the abstract, and so on. Work and re-work the abstract until it is so good that you would be fine if that was the only thing that was read; because that will often be the case."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-19",
    "href": "lectures/lecture-07-content.html#section-19",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nAn introduction needs to be self-contained and convey everything that a reader needs to know. We are not writing a mystery story. Instead, we want to give away the most important points in the introduction. For a ten-to-fifteen-page paper, an introduction may be two or three paragraphs of main content. @hayotacademicstyle [p. 90] says the goal of an introduction is to engage the reader, locate them in some discipline and background, and then tell them what happens in the rest of the paper. It should be completely reader-focused.\nThe introduction should set the scene and give the reader some background. For instance, we typically start a little broader. This provides some context to the paper. We then describe how the paper fits into that context, and give some high-level results, especially focused on the one key result that is the main part of the story. We provide more detail here than we provided in the abstract, but not the full extent. And we broadly discuss next steps in a sentence or two. Finally, we finish the introduction with an additional short final paragraph that highlights the structure of the paper."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-20",
    "href": "lectures/lecture-07-content.html#section-20",
    "title": "Introduction",
    "section": "",
    "text": "As an example (with made-up details):\n\nThe UK Conservative Party has always done well in rural electorates. And the 2016 Brexit vote was no different with a significant difference in support between rural and urban areas. But even by the standard of rural support for conservative issues, support for “Vote Leave” was unusually strong with “Vote Leave” being most heavily supported in the East Midlands and the East of England, while the strongest support for “Remain” was in Greater London.\nIn this paper we look at why the performance of “Vote Leave” in the 2016 Brexit referendum was so correlated with rurality. We construct a model in which support for “Vote Leave” at a voting area level is explained by the number of farms in the area, the average internet connectivity, and the median age. We find that as the median age of an area increases, the likelihood that an area supported “Vote Leave” decreases by 14 percentage points. Future work could look at the effect of having a Conservative MP which would allow a more nuanced understanding of these effects.\nThe remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.\n\nThe introduction needs to be self-contained and tell the reader almost everything that they need to know. A reader should be able to only read the introduction and have an accurate picture of all the major aspects of the whole paper. It would be rare to include graphs or tables in the introduction. An introduction should close by telegraphing the structure of the paper."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-21",
    "href": "lectures/lecture-07-content.html#section-21",
    "title": "Introduction",
    "section": "",
    "text": "Data\nRobert Caro, Lyndon Johnson’s biographer, describes the importance of conveying “a sense of place” when writing a biography [@caroonworking, p. 141]. He defines this as “the physical setting in which a book’s action is occurring: to see it clearly enough, in sufficient detail, so that he feels as if he himself were present while the action is occurring.” He provides the following example:\n\nWhen Rebekah walked out the front door of that little house, there was nothing—a roadrunner streaking behind some rocks with something long and wet dangling from his beak, perhaps, or a rabbit disappearing around a bush so fast that all she really saw was the flash of a white tail—but otherwise nothing. There was no movement except for the ripple of the leaves in the scattered trees, no sound except for the constant whisper of the wind\\(\\dots\\) If Rebekah climbed, almost in desperation, the hill in the back of the house, what she saw from its crest was more hills, an endless vista of hills, hills on which there was visible not a single house\\(\\dots\\) hills on which nothing moved, empty hills with, above them, empty sky; a hawk circling silently overhead was an event. But most of all, there was nothing human, no one to talk to.\n@caroonworking [p. 146]\n\nHow thoroughly we can imagine the circumstances of Johnson’s mother, Rebekah Baines Johnson. When writing our papers, we need to achieve that same sense of place, for our data, as Caro provides for the Hill County. We do this by being as explicit as possible. We typically have a whole section about it and this is designed to show the reader, as closely as possible, the actual data that underpin our story.\nWhen writing the data section, we are beginning our answer to the critical question about our claim, which is, how is it possible to know this? [@draftnumberfour, p. 78]. An excellent example of a data section is provided by @doll1950smoking. They are interested in the effect of smoking between control and treatment groups. After clearly describing their dataset they use tables to display relevant cross-tabs and graphs to contrast groups.\nIn the data section we need to thoroughly discuss the variables in the dataset that we are using. If there are other datasets that could have been used, but were not, then this should be mentioned and the choice justified. If variables were constructed or combined, then this process and motivation should be explained.\nWe want the reader to understand what the data that underpin the results look like. This means that we should graph the data that are used in our analysis, or as close to them as possible. And we should also include tables of summary statistics. If the dataset was created from some other source, then it can also help to include an example of that original source. For instance, if the dataset was created from survey responses then the underlying survey questions should be included in an appendix.\nSome judgment is required when it comes to the figures and tables in the data section. The reader should have the opportunity to understand the details, but it may be that some are better placed in an appendix. Figures and tables are a critical aspect of convincing people of a story. In a graph we can show the data and then let the reader decide for themselves. And using a table, we can summarize a dataset. At the very least, every variable should be shown in a graph and summarized in a table. If there are too many, then some of these could be relegated to an appendix, with the critical relationships shown in the main body. Figures and tables should be numbered and then cross-referenced in the text, for instance, “Figure 1 shows\\(\\dots\\)”, “Table 1 describes\\(\\dots\\)”. For every graph and table there should be accompanying text that describes their main aspects, and adds additional detail."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-22",
    "href": "lectures/lecture-07-content.html#section-22",
    "title": "Introduction",
    "section": "",
    "text": "We discuss the components of graphs and tables, including titles and labels, in ?@sec-static-communication. But here we will discuss captions, as they are between the text and the graph or table. Captions need to be informative and self-contained. @borkin2015beyond use eye-tracking to understand how visualizations are recognized and recalled. They find that captions need to make the central message of the figure clear, and that there should be redundancy. As @elementsofgraphingdata [p. 57] says, the “interplay between graph, caption, and text is a delicate one”, however the reader should be able to read only the caption and understand what the graph or table shows. A caption that is two lines long is not necessarily inappropriate. And all aspects of the graph or table should be explained. For instance, consider Figure 5 (a) and Figure 5 (b), both from @bowley [p. 151]. They are clear, and self-contained.\n\n\n\n\n\n\n\n\n\n\n\n(a) Example of a well-captioned figure\n\n\n\n\n\n\n\n\n\n\n\n(b) Example of a well-captioned table\n\n\n\n\n\n\n\nFigure 5: Examples of a graph and table from @bowley\n\n\n\nThe choice between a table and a graph comes down to how much information is to be conveyed. In general, if there is specific information that should be considered, such as a summary statistic, then a table is a good option. If we are interested in the reader making comparisons and understanding trends, then a graph is a good option [@gelman2002let]."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-23",
    "href": "lectures/lecture-07-content.html#section-23",
    "title": "Introduction",
    "section": "",
    "text": "Model\nWe often build a statistical model that we will use to explore the data, and it is normal to have a specific section about this. At a minimum you should specify the equations that describe the model being used and explain their components with plain language and cross-references.\nThe model section typically begins with the model being written out, explained, and justified. Depending on the expected reader, some background may be needed. After specifying the model with appropriate mathematical notation and cross-referencing it, the components of the model should then be defined and explained. Try to define each aspect of the notation. This helps convince the reader that the model was well-chosen and enhances the credibility of the paper. The model’s variables should correspond to those that were discussed in the data section, making a clear link between the two sections.\nThere should be some discussion of how features enter the model and why. Some examples could include:\n\nWhy use age rather than age-groups?\nWhy does state/province have a levels effect?\nWhy is gender a categorical variable? In general, we are trying to convey a sense that this is the appropriate model for the situation. We want the reader to understand how the aspects that were discussed in the data section assert themselves in the modeling decisions that were made.\n\nThe model section should close with some discussion of the assumptions that underpin the model. It should also have a brief discussion of alternative models or variants. You want the strengths and weaknesses to be clear and for the reader to know why this particular model was chosen.\nAt some point in this section, it is usually appropriate to specify the software that was used to run the model, and to provide some evidence of thought about the circumstances in which the model may not be appropriate. That second point would typically be expanded on in the discussion section. And there should be evidence of model validation and checking, model convergence, and/or diagnostic issues. Again, there is a balance needed here, and some of this content may be more appropriately placed in appendices.\nWhen technical terms are used, they should be briefly explained in plain language for readers who might not be familiar with it. For instance, @monicababynames integrates an explanation of the Gini coefficient that brings the reader along.\n\nTo look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.\n\nThere may be papers that do not include a statistical model. In that case, this “Model” section should be replaced by a broader “Methodology” section. It might describe the simulation that was conducted, or contain more general details about the approach."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-24",
    "href": "lectures/lecture-07-content.html#section-24",
    "title": "Introduction",
    "section": "",
    "text": "Results\nTwo excellent examples of results sections are provided by @kharecha2013prevented and @kiang2021racial. In the results section, we want to communicate the outcomes of the analysis in a clear way and without too much focus on the discussion of implications. The results section likely requires summary statistics, tables, and graphs. Each of those aspects should be cross-referenced and have text associated with them that details what is seen in each figure. This section should relay results; that is, we are interested in what the results are, rather than what they mean.\nThis section would also typically include tables of graphs of coefficient estimates based on the modeling. Various features of the estimates should be discussed, and differences between the models explained. It may be that different subsets of the data are considered separately. Again, all graphs and tables need to have text in plain language accompany them. A rough guide is that the amount of text should be at least equal to the amount of space taken up by the tables and graphs. For instance, if a full page is used to display a table of coefficient estimates, then that should be cross-referenced and accompanied by about a full page of text about that table."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-25",
    "href": "lectures/lecture-07-content.html#section-25",
    "title": "Introduction",
    "section": "",
    "text": "Discussion\nA discussion section may be the final section of a paper and would typically have four or five sub-sections.\nThe discussion section would typically begin with a sub-section that comprises a brief summary of what was done in the paper. This would be followed by two or three sub-sections that are devoted to the key things that we learn about the world from this paper. These sub-sections are the main opportunity to justify or detail the implications of the story being told in the paper. Typically, these sub-sections do not see newly introduced graphs or tables, but are instead focused on what we learn from those that were introduced in earlier sections. It may be that some of the results are discussed in relation to what others have found, and differences could be attempted to be reconciled here.\nFollowing these sub-sections of what we learn about the world, we would typically have a sub-section focused on some of the weaknesses of what was done. This could concern aspects such as the data that were used, the approach, and the model. In the case of the model we are especially concerned with those aspects that might affect the findings. This can be especially difficult in the case of machine learning models and @realml provide guidance for aspects to consider. And the final sub-section is typically a few paragraphs that specify what is left to learn, and how future work could proceed.\nIn general, we would expect this section to take at least 25 per cent of the total paper. This means that in an eight-page paper we would expect at least two pages of discussion."
  },
  {
    "objectID": "lectures/lecture-07-content.html#section-26",
    "href": "lectures/lecture-07-content.html#section-26",
    "title": "Introduction",
    "section": "",
    "text": "Brevity, typos, and grammar\nBrevity is important. This is partly because we write for the reader, and the reader has other priorities. But it is also because as the writer it forces us to consider what our most important points are, how we can best support them, and where our arguments are weakest. Jean Chrétien, is a former Canadian prime minister. In @jeanchretien [p. 105] he wrote that he used to ask “\\(\\dots\\)the officials to summarize their documents in two or three pages and attach the rest of the materials as background information. I soon discovered that this was a problem only for those who didn’t really know what they were talking about” .\nThis experience is not unique to Canada and it is not new. In @institueforgovernment Oliver Letwin, the former British cabinet member, describes there being “a huge amount of terrible guff, at huge, colossal, humongous length coming from some departments” and how he asked “for them to be one quarter of the length”. He found that the departments were able to accommodate this request without losing anything important. Winston Churchill asked for brevity during the Second World War, saying “the discipline of setting out the real points concisely will prove an aid to clearer thinking.” The letter from Szilard and Einstein to FDR that was the catalyst for the Manhattan Project was only two pages!\n@zinsser goes further and describes “the secret of good writing” being “to strip every sentence to its cleanest components.” Every sentence should be simplified to its essence. And every word that does not contribute should be removed.\nUnnecessary words, typos, and grammatical issues should be removed from papers. These mistakes affect the credibility of claims. If the reader cannot trust you to use a spell-checker, then why should they trust you to use logistic regression? RStudio has a spell-checker built in, but Microsoft Word and Google Docs are useful additional checks. Copy from the Quarto document and paste into Word, then look for the red and green lines, and fix them in the Quarto document.\nWe are not worried about the n-th degree of grammatical content. Instead, we are interested in grammar and sentence structure that occurs in conversational language use [@stephenking, p. 118]. The way to develop comfort is by reading widely and asking others to also read your work. Another useful tactic is to read your writing aloud, which can be useful for detecting odd sentences based on how they sound. One small aspect to check that will regularly come up is that any number from one to ten should be written as words, while 11 and over should be written as numbers."
  },
  {
    "objectID": "lectures/lecture-07-content.html#rules-for-writing",
    "href": "lectures/lecture-07-content.html#rules-for-writing",
    "title": "Introduction",
    "section": "Rules for writing",
    "text": "Rules for writing\nA variety of authors have established rules for writing. This famously includes those of @politicsandtheenglishlanguage which were reimagined by @johnsontheeconomist. A further reimagining of rules for writing, focused on telling stories with data, could be:\n\nFocus on the reader and their needs. Everything else is commentary.\nEstablish a structure and then rely on that to tell the story.\nWrite a first draft as quickly as possible.\nRewrite that draft extensively.\nBe concise and direct. Remove as many words as possible.\nUse words precisely. For instance, stock prices rise or fall, rather than improve or worsen.\nUse short sentences where possible.\nAvoid jargon.\nWrite as though your work will be on the front page of a newspaper.\nNever claim novelty or that you are the “first to study X”—there is always someone else who got there first.\n\n@fiske2021words have a list of rules for scientific papers and the appendix of @pineau2021improving provides a checklist for machine learning papers. But perhaps the last word should be from @Savage2019:\n\n[T]ry to write the best version of your paper: the one that you like. You can’t please an anonymous reader, but you should be able to please yourself. Your paper—you hope—is for posterity.\n@Savage2019 [p. 442]"
  },
  {
    "objectID": "lectures/lecture-07-content.html#references",
    "href": "lectures/lecture-07-content.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-07-content.html#footnotes",
    "href": "lectures/lecture-07-content.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile there is sometimes a need for a separate literature review section, another approach is to discuss relevant literature throughout the paper as appropriate. For instance, when there is literature relevant to the data then it should be discussed in this section, while literature relevant to the model, results, or discussion should be mentioned as appropriate in those sections.↩︎"
  },
  {
    "objectID": "lectures/lecture-03-content.html",
    "href": "lectures/lecture-03-content.html",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "flowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)\n\n\n\n\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by @citeBarrett\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan @alexander2023telling"
  },
  {
    "objectID": "lectures/lecture-03-content.html#the-firehose",
    "href": "lectures/lecture-03-content.html#the-firehose",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "flowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#the-firehose-1",
    "href": "lectures/lecture-03-content.html#the-firehose-1",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "Whenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by @citeBarrett\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan @alexander2023telling"
  },
  {
    "objectID": "lectures/lecture-03-content.html#preliminaries-1",
    "href": "lectures/lecture-03-content.html#preliminaries-1",
    "title": "Drinking From the Firehose",
    "section": "preliminaries",
    "text": "preliminaries\n\nRStudio / CodeSpaces / Whatever…"
  },
  {
    "objectID": "lectures/lecture-03-content.html#preliminaries-2",
    "href": "lectures/lecture-03-content.html#preliminaries-2",
    "title": "Drinking From the Firehose",
    "section": "preliminaries",
    "text": "preliminaries\n\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")"
  },
  {
    "objectID": "lectures/lecture-03-content.html#import-libraries",
    "href": "lectures/lecture-03-content.html#import-libraries",
    "title": "Drinking From the Firehose",
    "section": "import libraries",
    "text": "import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-content.html#plan-1",
    "href": "lectures/lecture-03-content.html#plan-1",
    "title": "Drinking From the Firehose",
    "section": " plan",
    "text": "plan\n\n\nAustralian Elections\n\n\n\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-03-content.html#plan-2",
    "href": "lectures/lecture-03-content.html#plan-2",
    "title": "Drinking From the Firehose",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure 1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a “division” in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-1",
    "href": "lectures/lecture-03-content.html#simulate-1",
    "title": "Drinking From the Firehose",
    "section": " simulate",
    "text": "simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-2",
    "href": "lectures/lecture-03-content.html#simulate-2",
    "title": "Drinking From the Firehose",
    "section": " simulate",
    "text": "simulate\n\nWe’ll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-3",
    "href": "lectures/lecture-03-content.html#simulate-3",
    "title": "Drinking From the Firehose",
    "section": " simulate",
    "text": "simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we’re creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named “Division”. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it’s used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options — “Liberal”, “Labor”, “National”, “Green”, and “Other” — into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the “Division” column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we’re picking “with replacement” (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn’t match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-4",
    "href": "lectures/lecture-03-content.html#simulate-4",
    "title": "Drinking From the Firehose",
    "section": " simulate",
    "text": "simulate\n🤘 We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party  \n      &lt;int&gt; &lt;chr&gt;  \n 1        1 Liberal\n 2        2 Green  \n 3        3 Other  \n 4        4 Liberal\n 5        5 Green  \n 6        6 Liberal\n 7        7 Labor  \n 8        8 Labor  \n 9        9 Labor  \n10       10 Green  \n# ℹ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-1",
    "href": "lectures/lecture-03-content.html#acquire-1",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-2",
    "href": "lectures/lecture-03-content.html#acquire-2",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nWe’ll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n\n\n\n✌️ R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief “Ode to the here package,” “here here,” which you can read… here."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-3",
    "href": "lectures/lecture-03-content.html#acquire-3",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n🤘 We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 × 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA…\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather… KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge… ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE…\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ℹ 141 more rows\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-4",
    "href": "lectures/lecture-03-content.html#acquire-4",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA…\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri… KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-5",
    "href": "lectures/lecture-03-content.html#acquire-5",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma… STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO…\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-6",
    "href": "lectures/lecture-03-content.html#acquire-6",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n“We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.” [@alexander2023telling]\n\n\nLet’s clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-7",
    "href": "lectures/lecture-03-content.html#acquire-7",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let’s look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ℹ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-8",
    "href": "lectures/lecture-03-content.html#acquire-8",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n\n\n✌️ R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-9",
    "href": "lectures/lecture-03-content.html#acquire-9",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-10",
    "href": "lectures/lecture-03-content.html#acquire-10",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n\n\n\n✌️ R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet’s rename."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-11",
    "href": "lectures/lecture-03-content.html#acquire-11",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-12",
    "href": "lectures/lecture-03-content.html#acquire-12",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let’s simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-13",
    "href": "lectures/lecture-03-content.html#acquire-13",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-14",
    "href": "lectures/lecture-03-content.html#acquire-14",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! 😎"
  },
  {
    "objectID": "lectures/lecture-03-content.html#aus_elections_clean_path",
    "href": "lectures/lecture-03-content.html#aus_elections_clean_path",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nLet’s save the cleaned data so that we can start with it data in the next stage. We’ll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-1",
    "href": "lectures/lecture-03-content.html#explore-understand-1",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-2",
    "href": "lectures/lecture-03-content.html#explore-understand-2",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nI’m using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won’t work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-3",
    "href": "lectures/lecture-03-content.html#explore-understand-3",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n😎"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-4",
    "href": "lectures/lecture-03-content.html#explore-understand-4",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-5",
    "href": "lectures/lecture-03-content.html#explore-understand-5",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nRemember, we’re trying to make something like this.\n\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer’s understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer’s eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-6",
    "href": "lectures/lecture-03-content.html#explore-understand-6",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\nLet’s visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it’s cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\n\n\n\n\n\n\nFigure 2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-7",
    "href": "lectures/lecture-03-content.html#explore-understand-7",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels\n\n\n\n\n\n\n\nFigure 3: Number of seats won, by political party, at the 2022 Australian Federal Election. 😎"
  },
  {
    "objectID": "lectures/lecture-03-content.html#section",
    "href": "lectures/lecture-03-content.html#section",
    "title": "Drinking From the Firehose",
    "section": "",
    "text": "cleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure 4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-03-content.html#share-1",
    "href": "lectures/lecture-03-content.html#share-1",
    "title": "Drinking From the Firehose",
    "section": " share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R [@citeR] including the tidyverse [@tidyverse] and janitor [@janitor]. We then created a graph of the number of seats that each political party won (Figure 3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, @biasbehindbars and @bronnerftw."
  },
  {
    "objectID": "lectures/lecture-03-content.html#plan-4",
    "href": "lectures/lecture-03-content.html#plan-4",
    "title": "Drinking From the Firehose",
    "section": " plan",
    "text": "plan\n\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure 5 (a) (next slide).\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure 5 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-03-content.html#plan-5",
    "href": "lectures/lecture-03-content.html#plan-5",
    "title": "Drinking From the Firehose",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a table\n\n\n\n\n\n\n\nFigure 5: Sketches of a dataset and table of the average number of beds occupied each month for shelters in Toronto."
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-6",
    "href": "lectures/lecture-03-content.html#simulate-6",
    "title": "Drinking From the Firehose",
    "section": " simulate",
    "text": "simulate\n\n\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process. When we turn to analysis, it will provide us with a guide. Conducting analysis without first using simulation can be thought of as shooting arrows without a target—while you are certainly doing something, it is not clear whether you are doing it well."
  },
  {
    "objectID": "lectures/lecture-03-content.html#simulate-7",
    "href": "lectures/lecture-03-content.html#simulate-7",
    "title": "Drinking From the Firehose",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            ) # Draw 1,095 times from the Poisson distribution\n    )\n\nsimulated_occupancy_data\n\n# A tibble: 1,095 × 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ℹ 1,085 more rows\n\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it later in the course."
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-16",
    "href": "lectures/lecture-03-content.html#acquire-16",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    # Each package is associated with a unique id  found in the \"For\n    # Developers\" tab of the relevant page from Open Data Toronto\n    # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    # Within that package, we are interested in the 2021 dataset\n    filter(name ==\n        \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    # Having reduced the dataset to one row we can get the resource\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-17",
    "href": "lectures/lecture-03-content.html#acquire-17",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 × 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME    \n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                \n1     1 21-01-01                    24 COSTI Immigrant Serv…\n2     2 21-01-01                    24 COSTI Immigrant Serv…\n3     3 21-01-01                    24 COSTI Immigrant Serv…\n4     4 21-01-01                    24 COSTI Immigrant Serv…\n5     5 21-01-01                    24 COSTI Immigrant Serv…\n6     6 21-01-01                    24 COSTI Immigrant Serv…\n# ℹ 28 more variables: SHELTER_ID &lt;dbl&gt;,\n#   SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;,\n#   LOCATION_POSTAL_CODE &lt;chr&gt;, …"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-18",
    "href": "lectures/lecture-03-content.html#acquire-18",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nWe’ll change the names to make them easier to type using clean_names(), and select() the relevant columns.\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6"
  },
  {
    "objectID": "lectures/lecture-03-content.html#acquire-19",
    "href": "lectures/lecture-03-content.html#acquire-19",
    "title": "Drinking From the Firehose",
    "section": " acquire",
    "text": "acquire\n\nAll that remains for this step is to save the cleaned dataset.\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\nWHERE ARE THESE NAs COMING FROM?"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-8",
    "href": "lectures/lecture-03-content.html#explore-understand-8",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\ntoronto_shelters_clean\n\n# A tibble: 50,944 × 2\n   occupancy_date occupied_beds\n   &lt;date&gt;                 &lt;dbl&gt;\n 1 2021-01-01                NA\n 2 2021-01-01                NA\n 3 2021-01-01                NA\n 4 2021-01-01                NA\n 5 2021-01-01                NA\n 6 2021-01-01                 6\n 7 2021-01-01                NA\n 8 2021-01-01                NA\n 9 2021-01-01                NA\n10 2021-01-01                NA\n# ℹ 50,934 more rows"
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-9",
    "href": "lectures/lecture-03-content.html#explore-understand-9",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\nTable 1: Shelter usage in Toronto in 2021\n\n\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426\n\n\n\n\n\n\n\n\n\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments—“label” and “abbr”—to get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in sec-farm-data and sec-exploratory-data-analysis. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create tbl-homelessoccupancyd."
  },
  {
    "objectID": "lectures/lecture-03-content.html#explore-understand-10",
    "href": "lectures/lecture-03-content.html#explore-understand-10",
    "title": "Drinking From the Firehose",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable(\n        col.names = c(\"Month\", \"Average daily number of&lt;br&gt;occupied beds (per shelter)\"),\n        digits = 1\n    )\n\n\n\n\nMonth\nAverage daily number ofoccupied beds (per shelter)\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5\n\n\n\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (tbl-homelessoccupancy). In particular we make the column names easier to read, and only show an appropriate number of decimal places."
  },
  {
    "objectID": "lectures/lecture-03-content.html#share-3",
    "href": "lectures/lecture-03-content.html#share-3",
    "title": "Drinking From the Firehose",
    "section": " share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R [@citeR] as well as the tidyverse [@Wickham2017], janitor [@janitor], opendatatoronto [@citeSharla], lubridate [@GrolemundWickham2011], and knitr [@citeknitr]. We then made a table of the average number of occupied beds each night for each month (tbl-homelessoccupancy).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (tbl-homelessoccupancy). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\n\n\n\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of @hao2019, that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Quantitative Research Methods",
    "section": "",
    "text": "Course Instructor John McLevey (he/him) Professor, Department of Sociology Memorial University\n\n  john.mclevey@uwaterloo.ca Note: I do not check or respond to email in the evenings or on weekends.\n\n\nWhere is class? CP-2003 (Chemistry-Physics, Computer Lab) When is class? Tuesdays & Thursdays, 1:30 - 2:50 pm Office Hours: A4054, Tuesdays & Thursdays, 3:00 - 4:00 pm\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)\n\n\n\n\n👋 Hello!\n\n\n\n\n\n\nThis course is built around Rohan Alexander’s (2023) Telling Stories with Data.\n\n\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s (2023) Telling Stories with Data, you will learn how to:\n\nplan an analysis and sketch your data and endpoint\nsimulate some data to “force you into the details”\nacquire, assess, and prepare empirical data for analysis\nexplore and analyze data by creating visualizations and fitting models\nshare the results of your work with the world!\n\nYou will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear models, generalized linear models, and multilevel models. Reproducibility and research ethics are considered throughout the workflow, and the entire course.\n\n\nAbout the Instructor\nJohn McLevey (he/him) Pronounced like mic-Leave-ee\n\n\nLand Acknowledgement\nWe acknowledge that the lands on which Memorial University’s campuses are situated are in the traditional territories of diverse Indigenous groups, and we acknowledge with respect the diverse histories and cultures of the Beothuk, Mi’kmaq, Innu, and Inuit of this province.\n\n\n\n\n\nReferences\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#the-firehose",
    "href": "lectures/lecture-03-slides.html#the-firehose",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "1.1 the firehose",
    "text": "1.1 the firehose\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s (2023) Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#the-firehose-1",
    "href": "lectures/lecture-03-slides.html#the-firehose-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "1.2 the firehose",
    "text": "1.2 the firehose\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by Barrett (2021)\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan Alexander (2023)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#preliminaries-1",
    "href": "lectures/lecture-03-slides.html#preliminaries-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "2.1 preliminaries",
    "text": "2.1 preliminaries\n\nRStudio / CodeSpaces / Whatever…"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#preliminaries-2",
    "href": "lectures/lecture-03-slides.html#preliminaries-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "2.2 preliminaries",
    "text": "2.2 preliminaries\n\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#import-libraries",
    "href": "lectures/lecture-03-slides.html#import-libraries",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "2.3 import libraries",
    "text": "2.3 import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#plan-1",
    "href": "lectures/lecture-03-slides.html#plan-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.1  plan",
    "text": "3.1  plan\n\n3.1.1 Australian Elections\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#plan-2",
    "href": "lectures/lecture-03-slides.html#plan-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.2  plan",
    "text": "3.2  plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure 1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a “division” in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-1",
    "href": "lectures/lecture-03-slides.html#simulate-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.1  simulate",
    "text": "4.1  simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-2",
    "href": "lectures/lecture-03-slides.html#simulate-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.2  simulate",
    "text": "4.2  simulate\n\nWe’ll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-3",
    "href": "lectures/lecture-03-slides.html#simulate-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.3  simulate",
    "text": "4.3  simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we’re creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named “Division”. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it’s used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options — “Liberal”, “Labor”, “National”, “Green”, and “Other” — into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the “Division” column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we’re picking “with replacement” (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn’t match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-4",
    "href": "lectures/lecture-03-slides.html#simulate-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.4  simulate",
    "text": "4.4  simulate\n🤘 We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 National\n 2        2 Other   \n 3        3 Green   \n 4        4 Green   \n 5        5 Green   \n 6        6 Labor   \n 7        7 Liberal \n 8        8 Labor   \n 9        9 National\n10       10 Labor   \n# ℹ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-1",
    "href": "lectures/lecture-03-slides.html#acquire-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.1  acquire",
    "text": "5.1  acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-2",
    "href": "lectures/lecture-03-slides.html#acquire-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.2  acquire",
    "text": "5.2  acquire\n\nWe’ll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n✌️ R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief “Ode to the here package,” “here here,” which you can read… here."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-3",
    "href": "lectures/lecture-03-slides.html#acquire-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.3  acquire",
    "text": "5.3  acquire\n🤘 We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 × 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA…\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather… KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge… ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE…\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ℹ 141 more rows\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-4",
    "href": "lectures/lecture-03-slides.html#acquire-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.4  acquire",
    "text": "5.4  acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA…\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri… KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-5",
    "href": "lectures/lecture-03-slides.html#acquire-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.5  acquire",
    "text": "5.5  acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma… STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO…\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-6",
    "href": "lectures/lecture-03-slides.html#acquire-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.6  acquire",
    "text": "5.6  acquire\n\n“We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.” (Alexander 2023)\n\n\nLet’s clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-7",
    "href": "lectures/lecture-03-slides.html#acquire-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.7  acquire",
    "text": "5.7  acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let’s look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ℹ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-8",
    "href": "lectures/lecture-03-slides.html#acquire-8",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.8  acquire",
    "text": "5.8  acquire\n\n\n\n✌️ R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-9",
    "href": "lectures/lecture-03-slides.html#acquire-9",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.9  acquire",
    "text": "5.9  acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-10",
    "href": "lectures/lecture-03-slides.html#acquire-10",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.10  acquire",
    "text": "5.10  acquire\n\n\n\n\n✌️ R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet’s rename."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-11",
    "href": "lectures/lecture-03-slides.html#acquire-11",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.11  acquire",
    "text": "5.11  acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-12",
    "href": "lectures/lecture-03-slides.html#acquire-12",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.12  acquire",
    "text": "5.12  acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let’s simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-13",
    "href": "lectures/lecture-03-slides.html#acquire-13",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.13  acquire",
    "text": "5.13  acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-14",
    "href": "lectures/lecture-03-slides.html#acquire-14",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.14  acquire",
    "text": "5.14  acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! 😎"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#aus_elections_clean_path",
    "href": "lectures/lecture-03-slides.html#aus_elections_clean_path",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.15  acquire",
    "text": "5.15  acquire\n\nLet’s save the cleaned data so that we can start with it data in the next stage. We’ll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-1",
    "href": "lectures/lecture-03-slides.html#explore-understand-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.1  explore / understand",
    "text": "6.1  explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-2",
    "href": "lectures/lecture-03-slides.html#explore-understand-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.2  explore / understand",
    "text": "6.2  explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n✌️ R Tip\n\n\nI’m using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won’t work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-3",
    "href": "lectures/lecture-03-slides.html#explore-understand-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.3  explore / understand",
    "text": "6.3  explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n😎"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-4",
    "href": "lectures/lecture-03-slides.html#explore-understand-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.4  explore / understand",
    "text": "6.4  explore / understand\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-5",
    "href": "lectures/lecture-03-slides.html#explore-understand-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.5  explore / understand",
    "text": "6.5  explore / understand\n\n\n\n\n\n\nRemember, we’re trying to make something like this.\n\n\n\n\n\n\n✌️ R Tip\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer’s understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer’s eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-6",
    "href": "lectures/lecture-03-slides.html#explore-understand-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.6  explore / understand",
    "text": "6.6  explore / understand\n\nLet’s visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it’s cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-6-output",
    "href": "lectures/lecture-03-slides.html#explore-understand-6-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.6  explore / understand",
    "text": "6.6  explore / understand\n\n\n\n\n\n\n\nFigure 2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-7",
    "href": "lectures/lecture-03-slides.html#explore-understand-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.7  explore / understand",
    "text": "6.7  explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-7-output",
    "href": "lectures/lecture-03-slides.html#explore-understand-7-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.7  explore / understand",
    "text": "6.7  explore / understand\n\n\n\n\n\n\n\nFigure 3: Number of seats won, by political party, at the 2022 Australian Federal Election. 😎"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#section",
    "href": "lectures/lecture-03-slides.html#section",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.8 ",
    "text": "6.8 \ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure 4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#share-1",
    "href": "lectures/lecture-03-slides.html#share-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "7.1  share",
    "text": "7.1  share\nExample taken directly from Alexander (2023), here.\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure 3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#plan-4",
    "href": "lectures/lecture-03-slides.html#plan-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "9.1  plan",
    "text": "9.1  plan\n\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure 5 (a) (next slide).\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure 5 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#plan-5",
    "href": "lectures/lecture-03-slides.html#plan-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "9.2  plan",
    "text": "9.2  plan\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a table\n\n\n\n\n\n\n\nFigure 5: Sketches of a dataset and table of the average number of beds occupied each month for shelters in Toronto."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-6",
    "href": "lectures/lecture-03-slides.html#simulate-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "10.1  simulate",
    "text": "10.1  simulate\n\n\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process. When we turn to analysis, it will provide us with a guide. Conducting analysis without first using simulation can be thought of as shooting arrows without a target—while you are certainly doing something, it is not clear whether you are doing it well."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-7",
    "href": "lectures/lecture-03-slides.html#simulate-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "10.2  simulate",
    "text": "10.2  simulate\n\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            ) # Draw 1,095 times from the Poisson distribution\n    )\n\nsimulated_occupancy_data\n\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it later in the course."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#simulate-7-output",
    "href": "lectures/lecture-03-slides.html#simulate-7-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "10.2  simulate",
    "text": "10.2  simulate\n\n# A tibble: 1,095 × 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ℹ 1,085 more rows"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-16",
    "href": "lectures/lecture-03-slides.html#acquire-16",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.1  acquire",
    "text": "11.1  acquire\n\n\ntoronto_shelters &lt;-\n    # Each package is associated with a unique id  found in the \"For\n    # Developers\" tab of the relevant page from Open Data Toronto\n    # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    # Within that package, we are interested in the 2021 dataset\n    filter(name ==\n        \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    # Having reduced the dataset to one row we can get the resource\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-17",
    "href": "lectures/lecture-03-slides.html#acquire-17",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.2  acquire",
    "text": "11.2  acquire\n\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 × 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME    \n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                \n1     1 21-01-01                    24 COSTI Immigrant Serv…\n2     2 21-01-01                    24 COSTI Immigrant Serv…\n3     3 21-01-01                    24 COSTI Immigrant Serv…\n4     4 21-01-01                    24 COSTI Immigrant Serv…\n5     5 21-01-01                    24 COSTI Immigrant Serv…\n6     6 21-01-01                    24 COSTI Immigrant Serv…\n# ℹ 28 more variables: SHELTER_ID &lt;dbl&gt;,\n#   SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;,\n#   LOCATION_POSTAL_CODE &lt;chr&gt;, …"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-18",
    "href": "lectures/lecture-03-slides.html#acquire-18",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.3  acquire",
    "text": "11.3  acquire\n\nWe’ll change the names to make them easier to type using clean_names(), and select() the relevant columns.\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#acquire-19",
    "href": "lectures/lecture-03-slides.html#acquire-19",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.4  acquire",
    "text": "11.4  acquire\n\nAll that remains for this step is to save the cleaned dataset.\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\nWHERE ARE THESE NAs COMING FROM?"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-8",
    "href": "lectures/lecture-03-slides.html#explore-understand-8",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.1  explore / understand",
    "text": "12.1  explore / understand\n\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\ntoronto_shelters_clean\n\n# A tibble: 50,944 × 2\n   occupancy_date occupied_beds\n   &lt;date&gt;                 &lt;dbl&gt;\n 1 2021-01-01                NA\n 2 2021-01-01                NA\n 3 2021-01-01                NA\n 4 2021-01-01                NA\n 5 2021-01-01                NA\n 6 2021-01-01                 6\n 7 2021-01-01                NA\n 8 2021-01-01                NA\n 9 2021-01-01                NA\n10 2021-01-01                NA\n# ℹ 50,934 more rows"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-9",
    "href": "lectures/lecture-03-slides.html#explore-understand-9",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.2  explore / understand",
    "text": "12.2  explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments—“label” and “abbr”—to get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in sec-farm-data and sec-exploratory-data-analysis. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create tbl-homelessoccupancyd."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-9-output",
    "href": "lectures/lecture-03-slides.html#explore-understand-9-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.2  explore / understand",
    "text": "12.2  explore / understand\n\n\n\nTable 1: Shelter usage in Toronto in 2021\n\n\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-10",
    "href": "lectures/lecture-03-slides.html#explore-understand-10",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.3  explore / understand",
    "text": "12.3  explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable(\n        col.names = c(\"Month\", \"Average daily number of&lt;br&gt;occupied beds (per shelter)\"),\n        digits = 1\n    )\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (tbl-homelessoccupancy). In particular we make the column names easier to read, and only show an appropriate number of decimal places."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#explore-understand-10-output",
    "href": "lectures/lecture-03-slides.html#explore-understand-10-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.3  explore / understand",
    "text": "12.3  explore / understand\n\n\n\n\nMonth\nAverage daily number ofoccupied beds (per shelter)\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#share-3",
    "href": "lectures/lecture-03-slides.html#share-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "13.1  share",
    "text": "13.1  share\nExample taken directly from Alexander (2023), here.\n\n\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R (R Core Team 2023) as well as the tidyverse (Wickham 2017), janitor (Firke 2023), opendatatoronto (Gelfand 2022), lubridate (Grolemund and Wickham 2011), and knitr (Xie 2023). We then made a table of the average number of occupied beds each night for each month (tbl-homelessoccupancy).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (tbl-homelessoccupancy). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\n\n\n\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of Hao (2019), that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "lectures/lecture-03-slides.html#references",
    "href": "lectures/lecture-03-slides.html#references",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "13.2 References",
    "text": "13.2 References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nBarrett, Malcolm. 2021. Data Science as an Atomic Habit. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\nBronner, Laura. 2020. “Why Statistics Don’t Capture the Full Extent of the Systemic Bias in Policing.” FiveThirtyEight, June. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\nCardoso, Tom. 2020. “Bias behind bars: A Globe investigation finds a prison system stacked against Black and Indigenous inmates.” The Globe and Mail, October. https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHao, Karen. 2019. “This is How AI Bias Really Happens—And Why It’s So Hard To Fix.” MIT Technology Review, February. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2017. tidyverse: Easily Install and Load the “Tidyverse”. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "lectures/lecture-04-content.html",
    "href": "lectures/lecture-04-content.html",
    "title": "Third Example",
    "section": "",
    "text": "library(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-04-content.html#import-libraries",
    "href": "lectures/lecture-04-content.html#import-libraries",
    "title": "Third Example",
    "section": "",
    "text": "library(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-04-content.html#plan-1",
    "href": "lectures/lecture-04-content.html#plan-1",
    "title": "Third Example",
    "section": " plan",
    "text": "plan\n\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure 1 (a) (next slide). We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure 1 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-04-content.html#plan-2",
    "href": "lectures/lecture-04-content.html#plan-2",
    "title": "Third Example",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\n\n\n\nFigure 1: Sketches of a dataset and graph about the neonatal mortality rate (NMR)"
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-1",
    "href": "lectures/lecture-04-content.html#simulate-1",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\n\nTo simulate some data that aligns with our plan, we will need three columns: country, year, and NMR. We can do this by repeating the name of each country 50 times with rep(), and enabling the passing of 50 years. Then we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country."
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-2",
    "href": "lectures/lecture-04-content.html#simulate-2",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", 50), rep(\"Australia\", 50),\n                rep(\"Canada\", 50), rep(\"Kenya\", 50)\n            ),\n        year =\n            rep(c(1971:2020), 4),\n        nmr =\n            runif(n = 200, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-3",
    "href": "lectures/lecture-04-content.html#simulate-3",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable."
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-4",
    "href": "lectures/lecture-04-content.html#simulate-4",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n                rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)\n            ),\n        year =\n            rep(c(1:number_of_years + 1970), 4),\n        nmr =\n            runif(n = number_of_years * 4, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place."
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-5",
    "href": "lectures/lecture-04-content.html#simulate-5",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat “country” is exclusively one of these four: “Argentina”, “Australia”, “Canada”, or “Kenya”.\nConversely, “country” contains all those four countries.\nThat “year” is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat “nmr” is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass."
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-6",
    "href": "lectures/lecture-04-content.html#simulate-6",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\nsimulated_nmr_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-04-content.html#simulate-7",
    "href": "lectures/lecture-04-content.html#simulate-7",
    "title": "Third Example",
    "section": " simulate",
    "text": "simulate\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others."
  },
  {
    "objectID": "lectures/lecture-04-content.html#section",
    "href": "lectures/lecture-04-content.html#section",
    "title": "Third Example",
    "section": "",
    "text": "The UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\nigme_data_path &lt;- here(\"data\", \"igme.csv\")\nigme_data_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/igme.csv\"\n\n\n\nraw_igme_data &lt;-\n    read_csv(\n        file =\n            \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n        show_col_types = FALSE\n    )\n\nwrite_csv(x = raw_igme_data, file = igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-1",
    "href": "lectures/lecture-04-content.html#section-1",
    "title": "Third Example",
    "section": "",
    "text": "raw_igme_data &lt;-\n    read_csv(\n        file = igme_data_path,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-2",
    "href": "lectures/lecture-04-content.html#section-2",
    "title": "Third Example",
    "section": "",
    "text": "With established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail()\n\nhead(raw_igme_data)\n\n# A tibble: 6 × 29\n  `Geographic area` Indicator              Sex   `Wealth Quintile` `Series Name`\n  &lt;chr&gt;             &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;        \n1 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n2 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n3 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n4 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n5 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n6 Afghanistan       Neonatal mortality ra… Total Total             Afghanistan …\n# ℹ 24 more variables: `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;, COUNTRY_NOTES &lt;chr&gt;, CONNECTION &lt;lgl&gt;,\n#   DEATH_CATEGORY &lt;lgl&gt;, CATEGORY &lt;chr&gt;, `Observation Status` &lt;chr&gt;,\n#   `Unit of measure` &lt;chr&gt;, `Series Category` &lt;chr&gt;, `Series Type` &lt;chr&gt;,\n#   STD_ERR &lt;dbl&gt;, REF_DATE &lt;dbl&gt;, `Age Group of Women` &lt;chr&gt;,\n#   `Time Since First Birth` &lt;chr&gt;, DEFINITION &lt;chr&gt;, INTERVAL &lt;dbl&gt;,\n#   `Series Method` &lt;chr&gt;, LOWER_BOUND &lt;dbl&gt;, UPPER_BOUND &lt;dbl&gt;, …"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-3",
    "href": "lectures/lecture-04-content.html#section-3",
    "title": "Third Example",
    "section": "",
    "text": "and what the names of the columns are with names()\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\""
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-4",
    "href": "lectures/lecture-04-content.html#section-4",
    "title": "Third Example",
    "section": "",
    "text": "We would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where “Sex” is “Total”, “Series Name” is “UN IGME estimate”, “Geographic area” is one of “Argentina”, “Australia”, “Canada”, and “Kenya”, and the “Indicator” is “Neonatal mortality rate”. After this we are interested in just a few columns: “geographic_area”, “time_period”, and “obs_value”."
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-5",
    "href": "lectures/lecture-04-content.html#section-5",
    "title": "Third Example",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    clean_names(raw_igme_data) |&gt;\n    filter(\n        sex == \"Total\",\n        series_name == \"UN IGME estimate\",\n        geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n        indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n    select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-6",
    "href": "lectures/lecture-04-content.html#section-6",
    "title": "Third Example",
    "section": "",
    "text": "We need to fix two other aspects: the class of “time_period” is character when we need it to be a year, and the name of “obs_value” should be “nmr” to be more informative."
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-7",
    "href": "lectures/lecture-04-content.html#section-7",
    "title": "Third Example",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    cleaned_igme_data |&gt;\n    mutate(\n        time_period = str_remove(time_period, \"-06\"),\n        time_period = as.integer(time_period)\n    ) |&gt;\n    filter(time_period &gt;= 1971) |&gt;\n    rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-8",
    "href": "lectures/lecture-04-content.html#section-8",
    "title": "Third Example",
    "section": "",
    "text": "Finally, we can check that our dataset passes the tests that we developed based on the simulated dataset."
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-9",
    "href": "lectures/lecture-04-content.html#section-9",
    "title": "Third Example",
    "section": "",
    "text": "cleaned_igme_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-10",
    "href": "lectures/lecture-04-content.html#section-10",
    "title": "Third Example",
    "section": "",
    "text": "All that remains is to save the nicely cleaned dataset.\n\ncleaned_igme_data_path &lt;- here(\"data\", \"cleaned_igme_data.csv\")\nwrite_csv(x = cleaned_igme_data, file = cleaned_igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-11",
    "href": "lectures/lecture-04-content.html#section-11",
    "title": "Third Example",
    "section": "",
    "text": "We would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\ncleaned_igme_data &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_igme_data.csv\"),\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-04-content.html#section-12",
    "href": "lectures/lecture-04-content.html#section-12",
    "title": "Third Example",
    "section": "",
    "text": "We can now make a graph of how NMR has changed over time and the differences between countries (Figure 2).\n\n\ncleaned_igme_data |&gt;\n    ggplot(aes(x = year, y = nmr, color = country)) +\n    geom_point() +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)"
  },
  {
    "objectID": "lectures/lecture-04-content.html#share-1",
    "href": "lectures/lecture-04-content.html#share-1",
    "title": "Third Example",
    "section": " share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries—Argentina, Australia, Canada, and Kenya—over the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R [@citeR].\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure 2). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  },
  {
    "objectID": "lectures/lecture-05-notes.html",
    "href": "lectures/lecture-05-notes.html",
    "title": "Reproducible Research with R and RStudio",
    "section": "",
    "text": "Required:  RA CH 3\nRecommended:  KH Ch 2\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#reading-assignment",
    "href": "lectures/lecture-05-notes.html#reading-assignment",
    "title": "Reproducible Research with R and RStudio",
    "section": "",
    "text": "Required:  RA CH 3\nRecommended:  KH Ch 2"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#lecture-slides",
    "href": "lectures/lecture-05-notes.html#lecture-slides",
    "title": "Reproducible Research with R and RStudio",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#key-concepts-and-skills",
    "href": "lectures/lecture-05-notes.html#key-concepts-and-skills",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.1 Key concepts and skills",
    "text": "1.1 Key concepts and skills\nKey concepts and skills\n\nReproducibility typically begins as something that someone imposes on you. It can be onerous and annoying. This typically lasts until you need to revisit a project after a small break. At that point you typically realize that reproducibility is not just a requirement for data science because it is the only way that we can make genuine progress, but because it helps us help ourselves.\nReproducibility implies sharing data, code, and environment. This is enhanced by using Quarto, R Projects, and Git and GitHub: Quarto builds documents that integrate normal text and R code; R Projects enable a file structure that is not dependent on a user’s personal directory set-up; and Git and GitHub make it easier to share code and data.\nThis is not an unimpeachable workflow, but one that is good enough and provides many of the benefits. We will improve various aspects of it through various tools, but improving code structure and comments goes a long way.\nThere are always errors that occur, and it is important to recognize that debugging is a skill that improves with practice. But one key aspect of being able to get help is to be able to make a reproducible example others can use."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#software-and-packages",
    "href": "lectures/lecture-05-notes.html#software-and-packages",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.2 Software and packages",
    "text": "1.2 Software and packages\n\n\n\nBase R (R Core Team 2023)\nAER (Kleiber and Zeileis 2008)\nfuture (Bengtsson 2021)\ngitcreds (Csárdi 2022)\nknitr (Xie 2023)\nlintr (Hester et al. 2022)\nrenv (Ushey 2022)\nreprex (Bryan et al. 2022)\nstyler (Müller and Walthert 2022)\ntidyverse (Wickham et al. 2019)\ntinytex (Xie 2019)\nusethis (Wickham, Bryan, and Barrett 2022)\n\n\n\nlibrary(AER)\nlibrary(future)\nlibrary(gitcreds)\nlibrary(knitr)\nlibrary(lintr)\nlibrary(renv)\nlibrary(reprex)\nlibrary(styler)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(usethis)"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section",
    "href": "lectures/lecture-05-notes.html#section",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.3 ",
    "text": "1.3 \n\nThe number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\\(\\dots\\) So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty.\nFrançois Chollet, 20 February 2020.\n\nIf science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is a critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, there is a need for reproducible data science workflows.\nM. Alexander (2019) defines reproducible research as that which can be exactly redone, given all the materials used. This underscores the importance of providing the code, data, and environment. The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables. Ironically, there are different definitions of reproducibility between disciplines. Barba (2018) surveys a variety of disciplines and concludes that the predominant language usage implies the following definitions:\n\nReproducible research is when “[a]uthors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.”\nA replication is a study “that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.”\n\nRegardless of what it is specifically called, Gelman (2016) identifies how large an issue the lack of it is in various social sciences. Work that is not reproducible does not contribute to our stock of knowledge about the world. This is wasteful and potentially even unethical. Since Gelman (2016), a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. That is also the case in the life sciences (Heil et al. 2021), cancer research (Begley and Ellis 2012; Mullard 2021), and computer science (Pineau et al. 2021).\nSome of the examples that Gelman (2016) talks about are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created “nudge” units that implement public policy (Sunstein and Reisch 2017) even though there is evidence that some of the claims lack credibility (Maier et al. 2022; Szaszi et al. 2022). Governments are increasingly using algorithms that they do not make open (Chouldechova et al. 2018). And Herndon, Ash, and Pollin (2014) document how research in economics that was used by governments to justify austerity policies following the 2007–2008 financial crisis turned out to not be reproducible.\nAt a minimum, and with few exceptions, we must release our code, datasets, and environment. Without these, it is difficult to know what a finding speaks to (Miyakawa 2020). More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked (Merali 2010; Hillel 2017; Silver 2020). Increasingly, following Buckheit and Donoho (1995), we consider a paper to be an advertisement, and for the associated code, data, and environment to be the actual work. Steve Jobs, a co-founder of Apple, talked about how people who are the best at their craft ensure that even the aspects of their work that no one else will ever see are as well finished and high quality as the aspects that are public facing (Isaacson 2011). The same is true in data science, where often one of the distinguishing aspects of high-quality work is that the README and code comments are as polished as, say, the abstract of the associated paper."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-1",
    "href": "lectures/lecture-05-notes.html#section-1",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.4 ",
    "text": "1.4 \nWorkflows exist within a cultural and social context, which imposes an additional ethical reason for the need for them to be reproducible. For instance, Wang and Kosinski (2018) train a neural network to distinguish between the faces of gay and heterosexual men. (Murphy (2017) provides a summary of the paper, the associated issues, and comments from its authors.) To do this, Wang and Kosinski (2018, 248) needed a dataset of photos of people that were “adult, Caucasian, fully visible, and of a gender that matched the one reported on the user’s profile”. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. The instructions provided to the Mechanical Turk workers for this task specify that Barack Obama, the 44th US President, who had a white mother and a black father, should be classified as “Black”; and that Latino is an ethnicity, rather than a race (Mattson 2017). The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.\nThis is just one specific concern about one part of the Wang and Kosinski (2018) workflow. Broader concerns are raised by others including Gelman, Mattson, and Simpson (2018). The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of Wang and Kosinski (2018) is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-2",
    "href": "lectures/lecture-05-notes.html#section-2",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.5 ",
    "text": "1.5 \nSome of the steps that we can take to make our work more reproducible include:\n\nEnsure the entire workflow is documented. This may involve addressing questions such as:\n\nHow was the original, unedited dataset obtained and is access likely to be persistent and available to others?\nWhat specific steps are being taken to transform the original, unedited data into the data that were analyzed, and how can this be made available to others?\nWhat analysis has been done, and how clearly can this be shared?\nHow has the final paper or report been built and to what extent can others follow that process themselves?\n\nNot worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to do the last, until you can do the first:\n\nCan you run your entire workflow again?\nCan another person run your entire workflow again?\nCan “future-you” run your entire workflow again?\nCan “future-another-person” run your entire workflow again?\n\nIncluding a detailed discussion about the limitations of the dataset and the approach in the final paper or report."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-3",
    "href": "lectures/lecture-05-notes.html#section-3",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.6 ",
    "text": "1.6 \nThe workflow that we advocate in this book is:\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s (2023) Telling Stories with Data workflow \n\n\n\n\nBut it can be alternatively considered as: “Think an awful lot, mostly read and write, sometimes code”."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-4",
    "href": "lectures/lecture-05-notes.html#section-4",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.7 ",
    "text": "1.7 \nThere are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes\n\nQuarto,\nR Projects\nGit and GitHub"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#quarto",
    "href": "lectures/lecture-05-notes.html#quarto",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.8 Quarto",
    "text": "1.8 Quarto\n\n1.8.1 Getting started\nQuarto integrates code and natural language in a way that is called “literate programming” (Knuth 1984). It is the successor to R Markdown, which was a variant of Markdown specifically designed to allow R code chunks to be included. Quarto uses a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a “What You See Is What You Get” (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level headings will look the same. But it means that we must designate or “mark up” how we would like certain aspects to appear. And it is only when we render the document that we get to see what it looks like. A visual editor option can also be used, and this hides the need for the user to do this mark-up themselves."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-5",
    "href": "lectures/lecture-05-notes.html#section-5",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.9 ",
    "text": "1.9 \nOne advantage of literate programming is that we get a “live” document in which code executes and then forms part of the document. Another advantage of Quarto is that similar code can compile into a variety of documents, including HTML and PDFs. Quarto also has default options for including a title, author, and date. One disadvantage is that it can take a while for a document to compile because the code needs to run.\nWe need to download Quarto from here. (Skip this step if you are using Posit Cloud because it is already installed.) We can then create a new Quarto document within RStudio: “File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “Quarto Document\\(\\dots\\)”.\nAfter opening a new Quarto document and selecting “Source” view, you will see the default top matter, contained within a pair of three dashes, as well as some examples of text showing a few of the markdown essential commands and R chunks, each of which are discussed further in the following sections."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-6",
    "href": "lectures/lecture-05-notes.html#section-6",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.10 ",
    "text": "1.10 \n\n1.10.1 Top matter\nTop matter consists of defining aspects such as the title, author, and date. It is contained within three dashes at the top of a Quarto document. For instance, the following would specify a title, a date that automatically updated to the date the document was rendered, and an author.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: html\n---"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-7",
    "href": "lectures/lecture-05-notes.html#section-7",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.11 ",
    "text": "1.11 \nAn abstract is a short summary of the paper, and we could add that to the top matter.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: html\n---"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-8",
    "href": "lectures/lecture-05-notes.html#section-8",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.12 ",
    "text": "1.12 \nBy default, Quarto will create an HTML document, but we can change the output format to produce a PDF. This uses LaTeX in the background and requires the installation of supporting packages. To do this install tinytex. But as it is used in the background we should not need to load it.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: pdf\n---"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-9",
    "href": "lectures/lecture-05-notes.html#section-9",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.13 ",
    "text": "1.13 \nWe can include references by specifying a BibTeX file in the top matter and then calling it within the text, as needed.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: pdf\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-10",
    "href": "lectures/lecture-05-notes.html#section-10",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.14 ",
    "text": "1.14 \nWe would need to make a separate file called “bibliography.bib” and save it next to the Quarto file. In the BibTeX file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with citation() and this can be added to the “bibliography.bib” file. The citation for a package can be found by including the package name, for instance citation(\"tidyverse\"), and again adding the output to the “.bib” file. It can be helpful to use Google Scholar or doi2bib to get citations for books or articles."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-11",
    "href": "lectures/lecture-05-notes.html#section-11",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.15 ",
    "text": "1.15 \nWe need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance “citeR”.\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@book{tellingstories,\n    title = {Telling Stories with Data},\n    author = {Rohan Alexander},\n    year = {2023},\n    publisher = {Chapman and Hall/CRC},\n    url = {https://tellingstorieswithdata.com}\n  }"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-12",
    "href": "lectures/lecture-05-notes.html#section-12",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.16 ",
    "text": "1.16 \nTo cite R in the Quarto document we then include @citeR, which would put brackets around the year: R Core Team (2023), or [@citeR], which would put brackets around the whole thing: (R Core Team 2023)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-13",
    "href": "lectures/lecture-05-notes.html#section-13",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.17 ",
    "text": "1.17 \nThe reference list at the end of the paper is automatically built based on calling the BibTeX file and including references in the paper. At the end of the Quarto document, include a heading “# References” and the actual citations will be included after that. When the Quarto file is rendered, Quarto sees these in the content, goes to the BibTeX file to get the reference details that it needs, builds the reference list, and then adds it at the end of the rendered document."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-14",
    "href": "lectures/lecture-05-notes.html#section-14",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.18 ",
    "text": "1.18 \n\n1.18.1 Essential commands\nQuarto uses a variation of Markdown as its underlying syntax. Essential Markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in RStudio: “Help” \\(\\rightarrow\\) “Markdown Quick Reference”. It is your choice as to whether you want to use the visual or source editor. But either way, it is good to understand these essentials because it will not always be possible to use a visual editor (for instance if you are quickly looking at a Quarto document in GitHub). As you get more experience it can be useful to use a text editor such as Sublime Text, or an alternative Integrated Development Environment such as VS Code.\n\nEmphasis: *italic*, **bold**\nHeaders (these go on their own line with a blank line before and after):\n\n         # First level header\n         \n         ## Second level header\n         \n         ### Third level header\n\nUnordered list, with sub-lists:\n\n    * Item 1\n    * Item 2\n        + Item 2a\n        + Item 2b\n\nOrdered list, with sub-lists:\n\n    1. Item 1\n    2. Item 2\n    3. Item 3\n        + Item 3a\n        + Item 3b\n\nURLs can be added: [this book](https://www.tellingstorieswithdata.com) results in this book.\nA paragraph is created by leaving a blank line.\n\nA paragraph about an idea, nicely spaced from the following paragraph.\n\nA paragraph about another idea, again spaced from the earlier paragraph."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-15",
    "href": "lectures/lecture-05-notes.html#section-15",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.19 ",
    "text": "1.19 \nOnce we have added some aspects, then we may want to see the actual document. To build the document click “Render”."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-16",
    "href": "lectures/lecture-05-notes.html#section-16",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.20 ",
    "text": "1.20 \n\n1.20.1 R chunks\nWe can include code for R and many other languages in code chunks within a Quarto document. When we render the document the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell Quarto that this is an R chunk. Anything inside this chunk will be considered R code and run as such. We use data from Kleiber and Zeileis (2008) who provide the R package AER to accompany their book Applied Econometrics with R. We could load the tidyverse and install and load AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-17",
    "href": "lectures/lecture-05-notes.html#section-17",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.21 ",
    "text": "1.21 \nThe output of that code is Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\n\nThere are various evaluation options that are available in chunks. We include these, each on a new line, by opening the line with the chunk-specific comment delimiter “#|” and then the option. Helpful options include:\n\necho: This controls whether the code itself is included in the document. For instance, #| echo: false would mean the code will be run and its output will show, but the code itself would not be included in the document.\ninclude: This controls whether the output of the code is included in the document. For instance, #| include: false would run the code, but would not result in any output, and the code itself would not be included in the document.\neval: This controls whether the code should be included in the document. For instance, #| eval: false would mean that the code is not run, and hence there would not be any output to include, but the code itself would be included in the document.\nwarning: This controls whether warnings should be included in the document. For instance, #| warning: false would mean that warnings are not included.\nmessage: This controls whether messages should be included in the document. For instance, #| message: false would mean that messages are not included in the document.\n\nFor instance, we could include the output, but not the code, and suppress any warnings.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nLeave a blank line on either side of an R chunk, otherwise it may not run properly. And use lower case for logical values, i.e. “false” not “FALSE”.\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then...\nThe Quarto document itself must load any datasets that are needed. It is not enough that they are in the environment. This is because the Quarto document evaluates the code in the document when it is rendered, not necessarily the environment.\n\n1.21.1 Equations\nWe can include equations by using LaTeX, which is based on the programming language TeX. We invoke math mode in LaTeX by using two dollar signs as opening and closing tags. Then whatever is inside is evaluated as LaTeX mark-up. For instance we can produce the compound interest formula with:\n$$\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n$$\n\\[\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-18",
    "href": "lectures/lecture-05-notes.html#section-18",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.22 ",
    "text": "1.22 \nLaTeX is a comprehensive mark-up language but we will mostly just use it to specify the model of interest. We include some examples here that contain the critical aspects we will draw on starting in ?@sec-its-just-a-linear-model.\n$$\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n$$\n\\[\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-19",
    "href": "lectures/lecture-05-notes.html#section-19",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.23 ",
    "text": "1.23 \nUnderscores are used to get subscripts: y_i for \\(y_i\\). And we can get a subscript of more than one item by surrounding it with curly braces: y_{i,c} for \\(y_{i,c}\\). In this case we wanted math mode within the line, and so we surround these with only one dollar sign as opening and closing tags.\nGreek letters are typically preceded by a backslash. Common Greek letters include: \\alpha for \\(\\alpha\\), \\beta for \\(\\beta\\), \\delta for \\(\\delta\\), \\epsilon for \\(\\epsilon\\), \\gamma for \\(\\gamma\\), \\lambda for \\(\\lambda\\), \\mu for \\(\\mu\\), \\phi for \\(\\phi\\), \\pi for \\(\\pi\\), \\Pi for \\(\\Pi\\), \\rho for \\(\\rho\\), \\sigma for \\(\\sigma\\), \\Sigma for \\(\\Sigma\\), \\tau for \\(\\tau\\), and \\theta for \\(\\theta\\)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-20",
    "href": "lectures/lecture-05-notes.html#section-20",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.24 ",
    "text": "1.24 \nLaTeX math mode assumes letters are variables and so makes them italic, but sometimes we want a word to appear in normal font because it is not a variable, such as “Normal”. In that case we surround it with \\mbox{}, for instance \\mbox{Normal} for \\(\\mbox{Normal}\\)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-21",
    "href": "lectures/lecture-05-notes.html#section-21",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.25 ",
    "text": "1.25 \nWe line up equations across multiple lines using \\begin{aligned} and \\end{aligned}. Then the item that is to be lined up is noted by an ampersand. The following is a model that we will estimate in ?@sec-multilevel-regression-with-post-stratification.\n$$\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-22",
    "href": "lectures/lecture-05-notes.html#section-22",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.26 ",
    "text": "1.26 \nFinally, certain functions are built into LaTeX. For instance, we can appropriately typeset “log” with \\log."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-23",
    "href": "lectures/lecture-05-notes.html#section-23",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.27 ",
    "text": "1.27 \n\n1.27.1 Cross-references\nIt can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, consider the following code."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-24",
    "href": "lectures/lecture-05-notes.html#section-24",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.28 ",
    "text": "1.28 \n\n```{r}\n#| label: fig-uniquename\n#| fig-cap: Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\n#| warning: false\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\")\n```\n\n\n\n\n\n\n\nFigure 2: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\n\nThen (@fig-uniquename) would produce: (Figure 2) as the name of the R chunk is fig-uniquename. We need to add “fig” to the start of the chunk name so that Quarto knows that this is a figure. We then include a “fig-cap:” in the R chunk that specifies a caption."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-25",
    "href": "lectures/lecture-05-notes.html#section-25",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.29 ",
    "text": "1.29 \nWe can add #| layout-ncol: 2 in an R chunk within a Quarto document to have two graphs appear side by side (Figure 3). Here Figure 3 (a) uses the minimal theme, and Figure 3 (b) uses the classic theme. These both cross-reference the same label #| label: fig-doctorgraphsidebyside in the R chunk, with an additional option added in the R chunk of #| fig-subcap: [\"Number of illnesses\",\"Number of visits to the doctor\"] which provides the sub-captions. The addition of a letter in-text is accomplished by adding “-1” and “-2” to the end of the label when it is used in-text: (@fig-doctorgraphsidebyside), @fig-doctorgraphsidebyside-1, and @fig-doctorgraphsidebyside-2 for (Figure 3), Figure 3 (a), and Figure 3 (b), respectively.\n```{r}\n#| eval: true\n#| warning: false\n#| label: fig-doctorgraphsidebyside\n#| fig-cap: \"Two variants of graphs\"\n#| fig-subcap: [\"Illnesses\",\"Visits to the doctor\"]\n#| layout-ncol: 2\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\") +\n    theme_minimal()\n\nDoctorVisits |&gt;\n    ggplot(aes(x = visits)) +\n    geom_histogram(stat = \"count\") +\n    theme_classic()\n```\n\n\n\n\n\n\n\n\n\n\n\n(a) Illnesses\n\n\n\n\n\n\n\n\n\n\n\n(b) Visits to the doctor\n\n\n\n\n\n\n\nFigure 3: Two variants of graphs"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-26",
    "href": "lectures/lecture-05-notes.html#section-26",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.30 ",
    "text": "1.30 \nWe can take a similar approach to cross-reference tables. For instance, (@tbl-docvisittable) will produce: (Table 1). In this case we specify “tbl” at the start of the label so that Quarto knows that it is a table. And we specify a caption for the table with “tbl-cap:”.\n\n```{r}\n#| label: tbl-docvisittable\n#| tbl-cap: \"Distribution of the number of doctor visits\"\n\nDoctorVisits |&gt;\n    count(visits) |&gt;\n    kable()\n```\n\n\n\nTable 1: Distribution of the number of doctor visits\n\n\n\n\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-27",
    "href": "lectures/lecture-05-notes.html#section-27",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.31 ",
    "text": "1.31 \nFinally, we can also cross-reference equations. To that we need to add a tag such as {#eq-macroidentity} which we then reference.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-gdpidentity}\nFor instance, we then use @eq-gdpidentity to produce Equation 1\n\\[\nY = C + I + G + (X - M)\n\\tag{1}\\]\nLabels should be relatively simple when using cross-references. In general, try to keep the names simple but unique, avoid punctuation, and stick to letters and hyphens. Try not to use underscores, because they can cause an error."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#r-projects-and-file-structure",
    "href": "lectures/lecture-05-notes.html#r-projects-and-file-structure",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.32 R Projects and file structure",
    "text": "1.32 R Projects and file structure\nProjects are widely used in software development and exist to keep all the files (data, analysis, report, etc) associated with a particular project together and related to each other. (This use of “project” in a software development sense, is distinct to a “project”, in the project management sense.) An R Project can be created in RStudio. Click “File” \\(\\rightarrow\\) “New Project”, then select “Empty project”, name the R Project and decide where to save it. For instance, a R Project focused on maternal mortality may be called “maternalmortality”. The use of R Projects enables “reliable, polite behavior across different computers or users and over time” (Bryan and Hester 2020). This is because they remove the context of that folder from its broader existence; files exist in relation to the base of the R Project, not the base of the computer."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-28",
    "href": "lectures/lecture-05-notes.html#section-28",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.33 ",
    "text": "1.33 \nOnce a project has been created, a new file with the extension “.RProj” will appear in that folder. An example of a folder with an R Project, a Quarto document, and an appropriate file structure is available here. That can be downloaded: “Code” \\(\\rightarrow\\) “Download ZIP”.\nThe main advantage of using an R Project is that we can reference files within it in a self-contained way. That means when others want to reproduce our work, they will not need to change all the file references and structure as everything is referenced in relation to the “.Rproj” file. For instance, instead of reading a CSV from, say, \"~/Documents/projects/book/data/\" you can read it from book/data/. It may be that someone else does not have a projects folder, and so the former would not work for them, while the latter would."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-29",
    "href": "lectures/lecture-05-notes.html#section-29",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.34 ",
    "text": "1.34 \nThe use of projects is required to meet the minimal level of reproducibility expected of credible work. The use of functions such as setwd(), and computer-specific file paths, bind work to a specific computer in a way that is not appropriate.\nThere are a variety of ways to set up a folder. A variant of Wilson et al. (2017) that is often useful when you are getting started is shown in the example file structure linked above."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-30",
    "href": "lectures/lecture-05-notes.html#section-30",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.35 ",
    "text": "1.35 \nexample_project/\n├── .gitignore\n├── LICENSE.md\n├── README.md\n├── example_project.Rproj\n├── inputs\n│   ├── data\n│   │   ├── unedited_data.csv\n│   │   └── ...\n│   ├── literature\n│   │   ├── alexander-tellingstorieswithdata.pdf\n│   │   ├── gelman-xboxpaper.pdf\n│   │   └── ...\n├── outputs\n│   ├── README.md\n│   ├── data\n│   │   ├── analysis_data.csv\n│   │   └── ...\n│   ├── paper\n│   │   ├── paper.pdf\n│   │   ├── paper.qmd\n│   │   ├── references.bib\n│   │   └── ...\n│   └── ...\n├── scripts\n│   ├── 00-simulate_data.R\n│   ├── 01-download_data.R\n│   ├── 02-data_cleaning.R\n│   ├── 03-test_data.R\n│   └── ...\n└── ...\nHere we have an inputs folder that contains original, unedited data that should not be written over (Wilson et al. 2017) and literature related to the project. An outputs folder contains data that we create using R, as well as the paper that we are writing. And a scripts folder is what modifies the unedited data and saves it into outputs. We will do most of our work in “scripts”, and the Quarto file for the paper in outputs. Useful other aspects include a README.md which will specify overview details about the project, and a LICENSE. An example of what to put in the README is here. Another helpful variant of this project skeleton is provided by Mineault and The Good Research Code Handbook Community (2021)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-31",
    "href": "lectures/lecture-05-notes.html#section-31",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.36 ",
    "text": "1.36"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-32",
    "href": "lectures/lecture-05-notes.html#section-32",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.1 ",
    "text": "2.1 \nIn this book we implement version control through a combination of Git and GitHub. There are a variety of reasons for this including:\n\nenhancing the reproducibility of work by making it easier to share code and data;\nmaking it easier to share work;\nimproving workflow by encouraging systematic approaches; and\nmaking it easier to work in teams."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-33",
    "href": "lectures/lecture-05-notes.html#section-33",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.2 ",
    "text": "2.2 \nGit is a version control system with a fascinating history (Brown 2018). The way one often starts doing version control is to have various copies of the one file: “first_go.R”, “first_go-fixed.R”, “first_go-fixed-with-mons-edits.R”. But this soon becomes cumbersome. One often soon turns to dates, for instance: “2022-01-01-analysis.R”, “2022-01-02-analysis.R”, “2022-01-03-analysis.R”, etc. While this keeps a record, it can be difficult to search when we need to go back, because it is hard to remember the date some change was made. In any case, it quickly gets unwieldy for a project that is being regularly worked on."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-34",
    "href": "lectures/lecture-05-notes.html#section-34",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.3 ",
    "text": "2.3 \nInstead of this, we use Git so that we can have one version of the file. Git keeps a record of the changes to that file, and a snapshot of that file at a given point in time. We determine when Git takes that snapshot. We additionally include a message saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, and the history can be more easily searched."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-35",
    "href": "lectures/lecture-05-notes.html#section-35",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.4 ",
    "text": "2.4 \nOne complication is that Git was designed for teams of software developers. As such, while it works, it can be a little ungainly for non-developers. Nonetheless Git has been usefully adapted for data science, even when the only collaborator one may have is one’s future self (Bryan 2018).\nGitHub, GitLab, and various other companies offer easier-to-use services that build on Git. While there are tradeoffs, we introduce GitHub here because it is the predominant platform (Eghbal 2020, 21). Git and GitHub are built into Posit Cloud, which provides a nice option if you have issues with local installation. One of the initial challenging aspects of Git is the terminology. Folders are called “repos”. Creating a snapshot is called a “commit”. One gets used to it eventually, but feeling confused initially is normal. Bryan (2020) is especially useful for setting up and using Git and GitHub."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-36",
    "href": "lectures/lecture-05-notes.html#section-36",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.5 ",
    "text": "2.5 \n\n2.5.1 Git\nWe first need to check whether Git is installed. Open RStudio, go to the Terminal, type the following, and then enter/return.\n\ngit --version\n\nIf you get a version number, then you are done (Figure 4 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Using Terminal to check whether Git is installed in RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Adding a username and email address to Git in RStudio\n\n\n\n\n\n\n\nFigure 4: An overview of the steps involved in setting up Git"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-37",
    "href": "lectures/lecture-05-notes.html#section-37",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.6 ",
    "text": "2.6 \nGit is pre-installed in Posit Cloud, it should be pre-installed on Mac, and it may be pre-installed on Windows. If you do not get a version number in response, then you need to install it. To do that you should follow the instructions specific to your operating system in Bryan (2020, chap. 5).\nAfter Git is installed we need to tell it a username and email. We need to do this because Git adds this information whenever we take a snapshot, or to use Git’s language, whenever we make a commit."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-38",
    "href": "lectures/lecture-05-notes.html#section-38",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.7 ",
    "text": "2.7 \nAgain, within the Terminal, type the following, replacing the details with yours, and then press “enter/return” after each line.\n\ngit config --global user.name \"Rohan Alexander\"\ngit config --global user.email \"rohan.alexander@utoronto.ca\"\ngit config --global --list\n\nWhen this set-up has been done properly, the values that you entered for “user.name” and “user.email” will be returned after the last line (Figure 4 (b)).\nThese details—username and email address—will be public. There are various ways to hide the email address if necessary, and GitHub provides instructions about this. Bryan (2020, chap. 7) provides more detailed instructions about this step, and a trouble-shooting guide."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-39",
    "href": "lectures/lecture-05-notes.html#section-39",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.8 ",
    "text": "2.8 \n\n2.8.1 GitHub\nNow that Git is set up, we need to set up GitHub. We created a GitHub account in ?@sec-fire-hose, which we use again here. After being signed in at github.com we first need to make a new folder, which is called a “repo” in Git. Look for a “+” in the top right, and then select “New Repository” (Figure 5 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Start process of creating a new repository\n\n\n\n\n\n\n\n\n\n\n\n(b) Copy the URL of the new repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding the project to Posit Cloud\n\n\n\n\n\n\n\n\n\n\n\n(d) Creating a PAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Adding files to be committed\n\n\n\n\n\n\n\n\n\n\n\n(f) Making a commit\n\n\n\n\n\n\n\nFigure 5: An overview of the steps involved in setting up GitHub"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-40",
    "href": "lectures/lecture-05-notes.html#section-40",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.9 ",
    "text": "2.9 \nAt this point we can add a sensible name for the repo. Leave it as “public” for now, because it can always be deleted later. And check the box to “Initialize this repository with a README”. Change “Add .gitignore” to R. After that, click “Create repository”.\nThis will take us to a screen that is fairly empty, but the details that we need—a URL—are in the green “Clone or Download” button, which we can copy by clicking the clipboard (Figure 5 (b))."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-41",
    "href": "lectures/lecture-05-notes.html#section-41",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.10 ",
    "text": "2.10 \nNow returning to RStudio, in Posit Cloud, we create a new project using “New Project from Git Repository”. It will ask for the URL that we just copied (Figure 5 (c)). If you are using a local computer, then this step is accomplished through the menu: “File” \\(\\rightarrow\\) “New Project…” \\(\\rightarrow\\) “Version Control” \\(\\rightarrow\\) “Git”, then paste in the URL, give the folder a meaningful name, check “Open in new session”, then click “Create Project”."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-42",
    "href": "lectures/lecture-05-notes.html#section-42",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.11 ",
    "text": "2.11 \nAt this point, a new folder has been created that we can use. We will want to be able to push it back to GitHub, and for that we will need to use a Personal Access Token (PAT) to link our RStudio Workspace with our GitHub account. We use usethis and gitcreds to enable this. These are, respectively, a package that automates repetitive tasks, and a package that authenticates with GitHub. To create a PAT, while signed into GitHub in the browser, and after installing and loading usethis run create_github_token() in your R session. GitHub will open in the browser with various options filled out (Figure 5 (d)). It can be useful to give the PAT an informative name by replacing “Note”, for instance “PAT for RStudio”, then click “Generate token”."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-43",
    "href": "lectures/lecture-05-notes.html#section-43",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.12 ",
    "text": "2.12 \nWe only have one chance to copy this token, and if we make a mistake then we will need to generate a new one. Do not include the PAT in any R script or Quarto document. Instead, after installing and loading gitcreds, run gitcreds_set(), which will then prompt you to add your PAT in the console."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-44",
    "href": "lectures/lecture-05-notes.html#section-44",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.13 ",
    "text": "2.13 \nTo use GitHub for a project that we are actively working on we follow this procedure:\n\nThe first thing to do is almost always to get any changes with “pull”. To do this, open the Git pane in RStudio, and click the blue down arrow. This gets any changes to the folder, as it is on GitHub, into our own version of the folder.\nWe can then make our changes to our copy of the folder. For instance, we could update the README, and then save it as normal.\nOnce this is done, we need to add, commit, and push. In the Git pane in RStudio, select the files to be added. This adds them to the staging area. Then click “Commit” (Figure 5 (e)). A new window will open. Add an informative message about the change that was made, and then click “Commit” in that new window (Figure 5 (f)). Finally, click “Push” to send the changes to GitHub."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-45",
    "href": "lectures/lecture-05-notes.html#section-45",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.14 ",
    "text": "2.14 \nThere are a few common pain-points when it comes to Git and GitHub. We recommend committing and pushing regularly, especially when you are new to version control. This increases the number of snapshots that you could come back to if needed. All commits should have an informative commit message. If you are new to version control, then the expectation of a good commit message is that it contains a short summary of the change, followed by a blank line, and then an explanation of the change including what the change is, and why it is being made. For instance, if your commit adds graphs to a paper, then a commit message could be:\nAdd graphs\n\nGraphs of unemployment and inflation added into Data section."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-46",
    "href": "lectures/lecture-05-notes.html#section-46",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.15 ",
    "text": "2.15 \nThere is some evidence of a relationship between overall quality and commit behavior (Sprint and Conci 2019). As you get more experience ideally the commit messages will act as a kind of journal of the project. But the main thing is to commit regularly.\nGit and GitHub were designed for software developers, rather than data scientists. GitHub limits the size of the files it will consider to 100MB, and even 50MB can prompt a warning. Data science projects regularly involve datasets that are larger than this. In ?@sec-store-and-share we discuss the use of data deposits, which can be especially useful when a project is completed, but when we are actively working on a project it can be useful to ignore large data files, at least as far as Git and GitHub are concerned. We do this using a “.gitignore” file, in which we list all of the files that we do not want to track using Git. The example folder contains a “.gitignore” file. And it can be helpful to run git_vaccinate() from usethis, which will add a variety of files to a global “.gitignore” file in case you forget to do it on a project basis. Mac users will find it useful that this will cause “.DS_Store” files to be ignored."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-47",
    "href": "lectures/lecture-05-notes.html#section-47",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.16 ",
    "text": "2.16 \nWe used the Git pane in RStudio which removed the need to use the Terminal, but it did not remove the need to go to GitHub and set up a new project. Having set up Git and GitHub, we can further improve this aspect of our workflow with usethis.\nFirst check that Git is set up with git_sitrep() from usethis. This should print information about the username and email. We can use use_git_config() to update these details if needed.\n\nuse_git_config(\n    user.name = \"Rohan Alexander\",\n    user.email = \"rohan.alexander@utoronto.ca\"\n)"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-48",
    "href": "lectures/lecture-05-notes.html#section-48",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.17 ",
    "text": "2.17 \nRather than starting a new project in GitHub, and then adding it locally, we can now use use_git() to initiate it and commit the files. Having committed, we can use use_github() to push to GitHub, which will create the folder on GitHub as well.\nIt is normal to be intimidated by Git and GitHub. Many data scientists only know a little about how to use it, and that is okay. Try to push regularly so that you have a recent snapshot in case you need it."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#sec-dealingwitherrors",
    "href": "lectures/lecture-05-notes.html#sec-dealingwitherrors",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.1 Dealing with errors",
    "text": "3.1 Dealing with errors\n\nWhen you are programming, eventually your code will break, when I say eventually, I mean like probably 10 or 20 times a day.\nGelfand (2021)"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-49",
    "href": "lectures/lecture-05-notes.html#section-49",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.2 ",
    "text": "3.2 \nEveryone who uses R, or any programming language for that matter, has trouble find them at some point. This is normal. Programming is hard. At some point code will not run or will throw an error. This happens to everyone. It is common to get frustrated, but to move forward we develop strategies to work through the issues:\n\nIf you are getting an error message, then sometimes it will be useful. Try to read it carefully to see if there is anything of use in it.\nTry to search for the error message. It can be useful to include “tidyverse” or “in R” in the search to help make the results more appropriate. Sometimes Stack Overflow results can be useful.\nLook at the help file for the function by putting “?” before the function, for instance, ?pivot_wider(). A common issue is to use a slightly incorrect argument name or format, such as accidentally including a string instead of an object name.\nLook at where the error is happening and remove or comment out code until the error is resolved, and then slowly add code back again.\nCheck the class of the object with class(), for instance, class(data_set$data_column). Ensure that it is what is expected.\nRestart R: “Session” \\(\\rightarrow\\) “Restart R and Clear Output”. Then load everything again.\nRestart your computer.\nSearch for what you are trying to do, rather than the error, being sure to include “tidyverse” or “in R” in the search to help make the results more appropriate. For instance, “save PDF of graph in R using ggplot”. Sometimes there are relevant blog posts or Stack Overflow answers that will help.\nMake a small, self-contained, reproducible example “reprex” to see if the issue can be isolated and to enable others to help.\nIf you are working in a Quarto doc then include label in the chunk options to make it easier to find where the mistake may be happening."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-50",
    "href": "lectures/lecture-05-notes.html#section-50",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.3 ",
    "text": "3.3 \nMore generally, while this is not always possible, it is almost always helpful to take a break and come back the next day."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-51",
    "href": "lectures/lecture-05-notes.html#section-51",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.4 ",
    "text": "3.4 \n\n3.4.1 Reproducible examples\nAsking for help is a skill like any other. We get better at it with practice. It is important to try not to say “this doesn’t work”, “I tried everything”, “your code does not work”, or “here is the error message, what do I do?”. In general, it is not possible to help based on these comments, because there are too many possible issues. You need to make it easy for others to help you. This involves a few steps.\n\nProvide a small, self-contained example of your data, and code, and detail what is going wrong.\nDocument what you have tried so far, including which Stack Overflow and Posit Forum posts you looked at, and why they are not what you are after.\nBe clear about the outcome that you would like."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-52",
    "href": "lectures/lecture-05-notes.html#section-52",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.5 ",
    "text": "3.5 \nBegin by creating a minimal REPRoducible EXample—a “reprex”. This is code that contains what is needed to reproduce the error, but only what is needed. This means that the code is likely a smaller, simpler version that nonetheless reproduces the error.\nSometimes this process enables one to solve the problem. If it does not, then it gives someone else a fighting chance of being able to help. There is almost no chance that you have got a problem that someone has not addressed before. It is more likely that the main difficulty is trying to communicate what you want to do and what is happening, in a way that allows others to recognize both. Developing tenacity is important.\nTo develop reproducible examples, reprex is especially useful. After installing it we:\n\nLoad the reprex package: library(reprex).\nHighlight and copy the code that is giving issues.\nRun reprex() in the console.\n\nIf the code is self-contained, then it will preview in the viewer. If it is not, then it will error, and you should rewrite the code so that it is self-contained."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-53",
    "href": "lectures/lecture-05-notes.html#section-53",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.6 ",
    "text": "3.6 \nIf you need data to reproduce the error, then you should use data that is built into R. There are a large number of datasets that are built into R and can be seen using library(help = \"datasets\"). But if possible, you should use a common option such as mtcars or faithful. Combining a reprex with a GitHub Gist that was introduced in ?@sec-fire-hose increases the chances that someone is able to help you."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-54",
    "href": "lectures/lecture-05-notes.html#section-54",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.7 ",
    "text": "3.7 \n\n3.7.1 Mentality\n\n(Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you\n(L)et’s break down the gates, there’s enough room for everyone\nSharla Gelfand, 10 March 2020.\n\nIf you write code, then you are a programmer, regardless of how you do it, what you are using it for, or who you are. But there are a few traits that one tends to notice great programmers have in common."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-55",
    "href": "lectures/lecture-05-notes.html#section-55",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.8 ",
    "text": "3.8 \n\nFocused: Often having an aim to “learn R” or similar tends to be problematic, because there is no real end point to that. It tends to be more efficient to have smaller, more specific goals, such as “make a histogram about the 2022 Australian Election with ggplot2”. This is something that can be focused on and achieved in a few hours. The issue with goals that are more nebulous, such as “I want to learn R”, is that it is easier to get lost on tangents and more difficult to get help. This can be demoralizing and lead to people quitting too early.\nCurious: It is almost always useful to “have a go”; that is, if you are not sure, then just try it. In general, the worst that happens is that you waste your time. You can rarely break something irreparably. For instance, if you want to know what happens if you pass a vector instead of a dataframe to ggplot() then try it.\nPragmatic: At the same time, it can be useful to stick within reasonable bounds, and make one small change each time. For instance, say you want to run some regressions, and are curious about the possibility of using rstanarm instead of lm(). A pragmatic way to proceed is to use one aspect from rstanarm initially and then make another change next time.\nTenacious: Again, this is a balancing act. Unexpected problems and issues arise with every project. On the one hand, persevering despite these is a good tendency. But on the other hand, sometimes one does need to be prepared to give up on something if it does not seem like a break through is possible. Mentors can be useful as they tend to be a better judge of what is reasonable.\nPlanned: It is almost always useful to excessively plan what you are going to do. For instance, you may want to make a histogram of some data. You should plan the steps that are needed and even sketch out how each step might be implemented. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is the back-up plan if the data do not exist there?\nDone is better than perfect: We all have various perfectionist tendencies, but it can be useful to initially try to turn them off to a certain extent. Initially just worry about writing code that works. You can always come back and improve aspects of it. But it is important to actually ship. Ugly code that gets the job done is better than beautiful code that is never finished."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#code-comments-and-style",
    "href": "lectures/lecture-05-notes.html#code-comments-and-style",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.9 Code comments and style",
    "text": "3.9 Code comments and style\nCode must be commented. Comments should focus on why certain code was written and to a lesser extent, why a common alternative was not selected. Indeed, it can be a good idea to write the comments before you write the code, explaining what you want to do and why, and then returning to write the code (Fowler and Beck 2018, 59).\nThere is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you are just working on your own. Most projects will evolve over time, and one purpose of code comments is to enable future-you to retrace what was done and why certain decisions were made (Bowers and Voors 2016)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-56",
    "href": "lectures/lecture-05-notes.html#section-56",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.10 ",
    "text": "3.10 \nComments in R scripts can be added by including the # symbol. (The behavior of # is different for lines inside an R chunk in a Quarto document where it acts as a comment, compared with lines outside an R chunk where it sets heading levels.) We do not have to put a comment at the start of the line, it can be midway through. In general, you do not need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if we read in some value then we may like to comment where it is coming from.\nYou should try to comment why you are doing something (Wickham 2021). What are you trying to achieve? You must comment to explain weird things. Like if you are removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you will not remember."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-57",
    "href": "lectures/lecture-05-notes.html#section-57",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.11 ",
    "text": "3.11 \nYou should break your code into sections. For instance, setting up the workspace, reading in datasets, manipulating and cleaning the datasets, analyzing the datasets, and finally producing tables and figures. Each of these should be separated with comments explaining what is going on, and sometimes into separate files, depending on the length."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-58",
    "href": "lectures/lecture-05-notes.html#section-58",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.12 ",
    "text": "3.12 \nAdditionally, at the top of each file it is important to note basic information, such as the purpose of the file, and prerequisites or dependencies, the date, the author and contact information, and finally any red flags or todos.\nYour R scripts should have a preamble and a clear demarcation of sections.\n#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Date: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep install.packages lines; comment out if need be\n# Load packages\nlibrary(tidyverse)\n\n# Read in the unedited data. \nraw_data &lt;- read_csv(\"inputs/data/unedited_data.csv\")\n\n\n#### Next section ####\n..."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-59",
    "href": "lectures/lecture-05-notes.html#section-59",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.13 ",
    "text": "3.13 \nFinally, try not to rely on a user commenting and uncommenting code, or any other manual step, such as directory specification, for code to work. This will preclude the use of automated code checking and testing.\nThis all takes time. As a rough rule of thumb, you should expect to spend at least as much time commenting and improving your code as you spent writing it. Some examples of nicely commented code include Dolatsara et al. (2021) and Burton, Cruz, and Hahn (2021)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#tests",
    "href": "lectures/lecture-05-notes.html#tests",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.14 Tests",
    "text": "3.14 Tests\nTests should be written throughout the code, and you need to write them as we go, not all at the end. This will slow you down. But it will help you to think, and to fix mistakes, which will make your code better and lead to better overall productivity. Code without tests should be viewed with suspicion. There is room for improvement when it comes to testing practices in R packages (Vidoni 2021), let alone R code more generally.\nThe need for other people, and ideally, automated processes, to run tests on code is one reason that we emphasize reproducibility. That is also why we emphasize smaller aspects such as not hardcoding file-paths, using projects, and not having spaces in file names."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-60",
    "href": "lectures/lecture-05-notes.html#section-60",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.15 ",
    "text": "3.15 \nIt is difficult to define a complete and general suite of tests, but broadly we want to test:\n\nboundary conditions,\nclasses,\nmissing data,\nthe number of observations and variables,\nduplicates, and\nregression results."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-61",
    "href": "lectures/lecture-05-notes.html#section-61",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.16 ",
    "text": "3.16 \nWe do all this initially on our simulated data and then move to the real data. The mirrors the evolution of testing during the Apollo Program. Initially testing occured based on expectations of requirements, and these tests were later updated to take into account actual launch measurements (Simpkinson 1971, 21). It is possible to write an infinite number of tests but a smaller number of high-quality tests is better than many thoughtless tests."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-62",
    "href": "lectures/lecture-05-notes.html#section-62",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.17 ",
    "text": "3.17 \nOne type of test is an “assertion”. Assertions are written throughout the code to check whether something is true and stop the code from running if not (Irving et al. 2021, 272). For instance, you might assert that a variable should be numeric. If it was tested against this assertion and found to be a character, then the test would fail and the script would stop running. Assertion tests in data science will typically be used in data cleaning and preparation scripts. We have more to say about these in ?@sec-clean-and-prepare. Unit tests check some complete aspect of code (Irving et al. 2021, 274). We will consider them more in ?@sec-its-just-a-linear-model when we consider modeling."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#efficiency",
    "href": "lectures/lecture-05-notes.html#efficiency",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.18 Efficiency",
    "text": "3.18 Efficiency\nGenerally in this book we are, and will continue to be, concerned with just getting something done. Not necessarily getting it done in the best or most efficient way, because to a large extent, being worried about that is a waste of time. For the most part one is better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But that eventually becomes unfeasible. At a certain point, and this differs depending on context, efficiency becomes important. Eventually ugly or slow code, and dogmatic insistence on a particular way of doing things, have an effect. And it is at that point that one needs to be open to new approaches to ensure efficiency. There is rarely a most common area for obvious performance gains. Instead, it is important to develop the ability to measure, evaluate, and think."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-63",
    "href": "lectures/lecture-05-notes.html#section-63",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.19 ",
    "text": "3.19 \nOne of the best ways to improve the efficiency of our code is preparing it in such a way that we can bring in a second pair of eyes. To make the most of their time, it is important that our code easy to read. So we start with “code linting” and “styling”. This does not speed up our code, per se, but instead makes it more efficient when another person comes to it, or we revisit it. This enables formal code review and refactoring, which is where we rewrite code to make it better, while not changing what it does (it does the same thing, but in a different way). We then turn to measurement of run time, and introduce parallel processing, where we allow our computer to run code for multiple processes at the same time"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#sharing-a-code-environment",
    "href": "lectures/lecture-05-notes.html#sharing-a-code-environment",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.20 Sharing a code environment",
    "text": "3.20 Sharing a code environment\nWe have discussed at length the need to share code, and we have put forward an approach to this using GitHub. And in ?@sec-store-and-share, we will discuss sharing data. But, there is another requirement to enable other people to run our code. In ?@sec-fire-hose we discussed how R itself, as well as R packages update from time to time, as new functionality is developed, errors fixed, and other general improvements made. ?@sec-r-essentials describes how one advantage of the tidyverse is that it can update faster than base R, because it is more specific. But this could mean that even if we were to share all the code and data that we use, it is possible that the software versions that have become available would cause errors."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-64",
    "href": "lectures/lecture-05-notes.html#section-64",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.21 ",
    "text": "3.21 \nThe solution to this is to detail the environment that was used. There are a large number of ways to do this, and they can add complexity. We just focus on documenting the version of R and R packages that were used, and making it easier for others to install that exact version. Essentially we are just isolating the set-up that we used because that will help with reproducibility (Perkel 2023). In R we can use renv to do this.\nOnce renv is installed and loaded, we use init() to get the infrastructure set-up that we will need. We are going to create a file that will record the packages and versions used. We then use snapshot() to actually document what we are using. This creates a “lockfile” that records the information."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-65",
    "href": "lectures/lecture-05-notes.html#section-65",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.22 ",
    "text": "3.22 \nIf we want to see which packages we are using in the R Project, then we can use dependencies(). Doing this for the example folder indicates that the following packages are used: rmarkdown, bookdown, knitr, rmarkdown, bookdown, knitr, palmerpenguins, tidyverse, renv, haven, readr, and tidyverse."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-66",
    "href": "lectures/lecture-05-notes.html#section-66",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.23 ",
    "text": "3.23 \nWe could open the lockfile file—“renv.lock”—to see the exact versions if we wanted. The lockfile also documents all the other packages that were installed and where they were downloaded from. Someone coming to this project from outside could then use restore() which would install the exact version of the packages that we used."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#code-linting-and-styling",
    "href": "lectures/lecture-05-notes.html#code-linting-and-styling",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.24 Code linting and styling",
    "text": "3.24 Code linting and styling\nBeing fast is valuable but it is mostly about being able to iterate fast, not necessarily having code that runs fast. Backus (1981, 26) describes how even in 1954 a programmer cost at least as much as a computer, and these days additional computational power is usually much cheaper than a programmer. Performant code is important, but it is also important to use other people’s time efficiently. Code is rarely only written once. Instead we typically have to come back to it, even if to just fix mistakes, and this means that code must be able to be read by humans (Matsumoto 2007, 478). If this is not done then there will be an efficiency cost.\nLinting and styling is the process of checking code, mostly for stylistic issues, and re-arranging code to make it easier to read. (There is another aspect of linting, which is dealing with programming errors, such as forgetting a closing bracket, but here we focus on stylistic issues.) Often the best efficiency gain comes from making it easier for others to read our code, even if this is just ourselves returning to the code after a break. Jane Street, a US proprietary trading firm, places a very strong focus on ensuring their code is readable, as a core part of risk mitigation (Minsky 2011). While we may not all have billions of dollars under the potentially mercurial management of code, we all would likely prefer that our code does not produce errors."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-67",
    "href": "lectures/lecture-05-notes.html#section-67",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.25 ",
    "text": "3.25 \nWe use lint() from lintr to lint our code. For instance, consider the following R code (saved as “linting_example.R”).\n\nSIMULATED_DATA &lt;-\n    tibble(\n        division = c(1:150, 151),\n        party = sample(\n            x = c(\"Liberal\"),\n            size = 151,\n            replace = T\n        )\n    )"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-68",
    "href": "lectures/lecture-05-notes.html#section-68",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.26 ",
    "text": "3.26 \n\nlint(filename = \"linting_example.R\")"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-69",
    "href": "lectures/lecture-05-notes.html#section-69",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.27 ",
    "text": "3.27 \nThe result is that the file “linting_example.R” is opened and the issues that lint() found are printed in “Markers” (Figure 6). It is then up to you to deal with the issues.\n\n\n\n\n\n\nFigure 6: Linting results from example R code"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-70",
    "href": "lectures/lecture-05-notes.html#section-70",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.28 ",
    "text": "3.28 \nMaking the recommended changes results in code that is more readable, and consistent with best practice, as defined by Wickham (2021).\n\nsimulated_data &lt;-\n    tibble(\n        division = c(1:150, 151),\n        party = sample(\n            x = c(\"Liberal\"),\n            size = 151,\n            replace = TRUE\n        )\n    )"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-71",
    "href": "lectures/lecture-05-notes.html#section-71",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.29 ",
    "text": "3.29 \nAt first it may seem that some aspects that the linter is identifying, like trailing whitespace and only using double quotes are small and inconsequential. But they distract from being able to fix bigger issues. Further, if we are not able to get small things right, then how could anyone trust that we could get the big things right? Therefore, it is important to have dealt with all the small aspects that a linter identifies."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-72",
    "href": "lectures/lecture-05-notes.html#section-72",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.30 ",
    "text": "3.30 \nIn addition to lintr we also use styler. This will automatically adjust style issues, in contrast to the linter, which gave a list of issues to look at. To run this we use style_file().\n\nstyle_file(path = \"linting_example.R\")\n\nThis will automatically make changes, such as spacing and indentation. As such this should be done regularly, rather than only once at the end of a project, so as to be able to review the changes and make sure no errors have been introduced."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#code-review",
    "href": "lectures/lecture-05-notes.html#code-review",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.31 Code review",
    "text": "3.31 Code review\nHaving dealt with all of these aspects of style, we can turn to code review. This is the process of having another person go through and critique the code. Many professional writers have editors, and code review is the closest that we come to that in data science. Code review is a critical part of writing code, and Irving et al. (2021, 465) describe it as “the most effective way to find bugs”. It is especially helpful, although quite daunting, when learning to code because getting feedback is a great way to improve."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-73",
    "href": "lectures/lecture-05-notes.html#section-73",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.32 ",
    "text": "3.32 \nGo out of your way to be polite and collegial when reviewing another person’s code. Small aspects to do with style, things like spacing and separation, should have been taken care of by a linter and styler, but if not, then make a general recommendation about that. Most of your time as a code reviewer in data science should be spent on aspects such as:\n\nIs there an informative README and how could it be improved?\nAre the file names and variable names consistent, informative, and meaningful?\nDo the comments allow you to understand why something is being done?\nAre the tests both appropriate and sufficient? Are there edge cases or corner solutions that are not considered? Similarly, are there unnecessary tests that could be removed?\nAre there magic numbers that could be changed to variables and explained?\nIs there duplicated code that could be changed?\nAre there any outstanding warnings that should be addressed?\nAre there any especially large functions or pipes that could be separated into smaller ones?\nIs the structure of the project appropriate?\nCan we change any of the code to data (Irving et al. 2021, 462)?"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-74",
    "href": "lectures/lecture-05-notes.html#section-74",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.33 ",
    "text": "3.33 \nFor instance, consider some code that looked for the names of prime ministers and presidents. When we first wrote this code we likely added the relevant names directly into the code. But as part of code review, we might instead recommend that this be changed. We might recommend creating a small dataset of relevant names, and then re-writing the code to have it look up that dataset.\nCode review ensures that the code can be understood by at least one other person. This is a critical part of building knowledge about the world. At Google, code review is not primarily about finding defects, although that may happen, but is instead about ensuring readability and maintainability as well as education (Sadowski et al. 2018). This is also the case at Jane Street where they use code review to catch bugs, share institutional knowledge, assist with training, and oblige staff to write code that can be read (Minsky 2015)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-75",
    "href": "lectures/lecture-05-notes.html#section-75",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.34 ",
    "text": "3.34 \nFinally, code review does not have to, and should not, be an onerous days-consuming process of reading all the code. The best code review is a quick review of just one file, focused on suggesting changes to just a handful of lines. Indeed, it may be better to have a review done by a small team of people rather than one individual. Do not review too much code at any one time. At most a few hundred lines, which should take around an hour, because any more than that has been found to be associated with reduced efficacy (Cohen, Teleki, and Brown 2006, 79)."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#code-refactoring",
    "href": "lectures/lecture-05-notes.html#code-refactoring",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.35 Code refactoring",
    "text": "3.35 Code refactoring\nTo refactor code means to rewrite it so that the new code achieves the same outcome as the old code, but the new code does it better. For instance, Chawla (2020) discuss how the code underpinning an important UK Covid model was initially written by epidemiologists, and months later clarified and cleaned up by a team from the Royal Society, Microsoft, and GitHub. This was valuable because it provided more confidence in the model, even though both versions produced the same outputs, given the same inputs."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-76",
    "href": "lectures/lecture-05-notes.html#section-76",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.36 ",
    "text": "3.36 \nWe typically refer to code refactoring in relation to code that someone else wrote. (Although it may be that we actually wrote the code, and it was just that it was some time ago.) When we start to refactor code, we want to make sure that the rewritten code achieves the same outcomes as the original code. This means that we need a suite of appropriate tests written that we can depend on. If these do not exist, then we may need to create them.\nWe rewrite code to make it easier for others to understand, which in turn allows more confidence in our conclusions. But before we can do that, we need to understand what the existing code is doing. One way to get started is to go through the code and add extensive comments. These comments are different to normal comments. They are our active process of trying to understand what is each code chunk trying to do and how could this be improved."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-77",
    "href": "lectures/lecture-05-notes.html#section-77",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.37 ",
    "text": "3.37 \nRefactoring code is an opportunity to ensure that it satisfies best practice. Trisovic et al. (2022) details some core recommendations based on examining 9,000 R scripts including:\n\nRemove setwd() and any absolute paths, and ensure that only relative paths, in relation to the “.Rproj” file, are used.\nEnsure there is a clear order of execution. We have recommended using numbers in filenames to achieve this initially, but eventually more sophisticated approaches, such as targets (Landau 2021), could be used instead.\nEnsure that code can run on a different computer."
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-78",
    "href": "lectures/lecture-05-notes.html#section-78",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.38 ",
    "text": "3.38 \nFor instance, consider the following code:\n\nsetwd(\"/Users/rohanalexander/Documents/telling_stories\")\n\nlibrary(tidyverse)\n\nd &lt;- read_csv(\"cars.csv\")\n\nmtcars &lt;-\n    mtcars |&gt;\n    mutate(K_P_L = mpg / 2.352)\n\nlibrary(datasauRus)\n\ndatasaurus_dozen"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#section-79",
    "href": "lectures/lecture-05-notes.html#section-79",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.39 ",
    "text": "3.39 \nWe could change that, starting by creating an R Project which enables us to remove setwd(), grouping all the library() calls at the top, using “&lt;-” instead of “=”, and being consistent with variable names:\n\nlibrary(tidyverse)\nlibrary(datasauRus)\n\ncars_data &lt;- read_csv(\"cars.csv\")\n\nmpg_to_kpl_conversion_factor &lt;- 2.352\n\nmtcars &lt;-\n    mtcars |&gt;\n    mutate(kpl = mpg / mpg_to_kpl_conversion_factor)"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#concluding-remarks",
    "href": "lectures/lecture-05-notes.html#concluding-remarks",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.40 Concluding remarks",
    "text": "3.40 Concluding remarks\nIn this chapter we have considered much and it is normal to be overwhelmed. Come back to the Quarto section as needed. Many people are confused by Git and GitHub and just know enough to get by. And while there was a lot of material in efficiency, the most important aspect of performant code is making it easier for another person to read it, even if that person is just yourself returning after a break."
  },
  {
    "objectID": "lectures/lecture-24-notes.html",
    "href": "lectures/lecture-24-notes.html",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#reading-assignment",
    "href": "lectures/lecture-24-notes.html#reading-assignment",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#lecture-slides",
    "href": "lectures/lecture-24-notes.html#lecture-slides",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#coming-soon",
    "href": "lectures/lecture-24-notes.html#coming-soon",
    "title": "Generalized Linear Models (GLMs)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#references",
    "href": "lectures/lecture-24-notes.html#references",
    "title": "Generalized Linear Models (GLMs)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-22-notes.html",
    "href": "lectures/lecture-22-notes.html",
    "title": "Linear Models",
    "section": "",
    "text": "Required:  RA Ch 12\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#reading-assignment",
    "href": "lectures/lecture-22-notes.html#reading-assignment",
    "title": "Linear Models",
    "section": "",
    "text": "Required:  RA Ch 12\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#lecture-slides",
    "href": "lectures/lecture-22-notes.html#lecture-slides",
    "title": "Linear Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#coming-soon",
    "href": "lectures/lecture-22-notes.html#coming-soon",
    "title": "Linear Models",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#references",
    "href": "lectures/lecture-22-notes.html#references",
    "title": "Linear Models",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-20-notes.html",
    "href": "lectures/lecture-20-notes.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:  RA Ch 11\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#reading-assignment",
    "href": "lectures/lecture-20-notes.html#reading-assignment",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:  RA Ch 11\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#lecture-slides",
    "href": "lectures/lecture-20-notes.html#lecture-slides",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#coming-soon",
    "href": "lectures/lecture-20-notes.html#coming-soon",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#references",
    "href": "lectures/lecture-20-notes.html#references",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-18-notes.html",
    "href": "lectures/lecture-18-notes.html",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:  RA Ch 9\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#reading-assignment",
    "href": "lectures/lecture-18-notes.html#reading-assignment",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:  RA Ch 9\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#lecture-slides",
    "href": "lectures/lecture-18-notes.html#lecture-slides",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#coming-soon",
    "href": "lectures/lecture-18-notes.html#coming-soon",
    "title": "Cleaning, Preparing, and Testing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#references",
    "href": "lectures/lecture-18-notes.html#references",
    "title": "Cleaning, Preparing, and Testing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-16-notes.html",
    "href": "lectures/lecture-16-notes.html",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#reading-assignment",
    "href": "lectures/lecture-16-notes.html#reading-assignment",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#lecture-slides",
    "href": "lectures/lecture-16-notes.html#lecture-slides",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#coming-soon",
    "href": "lectures/lecture-16-notes.html#coming-soon",
    "title": "Experiments and Surveys",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#references",
    "href": "lectures/lecture-16-notes.html#references",
    "title": "Experiments and Surveys",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-14-notes.html",
    "href": "lectures/lecture-14-notes.html",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#reading-assignment",
    "href": "lectures/lecture-14-notes.html#reading-assignment",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#lecture-slides",
    "href": "lectures/lecture-14-notes.html#lecture-slides",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#coming-soon",
    "href": "lectures/lecture-14-notes.html#coming-soon",
    "title": "APIs, Scraping, and Parsing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#references",
    "href": "lectures/lecture-14-notes.html#references",
    "title": "APIs, Scraping, and Parsing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-12-notes.html",
    "href": "lectures/lecture-12-notes.html",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#reading-assignment",
    "href": "lectures/lecture-12-notes.html#reading-assignment",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#lecture-slides",
    "href": "lectures/lecture-12-notes.html#lecture-slides",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#coming-soon",
    "href": "lectures/lecture-12-notes.html#coming-soon",
    "title": "Measurement, Censuses, and Sampling",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#references",
    "href": "lectures/lecture-12-notes.html#references",
    "title": "Measurement, Censuses, and Sampling",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-10-notes.html",
    "href": "lectures/lecture-10-notes.html",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required: \nRecommended: ) + \n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-10-notes.html#reading-assignment",
    "href": "lectures/lecture-10-notes.html#reading-assignment",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required: \nRecommended: ) +"
  },
  {
    "objectID": "lectures/lecture-10-notes.html#lecture-slides",
    "href": "lectures/lecture-10-notes.html#lecture-slides",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-08-notes.html",
    "href": "lectures/lecture-08-notes.html",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-08-notes.html#reading-assignment",
    "href": "lectures/lecture-08-notes.html#reading-assignment",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-08-notes.html#lecture-slides",
    "href": "lectures/lecture-08-notes.html#lecture-slides",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-06-notes.html",
    "href": "lectures/lecture-06-notes.html",
    "title": "Reproducible Research with R and RStudio",
    "section": "",
    "text": "Required:  RA CH 3\nRecommended:  KH Ch 2\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-06-notes.html#reading-assignment",
    "href": "lectures/lecture-06-notes.html#reading-assignment",
    "title": "Reproducible Research with R and RStudio",
    "section": "",
    "text": "Required:  RA CH 3\nRecommended:  KH Ch 2"
  },
  {
    "objectID": "lectures/lecture-06-notes.html#lecture-slides",
    "href": "lectures/lecture-06-notes.html#lecture-slides",
    "title": "Reproducible Research with R and RStudio",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-02-notes.html",
    "href": "lectures/lecture-02-notes.html",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "",
    "text": "Required:  RA Ch 1\nRecommended: \n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#reading-assignment",
    "href": "lectures/lecture-02-notes.html#reading-assignment",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "",
    "text": "Required:  RA Ch 1\nRecommended:"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#lecture-slides",
    "href": "lectures/lecture-02-notes.html#lecture-slides",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan",
    "href": "lectures/lecture-02-notes.html#plan",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "1.1 Plan",
    "text": "1.1 Plan\n\nTODO: Introduction to R\nEither KH and RA would be ideal, Tidyverse first"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#key-concepts-and-skills",
    "href": "lectures/lecture-05-slides.html#key-concepts-and-skills",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.1 Key concepts and skills",
    "text": "1.1 Key concepts and skills\nKey concepts and skills\n\nReproducibility typically begins as something that someone imposes on you. It can be onerous and annoying. This typically lasts until you need to revisit a project after a small break. At that point you typically realize that reproducibility is not just a requirement for data science because it is the only way that we can make genuine progress, but because it helps us help ourselves.\nReproducibility implies sharing data, code, and environment. This is enhanced by using Quarto, R Projects, and Git and GitHub: Quarto builds documents that integrate normal text and R code; R Projects enable a file structure that is not dependent on a user’s personal directory set-up; and Git and GitHub make it easier to share code and data.\nThis is not an unimpeachable workflow, but one that is good enough and provides many of the benefits. We will improve various aspects of it through various tools, but improving code structure and comments goes a long way.\nThere are always errors that occur, and it is important to recognize that debugging is a skill that improves with practice. But one key aspect of being able to get help is to be able to make a reproducible example others can use."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#software-and-packages",
    "href": "lectures/lecture-05-slides.html#software-and-packages",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.2 Software and packages",
    "text": "1.2 Software and packages\n\n\n\nBase R (R Core Team 2023)\nAER (Kleiber and Zeileis 2008)\nfuture (Bengtsson 2021)\ngitcreds (Csárdi 2022)\nknitr (Xie 2023)\nlintr (Hester et al. 2022)\nrenv (Ushey 2022)\nreprex (Bryan et al. 2022)\nstyler (Müller and Walthert 2022)\ntidyverse (Wickham et al. 2019)\ntinytex (Xie 2019)\nusethis (Wickham, Bryan, and Barrett 2022)\n\n\n\nlibrary(AER)\nlibrary(future)\nlibrary(gitcreds)\nlibrary(knitr)\nlibrary(lintr)\nlibrary(renv)\nlibrary(reprex)\nlibrary(styler)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(usethis)"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section",
    "href": "lectures/lecture-05-slides.html#section",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.3 ",
    "text": "1.3 \n\nThe number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\\(\\dots\\) So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty.\nFrançois Chollet, 20 February 2020.\n\nIf science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is a critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, there is a need for reproducible data science workflows.\nM. Alexander (2019) defines reproducible research as that which can be exactly redone, given all the materials used. This underscores the importance of providing the code, data, and environment. The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables. Ironically, there are different definitions of reproducibility between disciplines. Barba (2018) surveys a variety of disciplines and concludes that the predominant language usage implies the following definitions:\n\nReproducible research is when “[a]uthors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.”\nA replication is a study “that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.”\n\nRegardless of what it is specifically called, Gelman (2016) identifies how large an issue the lack of it is in various social sciences. Work that is not reproducible does not contribute to our stock of knowledge about the world. This is wasteful and potentially even unethical. Since Gelman (2016), a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. That is also the case in the life sciences (Heil et al. 2021), cancer research (Begley and Ellis 2012; Mullard 2021), and computer science (Pineau et al. 2021).\nSome of the examples that Gelman (2016) talks about are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created “nudge” units that implement public policy (Sunstein and Reisch 2017) even though there is evidence that some of the claims lack credibility (Maier et al. 2022; Szaszi et al. 2022). Governments are increasingly using algorithms that they do not make open (Chouldechova et al. 2018). And Herndon, Ash, and Pollin (2014) document how research in economics that was used by governments to justify austerity policies following the 2007–2008 financial crisis turned out to not be reproducible.\nAt a minimum, and with few exceptions, we must release our code, datasets, and environment. Without these, it is difficult to know what a finding speaks to (Miyakawa 2020). More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked (Merali 2010; Hillel 2017; Silver 2020). Increasingly, following Buckheit and Donoho (1995), we consider a paper to be an advertisement, and for the associated code, data, and environment to be the actual work. Steve Jobs, a co-founder of Apple, talked about how people who are the best at their craft ensure that even the aspects of their work that no one else will ever see are as well finished and high quality as the aspects that are public facing (Isaacson 2011). The same is true in data science, where often one of the distinguishing aspects of high-quality work is that the README and code comments are as polished as, say, the abstract of the associated paper."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-1",
    "href": "lectures/lecture-05-slides.html#section-1",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.4 ",
    "text": "1.4 \nWorkflows exist within a cultural and social context, which imposes an additional ethical reason for the need for them to be reproducible. For instance, Wang and Kosinski (2018) train a neural network to distinguish between the faces of gay and heterosexual men. (Murphy (2017) provides a summary of the paper, the associated issues, and comments from its authors.) To do this, Wang and Kosinski (2018, 248) needed a dataset of photos of people that were “adult, Caucasian, fully visible, and of a gender that matched the one reported on the user’s profile”. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. The instructions provided to the Mechanical Turk workers for this task specify that Barack Obama, the 44th US President, who had a white mother and a black father, should be classified as “Black”; and that Latino is an ethnicity, rather than a race (Mattson 2017). The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.\nThis is just one specific concern about one part of the Wang and Kosinski (2018) workflow. Broader concerns are raised by others including Gelman, Mattson, and Simpson (2018). The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of Wang and Kosinski (2018) is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-2",
    "href": "lectures/lecture-05-slides.html#section-2",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.5 ",
    "text": "1.5 \nSome of the steps that we can take to make our work more reproducible include:\n\nEnsure the entire workflow is documented. This may involve addressing questions such as:\n\nHow was the original, unedited dataset obtained and is access likely to be persistent and available to others?\nWhat specific steps are being taken to transform the original, unedited data into the data that were analyzed, and how can this be made available to others?\nWhat analysis has been done, and how clearly can this be shared?\nHow has the final paper or report been built and to what extent can others follow that process themselves?\n\nNot worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to do the last, until you can do the first:\n\nCan you run your entire workflow again?\nCan another person run your entire workflow again?\nCan “future-you” run your entire workflow again?\nCan “future-another-person” run your entire workflow again?\n\nIncluding a detailed discussion about the limitations of the dataset and the approach in the final paper or report."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-3",
    "href": "lectures/lecture-05-slides.html#section-3",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.6 ",
    "text": "1.6 \nThe workflow that we advocate in this book is:\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s (2023) Telling Stories with Data workflow \n\n\n\n\nBut it can be alternatively considered as: “Think an awful lot, mostly read and write, sometimes code”."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-4",
    "href": "lectures/lecture-05-slides.html#section-4",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.7 ",
    "text": "1.7 \nThere are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes\n\nQuarto,\nR Projects\nGit and GitHub"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#quarto",
    "href": "lectures/lecture-05-slides.html#quarto",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.8 Quarto",
    "text": "1.8 Quarto\n1.8.1 Getting started\nQuarto integrates code and natural language in a way that is called “literate programming” (Knuth 1984). It is the successor to R Markdown, which was a variant of Markdown specifically designed to allow R code chunks to be included. Quarto uses a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a “What You See Is What You Get” (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level headings will look the same. But it means that we must designate or “mark up” how we would like certain aspects to appear. And it is only when we render the document that we get to see what it looks like. A visual editor option can also be used, and this hides the need for the user to do this mark-up themselves."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-5",
    "href": "lectures/lecture-05-slides.html#section-5",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.9 ",
    "text": "1.9 \nOne advantage of literate programming is that we get a “live” document in which code executes and then forms part of the document. Another advantage of Quarto is that similar code can compile into a variety of documents, including HTML and PDFs. Quarto also has default options for including a title, author, and date. One disadvantage is that it can take a while for a document to compile because the code needs to run.\nWe need to download Quarto from here. (Skip this step if you are using Posit Cloud because it is already installed.) We can then create a new Quarto document within RStudio: “File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “Quarto Document\\(\\dots\\)”.\nAfter opening a new Quarto document and selecting “Source” view, you will see the default top matter, contained within a pair of three dashes, as well as some examples of text showing a few of the markdown essential commands and R chunks, each of which are discussed further in the following sections."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-6",
    "href": "lectures/lecture-05-slides.html#section-6",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.10 ",
    "text": "1.10 \n1.10.1 Top matter\nTop matter consists of defining aspects such as the title, author, and date. It is contained within three dashes at the top of a Quarto document. For instance, the following would specify a title, a date that automatically updated to the date the document was rendered, and an author.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: html\n---"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-7",
    "href": "lectures/lecture-05-slides.html#section-7",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.11 ",
    "text": "1.11 \nAn abstract is a short summary of the paper, and we could add that to the top matter.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: html\n---"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-8",
    "href": "lectures/lecture-05-slides.html#section-8",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.12 ",
    "text": "1.12 \nBy default, Quarto will create an HTML document, but we can change the output format to produce a PDF. This uses LaTeX in the background and requires the installation of supporting packages. To do this install tinytex. But as it is used in the background we should not need to load it.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: pdf\n---"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-9",
    "href": "lectures/lecture-05-slides.html#section-9",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.13 ",
    "text": "1.13 \nWe can include references by specifying a BibTeX file in the top matter and then calling it within the text, as needed.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: pdf\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-10",
    "href": "lectures/lecture-05-slides.html#section-10",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.14 ",
    "text": "1.14 \nWe would need to make a separate file called “bibliography.bib” and save it next to the Quarto file. In the BibTeX file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with citation() and this can be added to the “bibliography.bib” file. The citation for a package can be found by including the package name, for instance citation(\"tidyverse\"), and again adding the output to the “.bib” file. It can be helpful to use Google Scholar or doi2bib to get citations for books or articles."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-11",
    "href": "lectures/lecture-05-slides.html#section-11",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.15 ",
    "text": "1.15 \nWe need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance “citeR”.\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@book{tellingstories,\n    title = {Telling Stories with Data},\n    author = {Rohan Alexander},\n    year = {2023},\n    publisher = {Chapman and Hall/CRC},\n    url = {https://tellingstorieswithdata.com}\n  }"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-12",
    "href": "lectures/lecture-05-slides.html#section-12",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.16 ",
    "text": "1.16 \nTo cite R in the Quarto document we then include @citeR, which would put brackets around the year: R Core Team (2023), or [@citeR], which would put brackets around the whole thing: (R Core Team 2023)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-13",
    "href": "lectures/lecture-05-slides.html#section-13",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.17 ",
    "text": "1.17 \nThe reference list at the end of the paper is automatically built based on calling the BibTeX file and including references in the paper. At the end of the Quarto document, include a heading “# References” and the actual citations will be included after that. When the Quarto file is rendered, Quarto sees these in the content, goes to the BibTeX file to get the reference details that it needs, builds the reference list, and then adds it at the end of the rendered document."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-14",
    "href": "lectures/lecture-05-slides.html#section-14",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.18 ",
    "text": "1.18 \n1.18.1 Essential commands\nQuarto uses a variation of Markdown as its underlying syntax. Essential Markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in RStudio: “Help” \\(\\rightarrow\\) “Markdown Quick Reference”. It is your choice as to whether you want to use the visual or source editor. But either way, it is good to understand these essentials because it will not always be possible to use a visual editor (for instance if you are quickly looking at a Quarto document in GitHub). As you get more experience it can be useful to use a text editor such as Sublime Text, or an alternative Integrated Development Environment such as VS Code.\n\nEmphasis: *italic*, **bold**\nHeaders (these go on their own line with a blank line before and after):\n\n         # First level header\n         \n         ## Second level header\n         \n         ### Third level header\n\nUnordered list, with sub-lists:\n\n    * Item 1\n    * Item 2\n        + Item 2a\n        + Item 2b\n\nOrdered list, with sub-lists:\n\n    1. Item 1\n    2. Item 2\n    3. Item 3\n        + Item 3a\n        + Item 3b\n\nURLs can be added: [this book](https://www.tellingstorieswithdata.com) results in this book.\nA paragraph is created by leaving a blank line.\n\nA paragraph about an idea, nicely spaced from the following paragraph.\n\nA paragraph about another idea, again spaced from the earlier paragraph."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-15",
    "href": "lectures/lecture-05-slides.html#section-15",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.19 ",
    "text": "1.19 \nOnce we have added some aspects, then we may want to see the actual document. To build the document click “Render”."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-16",
    "href": "lectures/lecture-05-slides.html#section-16",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.20 ",
    "text": "1.20 \n1.20.1 R chunks\nWe can include code for R and many other languages in code chunks within a Quarto document. When we render the document the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell Quarto that this is an R chunk. Anything inside this chunk will be considered R code and run as such. We use data from Kleiber and Zeileis (2008) who provide the R package AER to accompany their book Applied Econometrics with R. We could load the tidyverse and install and load AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-17",
    "href": "lectures/lecture-05-slides.html#section-17",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.21 ",
    "text": "1.21 \nThe output of that code is Figure 1.\n\n\nFigure 1: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\nThere are various evaluation options that are available in chunks. We include these, each on a new line, by opening the line with the chunk-specific comment delimiter “#|” and then the option. Helpful options include:\n\necho: This controls whether the code itself is included in the document. For instance, #| echo: false would mean the code will be run and its output will show, but the code itself would not be included in the document.\ninclude: This controls whether the output of the code is included in the document. For instance, #| include: false would run the code, but would not result in any output, and the code itself would not be included in the document.\neval: This controls whether the code should be included in the document. For instance, #| eval: false would mean that the code is not run, and hence there would not be any output to include, but the code itself would be included in the document.\nwarning: This controls whether warnings should be included in the document. For instance, #| warning: false would mean that warnings are not included.\nmessage: This controls whether messages should be included in the document. For instance, #| message: false would mean that messages are not included in the document.\n\nFor instance, we could include the output, but not the code, and suppress any warnings.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nLeave a blank line on either side of an R chunk, otherwise it may not run properly. And use lower case for logical values, i.e. “false” not “FALSE”.\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then...\nThe Quarto document itself must load any datasets that are needed. It is not enough that they are in the environment. This is because the Quarto document evaluates the code in the document when it is rendered, not necessarily the environment.\n1.21.1 Equations\nWe can include equations by using LaTeX, which is based on the programming language TeX. We invoke math mode in LaTeX by using two dollar signs as opening and closing tags. Then whatever is inside is evaluated as LaTeX mark-up. For instance we can produce the compound interest formula with:\n$$\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n$$\n\\[\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-18",
    "href": "lectures/lecture-05-slides.html#section-18",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.22 ",
    "text": "1.22 \nLaTeX is a comprehensive mark-up language but we will mostly just use it to specify the model of interest. We include some examples here that contain the critical aspects we will draw on starting in ?@sec-its-just-a-linear-model.\n$$\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n$$\n\\[\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-19",
    "href": "lectures/lecture-05-slides.html#section-19",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.23 ",
    "text": "1.23 \nUnderscores are used to get subscripts: y_i for \\(y_i\\). And we can get a subscript of more than one item by surrounding it with curly braces: y_{i,c} for \\(y_{i,c}\\). In this case we wanted math mode within the line, and so we surround these with only one dollar sign as opening and closing tags.\nGreek letters are typically preceded by a backslash. Common Greek letters include: \\alpha for \\(\\alpha\\), \\beta for \\(\\beta\\), \\delta for \\(\\delta\\), \\epsilon for \\(\\epsilon\\), \\gamma for \\(\\gamma\\), \\lambda for \\(\\lambda\\), \\mu for \\(\\mu\\), \\phi for \\(\\phi\\), \\pi for \\(\\pi\\), \\Pi for \\(\\Pi\\), \\rho for \\(\\rho\\), \\sigma for \\(\\sigma\\), \\Sigma for \\(\\Sigma\\), \\tau for \\(\\tau\\), and \\theta for \\(\\theta\\)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-20",
    "href": "lectures/lecture-05-slides.html#section-20",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.24 ",
    "text": "1.24 \nLaTeX math mode assumes letters are variables and so makes them italic, but sometimes we want a word to appear in normal font because it is not a variable, such as “Normal”. In that case we surround it with \\mbox{}, for instance \\mbox{Normal} for \\(\\mbox{Normal}\\)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-21",
    "href": "lectures/lecture-05-slides.html#section-21",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.25 ",
    "text": "1.25 \nWe line up equations across multiple lines using \\begin{aligned} and \\end{aligned}. Then the item that is to be lined up is noted by an ampersand. The following is a model that we will estimate in ?@sec-multilevel-regression-with-post-stratification.\n$$\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-22",
    "href": "lectures/lecture-05-slides.html#section-22",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.26 ",
    "text": "1.26 \nFinally, certain functions are built into LaTeX. For instance, we can appropriately typeset “log” with \\log."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-23",
    "href": "lectures/lecture-05-slides.html#section-23",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.27 ",
    "text": "1.27 \n1.27.1 Cross-references\nIt can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, consider the following code."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-24",
    "href": "lectures/lecture-05-slides.html#section-24",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.28 ",
    "text": "1.28 \n\n```{r}\n#| label: fig-uniquename\n#| fig-cap: Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\n#| warning: false\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\")\n```\n\n\n\nFigure 2: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\nThen (@fig-uniquename) would produce: (Figure 2) as the name of the R chunk is fig-uniquename. We need to add “fig” to the start of the chunk name so that Quarto knows that this is a figure. We then include a “fig-cap:” in the R chunk that specifies a caption."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-25",
    "href": "lectures/lecture-05-slides.html#section-25",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.29 ",
    "text": "1.29 \nWe can add #| layout-ncol: 2 in an R chunk within a Quarto document to have two graphs appear side by side (Figure 3). Here Figure 3 (a) uses the minimal theme, and Figure 3 (b) uses the classic theme. These both cross-reference the same label #| label: fig-doctorgraphsidebyside in the R chunk, with an additional option added in the R chunk of #| fig-subcap: [\"Number of illnesses\",\"Number of visits to the doctor\"] which provides the sub-captions. The addition of a letter in-text is accomplished by adding “-1” and “-2” to the end of the label when it is used in-text: (@fig-doctorgraphsidebyside), @fig-doctorgraphsidebyside-1, and @fig-doctorgraphsidebyside-2 for (Figure 3), Figure 3 (a), and Figure 3 (b), respectively.\n```{r}\n#| eval: true\n#| warning: false\n#| label: fig-doctorgraphsidebyside\n#| fig-cap: \"Two variants of graphs\"\n#| fig-subcap: [\"Illnesses\",\"Visits to the doctor\"]\n#| layout-ncol: 2\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\") +\n    theme_minimal()\n\nDoctorVisits |&gt;\n    ggplot(aes(x = visits)) +\n    geom_histogram(stat = \"count\") +\n    theme_classic()\n```\n\n\n\n\n\n\n\n\n\n\n\n(a) Illnesses\n\n\n\n\n\n\n\n\n\n\n\n(b) Visits to the doctor\n\n\n\n\n\n\n\nFigure 3: Two variants of graphs"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-26",
    "href": "lectures/lecture-05-slides.html#section-26",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.30 ",
    "text": "1.30 \nWe can take a similar approach to cross-reference tables. For instance, (@tbl-docvisittable) will produce: (Table 1). In this case we specify “tbl” at the start of the label so that Quarto knows that it is a table. And we specify a caption for the table with “tbl-cap:”.\n\n```{r}\n#| label: tbl-docvisittable\n#| tbl-cap: \"Distribution of the number of doctor visits\"\n\nDoctorVisits |&gt;\n    count(visits) |&gt;\n    kable()\n```\n\n\n\nTable 1: Distribution of the number of doctor visits\n\n\n\n\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-27",
    "href": "lectures/lecture-05-slides.html#section-27",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.31 ",
    "text": "1.31 \nFinally, we can also cross-reference equations. To that we need to add a tag such as {#eq-macroidentity} which we then reference.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-gdpidentity}\nFor instance, we then use @eq-gdpidentity to produce Equation 1\n\\[\nY = C + I + G + (X - M)\n\\qquad(1)\\]\nLabels should be relatively simple when using cross-references. In general, try to keep the names simple but unique, avoid punctuation, and stick to letters and hyphens. Try not to use underscores, because they can cause an error."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#r-projects-and-file-structure",
    "href": "lectures/lecture-05-slides.html#r-projects-and-file-structure",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.32 R Projects and file structure",
    "text": "1.32 R Projects and file structure\nProjects are widely used in software development and exist to keep all the files (data, analysis, report, etc) associated with a particular project together and related to each other. (This use of “project” in a software development sense, is distinct to a “project”, in the project management sense.) An R Project can be created in RStudio. Click “File” \\(\\rightarrow\\) “New Project”, then select “Empty project”, name the R Project and decide where to save it. For instance, a R Project focused on maternal mortality may be called “maternalmortality”. The use of R Projects enables “reliable, polite behavior across different computers or users and over time” (Bryan and Hester 2020). This is because they remove the context of that folder from its broader existence; files exist in relation to the base of the R Project, not the base of the computer."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-28",
    "href": "lectures/lecture-05-slides.html#section-28",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.33 ",
    "text": "1.33 \nOnce a project has been created, a new file with the extension “.RProj” will appear in that folder. An example of a folder with an R Project, a Quarto document, and an appropriate file structure is available here. That can be downloaded: “Code” \\(\\rightarrow\\) “Download ZIP”.\nThe main advantage of using an R Project is that we can reference files within it in a self-contained way. That means when others want to reproduce our work, they will not need to change all the file references and structure as everything is referenced in relation to the “.Rproj” file. For instance, instead of reading a CSV from, say, \"~/Documents/projects/book/data/\" you can read it from book/data/. It may be that someone else does not have a projects folder, and so the former would not work for them, while the latter would."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-29",
    "href": "lectures/lecture-05-slides.html#section-29",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.34 ",
    "text": "1.34 \nThe use of projects is required to meet the minimal level of reproducibility expected of credible work. The use of functions such as setwd(), and computer-specific file paths, bind work to a specific computer in a way that is not appropriate.\nThere are a variety of ways to set up a folder. A variant of Wilson et al. (2017) that is often useful when you are getting started is shown in the example file structure linked above."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-30",
    "href": "lectures/lecture-05-slides.html#section-30",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.35 ",
    "text": "1.35 \nexample_project/\n├── .gitignore\n├── LICENSE.md\n├── README.md\n├── example_project.Rproj\n├── inputs\n│   ├── data\n│   │   ├── unedited_data.csv\n│   │   └── ...\n│   ├── literature\n│   │   ├── alexander-tellingstorieswithdata.pdf\n│   │   ├── gelman-xboxpaper.pdf\n│   │   └── ...\n├── outputs\n│   ├── README.md\n│   ├── data\n│   │   ├── analysis_data.csv\n│   │   └── ...\n│   ├── paper\n│   │   ├── paper.pdf\n│   │   ├── paper.qmd\n│   │   ├── references.bib\n│   │   └── ...\n│   └── ...\n├── scripts\n│   ├── 00-simulate_data.R\n│   ├── 01-download_data.R\n│   ├── 02-data_cleaning.R\n│   ├── 03-test_data.R\n│   └── ...\n└── ...\nHere we have an inputs folder that contains original, unedited data that should not be written over (Wilson et al. 2017) and literature related to the project. An outputs folder contains data that we create using R, as well as the paper that we are writing. And a scripts folder is what modifies the unedited data and saves it into outputs. We will do most of our work in “scripts”, and the Quarto file for the paper in outputs. Useful other aspects include a README.md which will specify overview details about the project, and a LICENSE. An example of what to put in the README is here. Another helpful variant of this project skeleton is provided by Mineault and The Good Research Code Handbook Community (2021)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-31",
    "href": "lectures/lecture-05-slides.html#section-31",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.36 ",
    "text": "1.36"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-32",
    "href": "lectures/lecture-05-slides.html#section-32",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.1 ",
    "text": "2.1 \nIn this book we implement version control through a combination of Git and GitHub. There are a variety of reasons for this including:\n\nenhancing the reproducibility of work by making it easier to share code and data;\nmaking it easier to share work;\nimproving workflow by encouraging systematic approaches; and\nmaking it easier to work in teams."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-33",
    "href": "lectures/lecture-05-slides.html#section-33",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.2 ",
    "text": "2.2 \nGit is a version control system with a fascinating history (Brown 2018). The way one often starts doing version control is to have various copies of the one file: “first_go.R”, “first_go-fixed.R”, “first_go-fixed-with-mons-edits.R”. But this soon becomes cumbersome. One often soon turns to dates, for instance: “2022-01-01-analysis.R”, “2022-01-02-analysis.R”, “2022-01-03-analysis.R”, etc. While this keeps a record, it can be difficult to search when we need to go back, because it is hard to remember the date some change was made. In any case, it quickly gets unwieldy for a project that is being regularly worked on."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-34",
    "href": "lectures/lecture-05-slides.html#section-34",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.3 ",
    "text": "2.3 \nInstead of this, we use Git so that we can have one version of the file. Git keeps a record of the changes to that file, and a snapshot of that file at a given point in time. We determine when Git takes that snapshot. We additionally include a message saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, and the history can be more easily searched."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-35",
    "href": "lectures/lecture-05-slides.html#section-35",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.4 ",
    "text": "2.4 \nOne complication is that Git was designed for teams of software developers. As such, while it works, it can be a little ungainly for non-developers. Nonetheless Git has been usefully adapted for data science, even when the only collaborator one may have is one’s future self (Bryan 2018).\nGitHub, GitLab, and various other companies offer easier-to-use services that build on Git. While there are tradeoffs, we introduce GitHub here because it is the predominant platform (Eghbal 2020, 21). Git and GitHub are built into Posit Cloud, which provides a nice option if you have issues with local installation. One of the initial challenging aspects of Git is the terminology. Folders are called “repos”. Creating a snapshot is called a “commit”. One gets used to it eventually, but feeling confused initially is normal. Bryan (2020) is especially useful for setting up and using Git and GitHub."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-36",
    "href": "lectures/lecture-05-slides.html#section-36",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.5 ",
    "text": "2.5 \n2.5.1 Git\nWe first need to check whether Git is installed. Open RStudio, go to the Terminal, type the following, and then enter/return.\n\ngit --version\n\nIf you get a version number, then you are done (Figure 4 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Using Terminal to check whether Git is installed in RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Adding a username and email address to Git in RStudio\n\n\n\n\n\n\n\nFigure 4: An overview of the steps involved in setting up Git"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-37",
    "href": "lectures/lecture-05-slides.html#section-37",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.6 ",
    "text": "2.6 \nGit is pre-installed in Posit Cloud, it should be pre-installed on Mac, and it may be pre-installed on Windows. If you do not get a version number in response, then you need to install it. To do that you should follow the instructions specific to your operating system in Bryan (2020, chap. 5).\nAfter Git is installed we need to tell it a username and email. We need to do this because Git adds this information whenever we take a snapshot, or to use Git’s language, whenever we make a commit."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-38",
    "href": "lectures/lecture-05-slides.html#section-38",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.7 ",
    "text": "2.7 \nAgain, within the Terminal, type the following, replacing the details with yours, and then press “enter/return” after each line.\n\ngit config --global user.name \"Rohan Alexander\"\ngit config --global user.email \"rohan.alexander@utoronto.ca\"\ngit config --global --list\n\nWhen this set-up has been done properly, the values that you entered for “user.name” and “user.email” will be returned after the last line (Figure 4 (b)).\nThese details—username and email address—will be public. There are various ways to hide the email address if necessary, and GitHub provides instructions about this. Bryan (2020, chap. 7) provides more detailed instructions about this step, and a trouble-shooting guide."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-39",
    "href": "lectures/lecture-05-slides.html#section-39",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.8 ",
    "text": "2.8 \n2.8.1 GitHub\nNow that Git is set up, we need to set up GitHub. We created a GitHub account in ?@sec-fire-hose, which we use again here. After being signed in at github.com we first need to make a new folder, which is called a “repo” in Git. Look for a “+” in the top right, and then select “New Repository” (Figure 5 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Start process of creating a new repository\n\n\n\n\n\n\n\n\n\n\n\n(b) Copy the URL of the new repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding the project to Posit Cloud\n\n\n\n\n\n\n\n\n\n\n\n(d) Creating a PAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Adding files to be committed\n\n\n\n\n\n\n\n\n\n\n\n(f) Making a commit\n\n\n\n\n\n\n\nFigure 5: An overview of the steps involved in setting up GitHub"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-40",
    "href": "lectures/lecture-05-slides.html#section-40",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.9 ",
    "text": "2.9 \nAt this point we can add a sensible name for the repo. Leave it as “public” for now, because it can always be deleted later. And check the box to “Initialize this repository with a README”. Change “Add .gitignore” to R. After that, click “Create repository”.\nThis will take us to a screen that is fairly empty, but the details that we need—a URL—are in the green “Clone or Download” button, which we can copy by clicking the clipboard (Figure 5 (b))."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-41",
    "href": "lectures/lecture-05-slides.html#section-41",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.10 ",
    "text": "2.10 \nNow returning to RStudio, in Posit Cloud, we create a new project using “New Project from Git Repository”. It will ask for the URL that we just copied (Figure 5 (c)). If you are using a local computer, then this step is accomplished through the menu: “File” \\(\\rightarrow\\) “New Project…” \\(\\rightarrow\\) “Version Control” \\(\\rightarrow\\) “Git”, then paste in the URL, give the folder a meaningful name, check “Open in new session”, then click “Create Project”."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-42",
    "href": "lectures/lecture-05-slides.html#section-42",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.11 ",
    "text": "2.11 \nAt this point, a new folder has been created that we can use. We will want to be able to push it back to GitHub, and for that we will need to use a Personal Access Token (PAT) to link our RStudio Workspace with our GitHub account. We use usethis and gitcreds to enable this. These are, respectively, a package that automates repetitive tasks, and a package that authenticates with GitHub. To create a PAT, while signed into GitHub in the browser, and after installing and loading usethis run create_github_token() in your R session. GitHub will open in the browser with various options filled out (Figure 5 (d)). It can be useful to give the PAT an informative name by replacing “Note”, for instance “PAT for RStudio”, then click “Generate token”."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-43",
    "href": "lectures/lecture-05-slides.html#section-43",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.12 ",
    "text": "2.12 \nWe only have one chance to copy this token, and if we make a mistake then we will need to generate a new one. Do not include the PAT in any R script or Quarto document. Instead, after installing and loading gitcreds, run gitcreds_set(), which will then prompt you to add your PAT in the console."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-44",
    "href": "lectures/lecture-05-slides.html#section-44",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.13 ",
    "text": "2.13 \nTo use GitHub for a project that we are actively working on we follow this procedure:\n\nThe first thing to do is almost always to get any changes with “pull”. To do this, open the Git pane in RStudio, and click the blue down arrow. This gets any changes to the folder, as it is on GitHub, into our own version of the folder.\nWe can then make our changes to our copy of the folder. For instance, we could update the README, and then save it as normal.\nOnce this is done, we need to add, commit, and push. In the Git pane in RStudio, select the files to be added. This adds them to the staging area. Then click “Commit” (Figure 5 (e)). A new window will open. Add an informative message about the change that was made, and then click “Commit” in that new window (Figure 5 (f)). Finally, click “Push” to send the changes to GitHub."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-45",
    "href": "lectures/lecture-05-slides.html#section-45",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.14 ",
    "text": "2.14 \nThere are a few common pain-points when it comes to Git and GitHub. We recommend committing and pushing regularly, especially when you are new to version control. This increases the number of snapshots that you could come back to if needed. All commits should have an informative commit message. If you are new to version control, then the expectation of a good commit message is that it contains a short summary of the change, followed by a blank line, and then an explanation of the change including what the change is, and why it is being made. For instance, if your commit adds graphs to a paper, then a commit message could be:\nAdd graphs\n\nGraphs of unemployment and inflation added into Data section."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-46",
    "href": "lectures/lecture-05-slides.html#section-46",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.15 ",
    "text": "2.15 \nThere is some evidence of a relationship between overall quality and commit behavior (Sprint and Conci 2019). As you get more experience ideally the commit messages will act as a kind of journal of the project. But the main thing is to commit regularly.\nGit and GitHub were designed for software developers, rather than data scientists. GitHub limits the size of the files it will consider to 100MB, and even 50MB can prompt a warning. Data science projects regularly involve datasets that are larger than this. In ?@sec-store-and-share we discuss the use of data deposits, which can be especially useful when a project is completed, but when we are actively working on a project it can be useful to ignore large data files, at least as far as Git and GitHub are concerned. We do this using a “.gitignore” file, in which we list all of the files that we do not want to track using Git. The example folder contains a “.gitignore” file. And it can be helpful to run git_vaccinate() from usethis, which will add a variety of files to a global “.gitignore” file in case you forget to do it on a project basis. Mac users will find it useful that this will cause “.DS_Store” files to be ignored."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-47",
    "href": "lectures/lecture-05-slides.html#section-47",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.16 ",
    "text": "2.16 \nWe used the Git pane in RStudio which removed the need to use the Terminal, but it did not remove the need to go to GitHub and set up a new project. Having set up Git and GitHub, we can further improve this aspect of our workflow with usethis.\nFirst check that Git is set up with git_sitrep() from usethis. This should print information about the username and email. We can use use_git_config() to update these details if needed.\n\nuse_git_config(\n    user.name = \"Rohan Alexander\",\n    user.email = \"rohan.alexander@utoronto.ca\"\n)"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-48",
    "href": "lectures/lecture-05-slides.html#section-48",
    "title": "Reproducible Research with R and RStudio",
    "section": "2.17 ",
    "text": "2.17 \nRather than starting a new project in GitHub, and then adding it locally, we can now use use_git() to initiate it and commit the files. Having committed, we can use use_github() to push to GitHub, which will create the folder on GitHub as well.\nIt is normal to be intimidated by Git and GitHub. Many data scientists only know a little about how to use it, and that is okay. Try to push regularly so that you have a recent snapshot in case you need it."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#sec-dealingwitherrors",
    "href": "lectures/lecture-05-slides.html#sec-dealingwitherrors",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.1 Dealing with errors",
    "text": "3.1 Dealing with errors\n\nWhen you are programming, eventually your code will break, when I say eventually, I mean like probably 10 or 20 times a day.\nGelfand (2021)"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-49",
    "href": "lectures/lecture-05-slides.html#section-49",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.2 ",
    "text": "3.2 \nEveryone who uses R, or any programming language for that matter, has trouble find them at some point. This is normal. Programming is hard. At some point code will not run or will throw an error. This happens to everyone. It is common to get frustrated, but to move forward we develop strategies to work through the issues:\n\nIf you are getting an error message, then sometimes it will be useful. Try to read it carefully to see if there is anything of use in it.\nTry to search for the error message. It can be useful to include “tidyverse” or “in R” in the search to help make the results more appropriate. Sometimes Stack Overflow results can be useful.\nLook at the help file for the function by putting “?” before the function, for instance, ?pivot_wider(). A common issue is to use a slightly incorrect argument name or format, such as accidentally including a string instead of an object name.\nLook at where the error is happening and remove or comment out code until the error is resolved, and then slowly add code back again.\nCheck the class of the object with class(), for instance, class(data_set$data_column). Ensure that it is what is expected.\nRestart R: “Session” \\(\\rightarrow\\) “Restart R and Clear Output”. Then load everything again.\nRestart your computer.\nSearch for what you are trying to do, rather than the error, being sure to include “tidyverse” or “in R” in the search to help make the results more appropriate. For instance, “save PDF of graph in R using ggplot”. Sometimes there are relevant blog posts or Stack Overflow answers that will help.\nMake a small, self-contained, reproducible example “reprex” to see if the issue can be isolated and to enable others to help.\nIf you are working in a Quarto doc then include label in the chunk options to make it easier to find where the mistake may be happening."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-50",
    "href": "lectures/lecture-05-slides.html#section-50",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.3 ",
    "text": "3.3 \nMore generally, while this is not always possible, it is almost always helpful to take a break and come back the next day."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-51",
    "href": "lectures/lecture-05-slides.html#section-51",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.4 ",
    "text": "3.4 \n3.4.1 Reproducible examples\nAsking for help is a skill like any other. We get better at it with practice. It is important to try not to say “this doesn’t work”, “I tried everything”, “your code does not work”, or “here is the error message, what do I do?”. In general, it is not possible to help based on these comments, because there are too many possible issues. You need to make it easy for others to help you. This involves a few steps.\n\nProvide a small, self-contained example of your data, and code, and detail what is going wrong.\nDocument what you have tried so far, including which Stack Overflow and Posit Forum posts you looked at, and why they are not what you are after.\nBe clear about the outcome that you would like."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-52",
    "href": "lectures/lecture-05-slides.html#section-52",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.5 ",
    "text": "3.5 \nBegin by creating a minimal REPRoducible EXample—a “reprex”. This is code that contains what is needed to reproduce the error, but only what is needed. This means that the code is likely a smaller, simpler version that nonetheless reproduces the error.\nSometimes this process enables one to solve the problem. If it does not, then it gives someone else a fighting chance of being able to help. There is almost no chance that you have got a problem that someone has not addressed before. It is more likely that the main difficulty is trying to communicate what you want to do and what is happening, in a way that allows others to recognize both. Developing tenacity is important.\nTo develop reproducible examples, reprex is especially useful. After installing it we:\n\nLoad the reprex package: library(reprex).\nHighlight and copy the code that is giving issues.\nRun reprex() in the console.\n\nIf the code is self-contained, then it will preview in the viewer. If it is not, then it will error, and you should rewrite the code so that it is self-contained."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-53",
    "href": "lectures/lecture-05-slides.html#section-53",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.6 ",
    "text": "3.6 \nIf you need data to reproduce the error, then you should use data that is built into R. There are a large number of datasets that are built into R and can be seen using library(help = \"datasets\"). But if possible, you should use a common option such as mtcars or faithful. Combining a reprex with a GitHub Gist that was introduced in ?@sec-fire-hose increases the chances that someone is able to help you."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-54",
    "href": "lectures/lecture-05-slides.html#section-54",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.7 ",
    "text": "3.7 \n3.7.1 Mentality\n\n(Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you\n(L)et’s break down the gates, there’s enough room for everyone\nSharla Gelfand, 10 March 2020.\n\nIf you write code, then you are a programmer, regardless of how you do it, what you are using it for, or who you are. But there are a few traits that one tends to notice great programmers have in common."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-55",
    "href": "lectures/lecture-05-slides.html#section-55",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.8 ",
    "text": "3.8 \n\nFocused: Often having an aim to “learn R” or similar tends to be problematic, because there is no real end point to that. It tends to be more efficient to have smaller, more specific goals, such as “make a histogram about the 2022 Australian Election with ggplot2”. This is something that can be focused on and achieved in a few hours. The issue with goals that are more nebulous, such as “I want to learn R”, is that it is easier to get lost on tangents and more difficult to get help. This can be demoralizing and lead to people quitting too early.\nCurious: It is almost always useful to “have a go”; that is, if you are not sure, then just try it. In general, the worst that happens is that you waste your time. You can rarely break something irreparably. For instance, if you want to know what happens if you pass a vector instead of a dataframe to ggplot() then try it.\nPragmatic: At the same time, it can be useful to stick within reasonable bounds, and make one small change each time. For instance, say you want to run some regressions, and are curious about the possibility of using rstanarm instead of lm(). A pragmatic way to proceed is to use one aspect from rstanarm initially and then make another change next time.\nTenacious: Again, this is a balancing act. Unexpected problems and issues arise with every project. On the one hand, persevering despite these is a good tendency. But on the other hand, sometimes one does need to be prepared to give up on something if it does not seem like a break through is possible. Mentors can be useful as they tend to be a better judge of what is reasonable.\nPlanned: It is almost always useful to excessively plan what you are going to do. For instance, you may want to make a histogram of some data. You should plan the steps that are needed and even sketch out how each step might be implemented. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is the back-up plan if the data do not exist there?\nDone is better than perfect: We all have various perfectionist tendencies, but it can be useful to initially try to turn them off to a certain extent. Initially just worry about writing code that works. You can always come back and improve aspects of it. But it is important to actually ship. Ugly code that gets the job done is better than beautiful code that is never finished."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#code-comments-and-style",
    "href": "lectures/lecture-05-slides.html#code-comments-and-style",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.9 Code comments and style",
    "text": "3.9 Code comments and style\nCode must be commented. Comments should focus on why certain code was written and to a lesser extent, why a common alternative was not selected. Indeed, it can be a good idea to write the comments before you write the code, explaining what you want to do and why, and then returning to write the code (Fowler and Beck 2018, 59).\nThere is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you are just working on your own. Most projects will evolve over time, and one purpose of code comments is to enable future-you to retrace what was done and why certain decisions were made (Bowers and Voors 2016)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-56",
    "href": "lectures/lecture-05-slides.html#section-56",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.10 ",
    "text": "3.10 \nComments in R scripts can be added by including the # symbol. (The behavior of # is different for lines inside an R chunk in a Quarto document where it acts as a comment, compared with lines outside an R chunk where it sets heading levels.) We do not have to put a comment at the start of the line, it can be midway through. In general, you do not need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if we read in some value then we may like to comment where it is coming from.\nYou should try to comment why you are doing something (Wickham 2021). What are you trying to achieve? You must comment to explain weird things. Like if you are removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you will not remember."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-57",
    "href": "lectures/lecture-05-slides.html#section-57",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.11 ",
    "text": "3.11 \nYou should break your code into sections. For instance, setting up the workspace, reading in datasets, manipulating and cleaning the datasets, analyzing the datasets, and finally producing tables and figures. Each of these should be separated with comments explaining what is going on, and sometimes into separate files, depending on the length."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-58",
    "href": "lectures/lecture-05-slides.html#section-58",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.12 ",
    "text": "3.12 \nAdditionally, at the top of each file it is important to note basic information, such as the purpose of the file, and prerequisites or dependencies, the date, the author and contact information, and finally any red flags or todos.\nYour R scripts should have a preamble and a clear demarcation of sections.\n#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Date: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep install.packages lines; comment out if need be\n# Load packages\nlibrary(tidyverse)\n\n# Read in the unedited data. \nraw_data &lt;- read_csv(\"inputs/data/unedited_data.csv\")\n\n\n#### Next section ####\n..."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-59",
    "href": "lectures/lecture-05-slides.html#section-59",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.13 ",
    "text": "3.13 \nFinally, try not to rely on a user commenting and uncommenting code, or any other manual step, such as directory specification, for code to work. This will preclude the use of automated code checking and testing.\nThis all takes time. As a rough rule of thumb, you should expect to spend at least as much time commenting and improving your code as you spent writing it. Some examples of nicely commented code include Dolatsara et al. (2021) and Burton, Cruz, and Hahn (2021)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#tests",
    "href": "lectures/lecture-05-slides.html#tests",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.14 Tests",
    "text": "3.14 Tests\nTests should be written throughout the code, and you need to write them as we go, not all at the end. This will slow you down. But it will help you to think, and to fix mistakes, which will make your code better and lead to better overall productivity. Code without tests should be viewed with suspicion. There is room for improvement when it comes to testing practices in R packages (Vidoni 2021), let alone R code more generally.\nThe need for other people, and ideally, automated processes, to run tests on code is one reason that we emphasize reproducibility. That is also why we emphasize smaller aspects such as not hardcoding file-paths, using projects, and not having spaces in file names."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-60",
    "href": "lectures/lecture-05-slides.html#section-60",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.15 ",
    "text": "3.15 \nIt is difficult to define a complete and general suite of tests, but broadly we want to test:\n\nboundary conditions,\nclasses,\nmissing data,\nthe number of observations and variables,\nduplicates, and\nregression results."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-61",
    "href": "lectures/lecture-05-slides.html#section-61",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.16 ",
    "text": "3.16 \nWe do all this initially on our simulated data and then move to the real data. The mirrors the evolution of testing during the Apollo Program. Initially testing occured based on expectations of requirements, and these tests were later updated to take into account actual launch measurements (Simpkinson 1971, 21). It is possible to write an infinite number of tests but a smaller number of high-quality tests is better than many thoughtless tests."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-62",
    "href": "lectures/lecture-05-slides.html#section-62",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.17 ",
    "text": "3.17 \nOne type of test is an “assertion”. Assertions are written throughout the code to check whether something is true and stop the code from running if not (Irving et al. 2021, 272). For instance, you might assert that a variable should be numeric. If it was tested against this assertion and found to be a character, then the test would fail and the script would stop running. Assertion tests in data science will typically be used in data cleaning and preparation scripts. We have more to say about these in ?@sec-clean-and-prepare. Unit tests check some complete aspect of code (Irving et al. 2021, 274). We will consider them more in ?@sec-its-just-a-linear-model when we consider modeling."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#efficiency",
    "href": "lectures/lecture-05-slides.html#efficiency",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.18 Efficiency",
    "text": "3.18 Efficiency\nGenerally in this book we are, and will continue to be, concerned with just getting something done. Not necessarily getting it done in the best or most efficient way, because to a large extent, being worried about that is a waste of time. For the most part one is better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But that eventually becomes unfeasible. At a certain point, and this differs depending on context, efficiency becomes important. Eventually ugly or slow code, and dogmatic insistence on a particular way of doing things, have an effect. And it is at that point that one needs to be open to new approaches to ensure efficiency. There is rarely a most common area for obvious performance gains. Instead, it is important to develop the ability to measure, evaluate, and think."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-63",
    "href": "lectures/lecture-05-slides.html#section-63",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.19 ",
    "text": "3.19 \nOne of the best ways to improve the efficiency of our code is preparing it in such a way that we can bring in a second pair of eyes. To make the most of their time, it is important that our code easy to read. So we start with “code linting” and “styling”. This does not speed up our code, per se, but instead makes it more efficient when another person comes to it, or we revisit it. This enables formal code review and refactoring, which is where we rewrite code to make it better, while not changing what it does (it does the same thing, but in a different way). We then turn to measurement of run time, and introduce parallel processing, where we allow our computer to run code for multiple processes at the same time"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#sharing-a-code-environment",
    "href": "lectures/lecture-05-slides.html#sharing-a-code-environment",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.20 Sharing a code environment",
    "text": "3.20 Sharing a code environment\nWe have discussed at length the need to share code, and we have put forward an approach to this using GitHub. And in ?@sec-store-and-share, we will discuss sharing data. But, there is another requirement to enable other people to run our code. In ?@sec-fire-hose we discussed how R itself, as well as R packages update from time to time, as new functionality is developed, errors fixed, and other general improvements made. ?@sec-r-essentials describes how one advantage of the tidyverse is that it can update faster than base R, because it is more specific. But this could mean that even if we were to share all the code and data that we use, it is possible that the software versions that have become available would cause errors."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-64",
    "href": "lectures/lecture-05-slides.html#section-64",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.21 ",
    "text": "3.21 \nThe solution to this is to detail the environment that was used. There are a large number of ways to do this, and they can add complexity. We just focus on documenting the version of R and R packages that were used, and making it easier for others to install that exact version. Essentially we are just isolating the set-up that we used because that will help with reproducibility (Perkel 2023). In R we can use renv to do this.\nOnce renv is installed and loaded, we use init() to get the infrastructure set-up that we will need. We are going to create a file that will record the packages and versions used. We then use snapshot() to actually document what we are using. This creates a “lockfile” that records the information."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-65",
    "href": "lectures/lecture-05-slides.html#section-65",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.22 ",
    "text": "3.22 \nIf we want to see which packages we are using in the R Project, then we can use dependencies(). Doing this for the example folder indicates that the following packages are used: rmarkdown, bookdown, knitr, rmarkdown, bookdown, knitr, palmerpenguins, tidyverse, renv, haven, readr, and tidyverse."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-66",
    "href": "lectures/lecture-05-slides.html#section-66",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.23 ",
    "text": "3.23 \nWe could open the lockfile file—“renv.lock”—to see the exact versions if we wanted. The lockfile also documents all the other packages that were installed and where they were downloaded from. Someone coming to this project from outside could then use restore() which would install the exact version of the packages that we used."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#code-linting-and-styling",
    "href": "lectures/lecture-05-slides.html#code-linting-and-styling",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.24 Code linting and styling",
    "text": "3.24 Code linting and styling\nBeing fast is valuable but it is mostly about being able to iterate fast, not necessarily having code that runs fast. Backus (1981, 26) describes how even in 1954 a programmer cost at least as much as a computer, and these days additional computational power is usually much cheaper than a programmer. Performant code is important, but it is also important to use other people’s time efficiently. Code is rarely only written once. Instead we typically have to come back to it, even if to just fix mistakes, and this means that code must be able to be read by humans (Matsumoto 2007, 478). If this is not done then there will be an efficiency cost.\nLinting and styling is the process of checking code, mostly for stylistic issues, and re-arranging code to make it easier to read. (There is another aspect of linting, which is dealing with programming errors, such as forgetting a closing bracket, but here we focus on stylistic issues.) Often the best efficiency gain comes from making it easier for others to read our code, even if this is just ourselves returning to the code after a break. Jane Street, a US proprietary trading firm, places a very strong focus on ensuring their code is readable, as a core part of risk mitigation (Minsky 2011). While we may not all have billions of dollars under the potentially mercurial management of code, we all would likely prefer that our code does not produce errors."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-67",
    "href": "lectures/lecture-05-slides.html#section-67",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.25 ",
    "text": "3.25 \nWe use lint() from lintr to lint our code. For instance, consider the following R code (saved as “linting_example.R”).\n\nSIMULATED_DATA &lt;-\n    tibble(\n        division = c(1:150, 151),\n        party = sample(\n            x = c(\"Liberal\"),\n            size = 151,\n            replace = T\n        )\n    )"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-68",
    "href": "lectures/lecture-05-slides.html#section-68",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.26 ",
    "text": "3.26 \n\nlint(filename = \"linting_example.R\")"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-69",
    "href": "lectures/lecture-05-slides.html#section-69",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.27 ",
    "text": "3.27 \nThe result is that the file “linting_example.R” is opened and the issues that lint() found are printed in “Markers” (Figure 6). It is then up to you to deal with the issues.\n\n\nFigure 6: Linting results from example R code"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-70",
    "href": "lectures/lecture-05-slides.html#section-70",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.28 ",
    "text": "3.28 \nMaking the recommended changes results in code that is more readable, and consistent with best practice, as defined by Wickham (2021).\n\nsimulated_data &lt;-\n    tibble(\n        division = c(1:150, 151),\n        party = sample(\n            x = c(\"Liberal\"),\n            size = 151,\n            replace = TRUE\n        )\n    )"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-71",
    "href": "lectures/lecture-05-slides.html#section-71",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.29 ",
    "text": "3.29 \nAt first it may seem that some aspects that the linter is identifying, like trailing whitespace and only using double quotes are small and inconsequential. But they distract from being able to fix bigger issues. Further, if we are not able to get small things right, then how could anyone trust that we could get the big things right? Therefore, it is important to have dealt with all the small aspects that a linter identifies."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-72",
    "href": "lectures/lecture-05-slides.html#section-72",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.30 ",
    "text": "3.30 \nIn addition to lintr we also use styler. This will automatically adjust style issues, in contrast to the linter, which gave a list of issues to look at. To run this we use style_file().\n\nstyle_file(path = \"linting_example.R\")\n\nThis will automatically make changes, such as spacing and indentation. As such this should be done regularly, rather than only once at the end of a project, so as to be able to review the changes and make sure no errors have been introduced."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#code-review",
    "href": "lectures/lecture-05-slides.html#code-review",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.31 Code review",
    "text": "3.31 Code review\nHaving dealt with all of these aspects of style, we can turn to code review. This is the process of having another person go through and critique the code. Many professional writers have editors, and code review is the closest that we come to that in data science. Code review is a critical part of writing code, and Irving et al. (2021, 465) describe it as “the most effective way to find bugs”. It is especially helpful, although quite daunting, when learning to code because getting feedback is a great way to improve."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-73",
    "href": "lectures/lecture-05-slides.html#section-73",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.32 ",
    "text": "3.32 \nGo out of your way to be polite and collegial when reviewing another person’s code. Small aspects to do with style, things like spacing and separation, should have been taken care of by a linter and styler, but if not, then make a general recommendation about that. Most of your time as a code reviewer in data science should be spent on aspects such as:\n\nIs there an informative README and how could it be improved?\nAre the file names and variable names consistent, informative, and meaningful?\nDo the comments allow you to understand why something is being done?\nAre the tests both appropriate and sufficient? Are there edge cases or corner solutions that are not considered? Similarly, are there unnecessary tests that could be removed?\nAre there magic numbers that could be changed to variables and explained?\nIs there duplicated code that could be changed?\nAre there any outstanding warnings that should be addressed?\nAre there any especially large functions or pipes that could be separated into smaller ones?\nIs the structure of the project appropriate?\nCan we change any of the code to data (Irving et al. 2021, 462)?"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-74",
    "href": "lectures/lecture-05-slides.html#section-74",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.33 ",
    "text": "3.33 \nFor instance, consider some code that looked for the names of prime ministers and presidents. When we first wrote this code we likely added the relevant names directly into the code. But as part of code review, we might instead recommend that this be changed. We might recommend creating a small dataset of relevant names, and then re-writing the code to have it look up that dataset.\nCode review ensures that the code can be understood by at least one other person. This is a critical part of building knowledge about the world. At Google, code review is not primarily about finding defects, although that may happen, but is instead about ensuring readability and maintainability as well as education (Sadowski et al. 2018). This is also the case at Jane Street where they use code review to catch bugs, share institutional knowledge, assist with training, and oblige staff to write code that can be read (Minsky 2015)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-75",
    "href": "lectures/lecture-05-slides.html#section-75",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.34 ",
    "text": "3.34 \nFinally, code review does not have to, and should not, be an onerous days-consuming process of reading all the code. The best code review is a quick review of just one file, focused on suggesting changes to just a handful of lines. Indeed, it may be better to have a review done by a small team of people rather than one individual. Do not review too much code at any one time. At most a few hundred lines, which should take around an hour, because any more than that has been found to be associated with reduced efficacy (Cohen, Teleki, and Brown 2006, 79)."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#code-refactoring",
    "href": "lectures/lecture-05-slides.html#code-refactoring",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.35 Code refactoring",
    "text": "3.35 Code refactoring\nTo refactor code means to rewrite it so that the new code achieves the same outcome as the old code, but the new code does it better. For instance, Chawla (2020) discuss how the code underpinning an important UK Covid model was initially written by epidemiologists, and months later clarified and cleaned up by a team from the Royal Society, Microsoft, and GitHub. This was valuable because it provided more confidence in the model, even though both versions produced the same outputs, given the same inputs."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-76",
    "href": "lectures/lecture-05-slides.html#section-76",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.36 ",
    "text": "3.36 \nWe typically refer to code refactoring in relation to code that someone else wrote. (Although it may be that we actually wrote the code, and it was just that it was some time ago.) When we start to refactor code, we want to make sure that the rewritten code achieves the same outcomes as the original code. This means that we need a suite of appropriate tests written that we can depend on. If these do not exist, then we may need to create them.\nWe rewrite code to make it easier for others to understand, which in turn allows more confidence in our conclusions. But before we can do that, we need to understand what the existing code is doing. One way to get started is to go through the code and add extensive comments. These comments are different to normal comments. They are our active process of trying to understand what is each code chunk trying to do and how could this be improved."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-77",
    "href": "lectures/lecture-05-slides.html#section-77",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.37 ",
    "text": "3.37 \nRefactoring code is an opportunity to ensure that it satisfies best practice. Trisovic et al. (2022) details some core recommendations based on examining 9,000 R scripts including:\n\nRemove setwd() and any absolute paths, and ensure that only relative paths, in relation to the “.Rproj” file, are used.\nEnsure there is a clear order of execution. We have recommended using numbers in filenames to achieve this initially, but eventually more sophisticated approaches, such as targets (Landau 2021), could be used instead.\nEnsure that code can run on a different computer."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-78",
    "href": "lectures/lecture-05-slides.html#section-78",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.38 ",
    "text": "3.38 \nFor instance, consider the following code:\n\nsetwd(\"/Users/rohanalexander/Documents/telling_stories\")\n\nlibrary(tidyverse)\n\nd &lt;- read_csv(\"cars.csv\")\n\nmtcars &lt;-\n    mtcars |&gt;\n    mutate(K_P_L = mpg / 2.352)\n\nlibrary(datasauRus)\n\ndatasaurus_dozen"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#section-79",
    "href": "lectures/lecture-05-slides.html#section-79",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.39 ",
    "text": "3.39 \nWe could change that, starting by creating an R Project which enables us to remove setwd(), grouping all the library() calls at the top, using “&lt;-” instead of “=”, and being consistent with variable names:\n\nlibrary(tidyverse)\nlibrary(datasauRus)\n\ncars_data &lt;- read_csv(\"cars.csv\")\n\nmpg_to_kpl_conversion_factor &lt;- 2.352\n\nmtcars &lt;-\n    mtcars |&gt;\n    mutate(kpl = mpg / mpg_to_kpl_conversion_factor)"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#concluding-remarks",
    "href": "lectures/lecture-05-slides.html#concluding-remarks",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.40 Concluding remarks",
    "text": "3.40 Concluding remarks\nIn this chapter we have considered much and it is normal to be overwhelmed. Come back to the Quarto section as needed. Many people are confused by Git and GitHub and just know enough to get by. And while there was a lot of material in efficiency, the most important aspect of performant code is making it easier for another person to read it, even if that person is just yourself returning after a break."
  },
  {
    "objectID": "lectures/lecture-05-slides.html#references",
    "href": "lectures/lecture-05-slides.html#references",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.41 References",
    "text": "3.41 References"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#references-1",
    "href": "lectures/lecture-05-slides.html#references-1",
    "title": "Reproducible Research with R and RStudio",
    "section": "3.42 References",
    "text": "3.42 References\n\n\nAlexander, Monica. 2019. “Reproducibility in Demographic Research.” https://www.monicaalexander.com/posts/2019-10-20-reproducibility/.\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nBackus, John. 1981. “The History of FORTRAN I, II, and III.” In History of Programming Languages, edited by Richard Wexelblat, 25–74. Academic Press.\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible Research.” https://arxiv.org/abs/1802.03311.\n\n\nBegley, Glenn, and Lee Ellis. 2012. “Raise Standards for Preclinical Cancer Research.” Nature 483 (7391): 531--533. https://doi.org/10.1038/483531a.\n\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and Distributed Processing in R using Futures.” The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBowers, Jake, and Maarten Voors. 2016. “How to Improve Your Relationship with Your Future Self.” Revista de Ciencia Polı́tica 36 (3): 829–48. https://doi.org/10.4067/S0718-090X2016000300011.\n\n\nBrown, Zack. 2018. “A Git Origin Story.” Linux Journal, July. https://www.linuxjournal.com/content/git-origin-story.\n\n\nBryan, Jenny. 2018. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\n———. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com.\n\n\nBryan, Jenny, and Jim Hester. 2020. What They Forgot to Teach You About R. https://rstats.wtf/index.html.\n\n\nBryan, Jenny, Jim Hester, David Robinson, Hadley Wickham, and Christophe Dervieux. 2022. reprex: Prepare Reproducible Example Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBuckheit, Jonathan, and David Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. https://doi.org/10.1007/978-1-4612-2544-7_5.\n\n\nBurton, Jason, Nicole Cruz, and Ulrike Hahn. 2021. “Reconsidering Evidence of Moral Contagion in Online Social Networks.” Nature Human Behaviour 5 (12): 1629–35. https://doi.org/10.1038/s41562-021-01133-5.\n\n\nChawla, Dalmeet Singh. 2020. “Critiqued Coronavirus Simulation Gets Thumbs up from Code-Checking Efforts.” Nature 582: 323–24. https://doi.org/10.1038/d41586-020-01685-y.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions.” In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, edited by Sorelle Friedler and Christo Wilson, 81:134–48. Proceedings of Machine Learning Research. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nCohen, Jason, Steven Teleki, and Eric Brown. 2006. Best Kept Secrets of Peer Code Review. Smart Bear Incorporated.\n\n\nCsárdi, Gábor. 2022. gitcreds: Query “git” Credentials from “R”. https://CRAN.R-project.org/package=gitcreds.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed, and Allison Jones-Farmer. 2021. “Explaining Predictive Model Performance: An Experimental Study of Data Preparation and Model Choice.” Big Data, October. https://doi.org/10.1089/big.2021.0067.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance of Open Source Software. California: Stripe Press.\n\n\nFowler, Martin, and Kent Beck. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. New York: Addison-Wesley Professional.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... Please.” YouTube, February. https://youtu.be/G5Nm-GpmrLw.\n\n\nGelman, Andrew. 2016. “What has happened down here is the winds have changed,” September. https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\nGelman, Andrew, Greggor Mattson, and Daniel Simpson. 2018. “Gaydar and the Fallacy of Decontextualized Measurement.” Sociological Science 5 (12): 270–80. https://doi.org/10.15195/v5.a12.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, and Stephanie Hicks. 2021. “Reproducibility Standards for Machine Learning in the Life Sciences.” Nature Methods 18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” Cambridge Journal of Economics 38 (2): 257–79. https://doi.org/10.1093/cje/bet075.\n\n\nHester, Jim, Florent Angly, Russ Hyde, Michael Chirico, Kun Ren, Alexander Rosenstock, and Indrajeet Patil. 2022. lintr: A “Linter” for R Code. https://CRAN.R-project.org/package=lintr.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte Wickham, and Greg Wilson. 2021. Research Software Engineering with Python. Chapman; Hall/CRC.\n\n\nIsaacson, Walter. 2011. Steve Jobs. 1st ed. Simon & Schuster.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, Donald. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLandau, William Michael. 2021. “The targets R Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nMaier, Maximilian, František Bartoš, Tom Stanley, David Shanks, Adam Harris, and Eric-Jan Wagenmakers. 2022. “No Evidence for Nudging After Adjusting for Publication Bias.” Proceedings of the National Academy of Sciences 119 (31): e2200300119. https://doi.org/10.1073/pnas.2200300119.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as an Essay.” In Beautiful Code, edited by Andy Oram and Greg Wilson, 477–81. O’Reilly.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers Gayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.” Nature 467 (7317): 775–77. https://doi.org/10.1038/467775a.\n\n\nMineault, Patrick, and The Good Research Code Handbook Community. 2021. “The Good Research Code Handbook.” https://doi.org/10.5281/zenodo.5796873.\n\n\nMinsky, Yaron. 2011. “OCaml for the masses.” Communications of the ACM 54 (11): 53–58. https://doi.org/10.1145/2018396.2018413.\n\n\n———. 2015. “Automated Trading and OCaml with Yaron Minsky.” Hackers — Software Engineering Daily, November. https://softwareengineeringdaily.com/2015/11/09/automated-trading-and-ocaml-with-yaron-minsky/.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another Possible Source of the Reproducibility Crisis.” Molecular Brain 13 (1): 1–6. https://doi.org/10.1186/s13041-020-0552-2.\n\n\nMullard, Asher. 2021. “Half of Top Cancer Studies Fail High-Profile Reproducibility Effort.” Nature 600 (7889): 368--369. https://doi.org/10.1038/d41586-021-03691-0.\n\n\nMüller, Kirill, and Lorenz Walthert. 2022. styler: Non-Invasive Pretty Printing of R Code. https://CRAN.R-project.org/package=styler.\n\n\nMurphy, Heather. 2017. “Why Stanford Researchers Tried to Create a ‘Gaydar’ Machine.” The New York Times, October. https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html.\n\n\nPerkel, Jeffrey. 2023. “The Sleight-of-Hand Trick That Can Simplify Scientific Computing.” Nature 617 (7959): 212--213. https://doi.org/10.1038/d41586-023-01469-0.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research (a Report from the NeurIPS 2019 Reproducibility Program).” Journal of Machine Learning Research 22 (164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and Alberto Bacchelli. 2018. “Modern Code Review: A Case Study at Google.” In Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, 181–90. ICSE-SEIP ’18. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3183519.3183525.\n\n\nSilver, Nate. 2020. “We Fixed an Issue with How Our Primary Forecast Was Calculating Candidates’ Demographic Strengths.” FiveThirtyEight, February. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSimpkinson, Scott. 1971. “Testing to Ensure Mission Success.” In What Made Apollo a Success, edited by NASA, 21–29.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining GitHub Classroom Commit Behavior in Elective and Introductory Computer Science Courses.” Journal of Computing Sciences in Colleges 35 (1): 76–84.\n\n\nSunstein, Cass, and Lucia Reisch. 2017. The Economics of Nudge. Routledge.\n\n\nSzaszi, Barnabas, Anthony Higney, Aaron Charlton, Andrew Gelman, Ignazio Ziano, Balazs Aczel, Daniel Goldstein, David Yeager, and Elizabeth Tipton. 2022. “No Reason to Expect Large and Consistent Effects of Nudge Interventions.” Proceedings of the National Academy of Sciences 119 (31): e2200732119. https://doi.org/10.1073/pnas.2200732119.\n\n\nTrisovic, Ana, Matthew Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.\n\n\nUshey, Kevin. 2022. renv: Project Environments. https://CRAN.R-project.org/package=renv.\n\n\nVidoni, Melina. 2021. “Evaluating Unit Testing Practices in R Packages.” In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 1523–34. https://doi.org/10.1109/ICSE43902.2021.00136.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWickham, Hadley. 2021. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2019. “TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "lectures/lecture-24-slides.html#coming-soon",
    "href": "lectures/lecture-24-slides.html#coming-soon",
    "title": "Generalized Linear Models (GLMs)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-24-slides.html#references",
    "href": "lectures/lecture-24-slides.html#references",
    "title": "Generalized Linear Models (GLMs)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-24-slides.html#references-1",
    "href": "lectures/lecture-24-slides.html#references-1",
    "title": "Generalized Linear Models (GLMs)",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#coming-soon",
    "href": "lectures/lecture-22-slides.html#coming-soon",
    "title": "Linear Models",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#references",
    "href": "lectures/lecture-22-slides.html#references",
    "title": "Linear Models",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#references-1",
    "href": "lectures/lecture-22-slides.html#references-1",
    "title": "Linear Models",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#coming-soon",
    "href": "lectures/lecture-20-slides.html#coming-soon",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#references",
    "href": "lectures/lecture-20-slides.html#references",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#references-1",
    "href": "lectures/lecture-20-slides.html#references-1",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#coming-soon",
    "href": "lectures/lecture-18-slides.html#coming-soon",
    "title": "Cleaning, Preparing, and Testing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#references",
    "href": "lectures/lecture-18-slides.html#references",
    "title": "Cleaning, Preparing, and Testing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#references-1",
    "href": "lectures/lecture-18-slides.html#references-1",
    "title": "Cleaning, Preparing, and Testing",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#coming-soon",
    "href": "lectures/lecture-16-slides.html#coming-soon",
    "title": "Experiments and Surveys",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#references",
    "href": "lectures/lecture-16-slides.html#references",
    "title": "Experiments and Surveys",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#references-1",
    "href": "lectures/lecture-16-slides.html#references-1",
    "title": "Experiments and Surveys",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#coming-soon",
    "href": "lectures/lecture-14-slides.html#coming-soon",
    "title": "APIs, Scraping, and Parsing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#references",
    "href": "lectures/lecture-14-slides.html#references",
    "title": "APIs, Scraping, and Parsing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#references-1",
    "href": "lectures/lecture-14-slides.html#references-1",
    "title": "APIs, Scraping, and Parsing",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#coming-soon",
    "href": "lectures/lecture-12-slides.html#coming-soon",
    "title": "Measurement, Censuses, and Sampling",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#references",
    "href": "lectures/lecture-12-slides.html#references",
    "title": "Measurement, Censuses, and Sampling",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#references-1",
    "href": "lectures/lecture-12-slides.html#references-1",
    "title": "Measurement, Censuses, and Sampling",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-10-slides.html#references",
    "href": "lectures/lecture-10-slides.html#references",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.1 References",
    "text": "1.1 References"
  },
  {
    "objectID": "lectures/lecture-08-slides.html#references",
    "href": "lectures/lecture-08-slides.html#references",
    "title": "Writing and Developing Research Questions",
    "section": "1.1 References",
    "text": "1.1 References"
  },
  {
    "objectID": "lectures/lecture-06-slides.html#references",
    "href": "lectures/lecture-06-slides.html#references",
    "title": "Reproducible Research with R and RStudio",
    "section": "1.1 References",
    "text": "1.1 References"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-slides.html#welcome-to-soci-3040",
    "title": "Introduction + Telling Stories with Data",
    "section": "👋 Welcome to SOCI 3040!",
    "text": "👋 Welcome to SOCI 3040!\nThis is an introduction the knowledge and skills you need to tell credible stories with quantitative data…\n\nTODO: Add intro slides and overview\nThey read the chapter for Thursday, but Thursday will be intro to R in lab session"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#why-data-storytelling-matters",
    "href": "lectures/lecture-01-slides.html#why-data-storytelling-matters",
    "title": "Introduction + Telling Stories with Data",
    "section": "Why Data Storytelling Matters",
    "text": "Why Data Storytelling Matters\n\n“A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.” (Alexander 2023)\n\n\nCore foundation of quantitative research methods\nBridge between analysis and understanding\nEssential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#common-concerns",
    "href": "lectures/lecture-01-slides.html#common-concerns",
    "title": "Introduction + Telling Stories with Data",
    "section": "Common Concerns",
    "text": "Common Concerns\nFive Key Questions for Data Stories\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What’s missing?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data?\n\n\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components",
    "href": "lectures/lecture-01-slides.html#core-workflow-components",
    "title": "Introduction + Telling Stories with Data",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\n\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) workflow for telling stories with data.\n\n\n\n\n\n\nPlan and sketch endpoint\nSimulate and consider data\nAcquire and prepare data\nExplore and understand data\nShare findings"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#plan-and-sketch",
    "href": "lectures/lecture-01-slides.html#plan-and-sketch",
    "title": "Introduction + Telling Stories with Data",
    "section": " Plan and Sketch",
    "text": "Plan and Sketch\n\n\n\ndeliberate, reasoned decisions\npurposeful adjustments\neven 10 minutes of planning is valuable\n\n\n\n\n\nThink of Alice’s conversation with the Cheshire Cat 😸. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.\n\n\n\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice’s conversation with the Cheshire Cat in Alice’s Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn’t require extensive time—often just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#simulate-data",
    "href": "lectures/lecture-01-slides.html#simulate-data",
    "title": "Introduction + Telling Stories with Data",
    "section": " Simulate Data",
    "text": "Simulate Data\n\nForces detailed thinking\nClarifies expected data structure and distributions.\nHelps with cleaning and preparation\nIdentifies potential issues beforehand.\nProvides clear testing framework\nEnsures data meets expectations.\n“Almost free” with modern computing\nProvides “an intimate feeling for the situation” (Hamming [1997] 2020)\n\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides “an intimate feeling for the situation” and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#acquire-and-prepare",
    "href": "lectures/lecture-01-slides.html#acquire-and-prepare",
    "title": "Introduction + Telling Stories with Data",
    "section": " Acquire and Prepare",
    "text": "Acquire and Prepare\n\nOften overlooked but crucial stage\nMany difficult decisions required: data sources, formats, permissions.\nCan significantly affect statistical results (Huntington-Klein et al. 2021)\nCommon challenges: quantity (too little or too much data) and quality\n\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed—either by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#explore-and-understand",
    "href": "lectures/lecture-01-slides.html#explore-and-understand",
    "title": "Introduction + Telling Stories with Data",
    "section": " Explore and Understand",
    "text": "Explore and Understand\n\nBegin with descriptive statistics\nMove to statistical models\nRemember: Models are tools, not truth\nModels reflect:\n\nEarlier decisions\nData acquisition choices\nCleaning procedures\n\n\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It’s important to remember that statistical models are tools—not absolute truths—and they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#share-findings",
    "href": "lectures/lecture-01-slides.html#share-findings",
    "title": "Introduction + Telling Stories with Data",
    "section": " Share Findings",
    "text": "Share Findings\n\nHigh-fidelity communication essential\nDocument all decisions\nBuild credibility through transparency\n\nInclude:\n\nWhat was done\nWhy it was done\nWhat was found\nWeaknesses of the approach\n\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it’s essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process—from data acquisition to analysis—builds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#the-foundation",
    "href": "lectures/lecture-01-slides.html#the-foundation",
    "title": "Introduction + Telling Stories with Data",
    "section": "The Foundation",
    "text": "The Foundation\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#communication-most-important",
    "href": "lectures/lecture-01-slides.html#communication-most-important",
    "title": "Introduction + Telling Stories with Data",
    "section": " Communication (Most Important)",
    "text": "Communication (Most Important)\n\n“Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly.” (Alexander 2023)\n\n\n“One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.” (Alexander 2023)\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience’s perspective"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#reproducibility",
    "href": "lectures/lecture-01-slides.html#reproducibility",
    "title": "Introduction + Telling Stories with Data",
    "section": " Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\nRequirements:\n\nOpen access to code\nData availability or simulation\nAutomated testing\nClear documentation\nAim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#ethics",
    "href": "lectures/lecture-01-slides.html#ethics",
    "title": "Introduction + Telling Stories with Data",
    "section": " Ethics",
    "text": "Ethics\n\n“This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?” (Alexander 2023)\n\n\nConsider the full context of the dataset (D’Ignazio and Klein 2020)\nAcknowledge the social, cultural, and political forces (Crawford 2021)\nUse data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#questions",
    "href": "lectures/lecture-01-slides.html#questions",
    "title": "Introduction + Telling Stories with Data",
    "section": " Questions",
    "text": "Questions\n\nQuestions evolve through understanding\nChallenge of operationalizing variables\nCuriosity is essential, drives deeper exploration\nValue of “hybrid” knowledge that combines multiple disciplines\nComfort with asking “dumb” questions\n\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a “hybrid”—cultivating knowledge across various disciplines—and being comfortable with asking seemingly simple or “dumb” questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#measurement",
    "href": "lectures/lecture-01-slides.html#measurement",
    "title": "Introduction + Telling Stories with Data",
    "section": " Measurement",
    "text": "Measurement\n\n“The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.” (Alexander 2023)\n\n\n\nMeasuring even simple things is challenging\nExample: Measuring height\n\nShoes on or off?\nTime of day affects height.\nDifferent tools yield different results.\n\nMore complex measurements are even harder. How do we measure happiness or pain?\n\nMeasurement requires decisions and is not value-free\nContext and purpose guide all measurement choices\n\n\n\n\n\nPicasso’s dog and the challenges of reduction.\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person’s height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso’s minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we’re measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#data-collection-cleaning",
    "href": "lectures/lecture-01-slides.html#data-collection-cleaning",
    "title": "Introduction + Telling Stories with Data",
    "section": " &  Data Collection & Cleaning",
    "text": "&  Data Collection & Cleaning\n\n“Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.” (Alexander 2023)\n\n\nCollection determines possibilities\n\nWhat and how we measure matters.\n\nCleaning requires many decisions\n\nExample: Survey responses on gender\nOptions: “man”, “woman”, “prefer not to say”, “other”\nHandling “prefer not to say” and open-text responses.\n\nDocument every step\n\nEnsures transparency and reproducibility.\n\nConsider implications of choices\n\nEthical considerations and representation.\n\n\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like “man,” “woman,” “prefer not to say,” and “other” (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude “prefer not to say” responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents’ specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it’s vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#eda-modeling-scaling",
    "href": "lectures/lecture-01-slides.html#eda-modeling-scaling",
    "title": "Introduction + Telling Stories with Data",
    "section": "+ EDA, Modeling, & Scaling",
    "text": "+ EDA, Modeling, & Scaling\nExploratory Data Analysis (EDA)\n\nIterative process\nNever truly complete\nShapes understanding\n\nModeling\n\nTool for understanding\nNot a recipe to follow\nJust one representation of reality\nStatistical significance \\(\\neq\\) scientific significance\nStatistical models help us explore the shape of the data; are like echolocation\n\nScaling\n\nUsing programming languages like R and Python\n\nHandle large datasets efficiently\nAutomate repetitive tasks\nShare work widely and quickly\n\nOutputs can reach many people easily\nAPIs can make analyses accessible in real-time\n\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there’s always more to explore. Although it’s challenging to delineate where EDA ends and formal statistical modeling begins—since our beliefs and understanding evolve continuously—EDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it’s crucial that all steps taken during EDA are recorded and shared.\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it’s a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference—the process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it’s essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly—whether it’s one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data",
    "title": "Introduction + Telling Stories with Data",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\n“There is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.” (Hamming [1997] 2020, 177)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction 👨‍🍳\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data—plausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it’s crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment—much like a chef monitoring a reduction to achieve the desired consistency without overcooking—to determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors—from selecting what to measure to deciding on the methods and accuracy required. Advances in instruments—from telescopes in astronomy to real-time internet data collection—have expanded our ability to gather data, much like new culinary techniques enhance a chef’s ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our “reduction” process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#what-is-data-science",
    "href": "lectures/lecture-01-slides.html#what-is-data-science",
    "title": "Introduction + Telling Stories with Data",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n“Data science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict.” (Alexander 2023)\n\nKey Principles\n\nData are generated, and must be gathered, cleaned, and prepared\nThese decisions matter\nThe process will be difficult\nDevelop resilience and intrinsic motivation"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#the-power-of-multiple-perspectives",
    "href": "lectures/lecture-01-slides.html#the-power-of-multiple-perspectives",
    "title": "Introduction + Telling Stories with Data",
    "section": "The Power of Multiple Perspectives",
    "text": "The Power of Multiple Perspectives\n\n“The strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past.” (Alexander 2023)\n\n\nData science is multi-disciplinary\nCombines statistics, software engineering, subject-matter expertise, and more.\nDiversity enhances understanding\nDifferent perspectives lead to better questions and solutions.\nCollaboration is key\nRespect and integrate insights from various fields."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#embracing-the-challenge",
    "href": "lectures/lecture-01-slides.html#embracing-the-challenge",
    "title": "Introduction + Telling Stories with Data",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\nOur world is messy, and so are our data. Telling stories with data is difficult but rewarding.\n\nDevelop resilience and intrinsic motivation\nAccept that failure is part of the process.\nConsider possibilities and probabilities\nLearn to make trade-offs.\nNo perfect analysis exists\nAim for transparency and continuous improvement.\n\n\n“Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.” (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#key-takeaways",
    "href": "lectures/lecture-01-slides.html#key-takeaways",
    "title": "Introduction + Telling Stories with Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#references",
    "href": "lectures/lecture-01-slides.html#references",
    "title": "Introduction + Telling Stories with Data",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992."
  },
  {
    "objectID": "lectures/inputs/quarto_example-figures_crossreferences.html",
    "href": "lectures/inputs/quarto_example-figures_crossreferences.html",
    "title": "🔥 Quantitative Research Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n\n\n\n\n\n\n\nFigure 1: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey"
  },
  {
    "objectID": "deliverables/assignment-3.html",
    "href": "deliverables/assignment-3.html",
    "title": " Data Stories 3",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Praesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 3"
    ]
  },
  {
    "objectID": "deliverables/assignment-3.html#instructions",
    "href": "deliverables/assignment-3.html#instructions",
    "title": " Data Stories 3",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Praesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 3"
    ]
  },
  {
    "objectID": "deliverables/assignment-3.html#grading-rubric",
    "href": "deliverables/assignment-3.html#grading-rubric",
    "title": " Data Stories 3",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)\n\nComing soon… Nullam dapibus cursus dolor sit amet consequat. Nulla facilisi. Curabitur vel nulla non magna lacinia tincidunt. Duis porttitor quam leo, et blandit velit efficitur ut. Etiam auctor tincidunt porttitor. Phasellus sed accumsan mi. Fusce ut erat dui. Suspendisse eu augue eget turpis condimentum finibus eu non lorem. Donec finibus eros eu ante condimentum, sed pharetra sapien sagittis. Phasellus non dolor ac ante mollis auctor nec et sapien. Pellentesque vulputate at nisi eu tincidunt. Vestibulum at dolor aliquam, hendrerit purus eu, eleifend massa. Morbi consectetur eros id tincidunt gravida. Fusce ut enim quis orci hendrerit lacinia sed vitae enim.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 3"
    ]
  },
  {
    "objectID": "deliverables/assignment-3.html#student-work",
    "href": "deliverables/assignment-3.html#student-work",
    "title": " Data Stories 3",
    "section": "Student Work",
    "text": "Student Work\nComing soon… Etiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\n\n\n\n\n\n\n\n\nThis is a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\nFigure 2: A selection of visualizations from the first data stories assignment. Click an image to open a lightbox gallery.\n\n\n\nComing soon… Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 3"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html",
    "href": "deliverables/assignment-1.html",
    "title": " Data Stories 1",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 1"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html#instructions",
    "href": "deliverables/assignment-1.html#instructions",
    "title": " Data Stories 1",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 1"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html#grading-rubric",
    "href": "deliverables/assignment-1.html#grading-rubric",
    "title": " Data Stories 1",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)\n\nComing soon… Etiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 1"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html#student-work",
    "href": "deliverables/assignment-1.html#student-work",
    "title": " Data Stories 1",
    "section": "Student Work",
    "text": "Student Work\nComing soon… Ut ut condimentum augue, nec eleifend nisl. Sed facilisis egestas odio ac pretium. Pellentesque consequat magna sed venenatis sagittis. Vivamus feugiat lobortis magna vitae accumsan. Pellentesque euismod malesuada hendrerit. Ut non mauris non arcu condimentum sodales vitae vitae dolor. Nullam dapibus, velit eget lacinia rutrum, ipsum justo malesuada odio, et lobortis sapien magna vel lacus. Nulla purus neque, hendrerit non malesuada eget, mattis vel erat. Suspendisse potenti.\n\n\n\n\n\n\n\n\nThis is a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\nFigure 2: A selection of visualizations from the first data stories assignment. Click an image to open a lightbox gallery.\n\n\n\nComing soon… Nullam dapibus cursus dolor sit amet consequat. Nulla facilisi. Curabitur vel nulla non magna lacinia tincidunt. Duis porttitor quam leo, et blandit velit efficitur ut. Etiam auctor tincidunt porttitor. Phasellus sed accumsan mi. Fusce ut erat dui. Suspendisse eu augue eget turpis condimentum finibus eu non lorem. Donec finibus eros eu ante condimentum, sed pharetra sapien sagittis. Phasellus non dolor ac ante mollis auctor nec et sapien. Pellentesque vulputate at nisi eu tincidunt. Vestibulum at dolor aliquam, hendrerit purus eu, eleifend massa. Morbi consectetur eros id tincidunt gravida. Fusce ut enim quis orci hendrerit lacinia sed vitae enim.\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 1"
    ]
  },
  {
    "objectID": "lectures/lecture-25-content.html#references",
    "href": "lectures/lecture-25-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-23-content.html#references",
    "href": "lectures/lecture-23-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-21-content.html#references",
    "href": "lectures/lecture-21-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-19-content.html#references",
    "href": "lectures/lecture-19-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-17-content.html#references",
    "href": "lectures/lecture-17-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-15-content.html#references",
    "href": "lectures/lecture-15-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-13-content.html#references",
    "href": "lectures/lecture-13-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-11-content.html#references",
    "href": "lectures/lecture-11-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-06-content.html",
    "href": "lectures/lecture-06-content.html",
    "title": "Content Plan",
    "section": "",
    "text": "Content Plan\n1/2 of the content from Lecture 05. Split the content and add the lab as you are working on that content."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-4.html",
    "href": "deliverables/submissions/assignment-4/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut.\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-2.html",
    "href": "deliverables/submissions/assignment-4/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut.\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-5.html",
    "href": "deliverables/submissions/assignment-3/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nNulla eget cursus ipsum. Vivamus porttitor leo diam, sed volutpat lectus facilisis sit amet. Maecenas et pulvinar metus. Ut at dignissim tellus. In in tincidunt elit. Etiam vulputate lobortis arcu, vel faucibus leo lobortis ac. Aliquam erat volutpat. In interdum orci ac est euismod euismod. Nunc eleifend tristique risus, at lacinia odio commodo in. Sed aliquet ligula odio, sed tempor neque ultricies sit amet.\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-3.html",
    "href": "deliverables/submissions/assignment-3/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-1.html",
    "href": "deliverables/submissions/assignment-3/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit.\nMaecenas turpis velit, ultricies non elementum vel, luctus nec nunc. Nulla a diam interdum, faucibus sapien viverra, finibus metus. Donec non tortor diam. In ut elit aliquet, bibendum sem et, aliquam tortor. Donec congue, sem at rhoncus ultrices, nunc augue cursus erat, quis porttitor mauris libero ut ex. Nullam quis leo urna. Donec faucibus ligula eget pellentesque interdum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rhoncus interdum erat ut ultricies. Aenean tempus ex non elit suscipit, quis dignissim enim efficitur. Proin laoreet enim massa, vitae laoreet nulla mollis quis."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-4.html",
    "href": "deliverables/submissions/assignment-2/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit.\nMaecenas turpis velit, ultricies non elementum vel, luctus nec nunc. Nulla a diam interdum, faucibus sapien viverra, finibus metus. Donec non tortor diam. In ut elit aliquet, bibendum sem et, aliquam tortor. Donec congue, sem at rhoncus ultrices, nunc augue cursus erat, quis porttitor mauris libero ut ex. Nullam quis leo urna. Donec faucibus ligula eget pellentesque interdum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rhoncus interdum erat ut ultricies. Aenean tempus ex non elit suscipit, quis dignissim enim efficitur. Proin laoreet enim massa, vitae laoreet nulla mollis quis."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-2.html",
    "href": "deliverables/submissions/assignment-2/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nVestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-5.html",
    "href": "deliverables/submissions/assignment-1/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-3.html",
    "href": "deliverables/submissions/assignment-1/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-1.html",
    "href": "deliverables/submissions/assignment-1/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit.\nMaecenas turpis velit, ultricies non elementum vel, luctus nec nunc. Nulla a diam interdum, faucibus sapien viverra, finibus metus. Donec non tortor diam. In ut elit aliquet, bibendum sem et, aliquam tortor. Donec congue, sem at rhoncus ultrices, nunc augue cursus erat, quis porttitor mauris libero ut ex. Nullam quis leo urna. Donec faucibus ligula eget pellentesque interdum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rhoncus interdum erat ut ultricies. Aenean tempus ex non elit suscipit, quis dignissim enim efficitur. Proin laoreet enim massa, vitae laoreet nulla mollis quis."
  },
  {
    "objectID": "syllabus/policies.html#department-and-faculty-policies",
    "href": "syllabus/policies.html#department-and-faculty-policies",
    "title": " Policies",
    "section": "Department and Faculty Policies",
    "text": "Department and Faculty Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/policies.html#university-policies",
    "href": "syllabus/policies.html#university-policies",
    "title": " Policies",
    "section": "University Policies",
    "text": "University Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/computing.html",
    "href": "syllabus/computing.html",
    "title": " Computing",
    "section": "",
    "text": "R and Quarto 😁 – No Experience Necessary  You will learn to use R (a free/open source programming language and environment for statistical computing and graphics) and Quarto (a free/open source publishing system for creating dynamic and reproducible manuscripts, reports, websites, and presentations) in this course. Note that I assume no prior experience with R or Quarto. Everything you need to know about both will be introduced in the course. While some prior programming experience is an asset, it is by no means necessary.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#cp-2003",
    "href": "syllabus/computing.html#cp-2003",
    "title": " Computing",
    "section": "CP-2003",
    "text": "CP-2003",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#remote-access",
    "href": "syllabus/computing.html#remote-access",
    "title": " Computing",
    "section": "Remote Access",
    "text": "Remote Access",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#macos",
    "href": "syllabus/computing.html#macos",
    "title": " Computing",
    "section": "macOS",
    "text": "macOS",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#windows",
    "href": "syllabus/computing.html#windows",
    "title": " Computing",
    "section": "Windows",
    "text": "Windows",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#linux",
    "href": "syllabus/computing.html#linux",
    "title": " Computing",
    "section": "Linux",
    "text": "Linux",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#github-codespaces",
    "href": "syllabus/computing.html#github-codespaces",
    "title": " Computing",
    "section": "GitHub CodeSpaces",
    "text": "GitHub CodeSpaces",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#posit-cloud",
    "href": "syllabus/computing.html#posit-cloud",
    "title": " Computing",
    "section": "Posit Cloud",
    "text": "Posit Cloud",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "deliverables/assignment-5.html",
    "href": "deliverables/assignment-5.html",
    "title": " Final Exam",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "lectures/lecture-02-content.html",
    "href": "lectures/lecture-02-content.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "TODO: Introduction to R\nEither KH and RA would be ideal, Tidyverse first"
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan",
    "href": "lectures/lecture-02-content.html#plan",
    "title": "R Fundamentals",
    "section": "",
    "text": "TODO: Introduction to R\nEither KH and RA would be ideal, Tidyverse first"
  },
  {
    "objectID": "lectures/inputs/quarto_example-equations_crossreferences.html",
    "href": "lectures/inputs/quarto_example-equations_crossreferences.html",
    "title": "🔥 Quantitative Research Methods",
    "section": "",
    "text": "\\[\nY = C + I + G + (X - M)\n\\tag{1}\\]"
  },
  {
    "objectID": "lectures/lecture-10-content.html",
    "href": "lectures/lecture-10-content.html",
    "title": "Content Plan",
    "section": "",
    "text": "Content Plan\n1/2 of the content in Lecture 09. Maybe some stuff from KH."
  },
  {
    "objectID": "lectures/list-lectures.html",
    "href": "lectures/list-lectures.html",
    "title": " Class & Lab Notes",
    "section": "",
    "text": "Introduction + Telling Stories with Data\n\n\n\nTuesday, January 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTelling Stories with Data + R and the Tidyverse\n\n\n\nThursday, January 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDrinking from the Firehose – Data Analysis Workflow\n\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDrinking from the Firehose – Data Analysis Workflow\n\n\n\nThursday, January 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research with R and RStudio\n\n\n\nTuesday, January 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research with R and RStudio\n\n\n\nThursday, January 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWriting and Developing Research Questions\n\n\n\nTuesday, January 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWriting and Developing Research Questions\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Graphs, Tables, & Maps\n\n\n\nTuesday, February 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Graphs, Tables, & Maps\n\n\n\nThursday, February 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement, Censuses, and Sampling\n\n\n\nTuesday, February 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement, Censuses, and Sampling\n\n\n\nThursday, February 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs, Scraping, and Parsing\n\n\n\nTuesday, February 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs, Scraping, and Parsing\n\n\n\nThursday, February 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments and Surveys\n\n\n\nTuesday, March 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments and Surveys\n\n\n\nThursday, March 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning, Preparing, and Testing\n\n\n\nTuesday, March 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning, Preparing, and Testing\n\n\n\nThursday, March 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nTuesday, March 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nThursday, March 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models\n\n\n\nTuesday, March 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models\n\n\n\nThursday, March 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models (GLMs)\n\n\n\nTuesday, April 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models (GLMs)\n\n\n\nThursday, April 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProject Work\n\n\n\nTuesday, April 8, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus/schedule.html",
    "href": "syllabus/schedule.html",
    "title": " Course Schedule",
    "section": "",
    "text": "No.\n\n\nClass Date\n\n\nClass & Lab Notes + Slides\n\n\n Required\n\n\n Recommended\n\n\nDue by 5:00 pm\n\n\n\n\n\n\n01\n\n\n1/7/25\n\n\nIntroduction + Telling Stories with Data\n\n\nBrowse this site\n\n\n \n\n\n \n\n\n\n\n02\n\n\n1/9/25\n\n\nTelling Stories with Data + R and the Tidyverse\n\n\n RA Ch 1\n\n\n \n\n\n \n\n\n\n\n03\n\n\n1/14/25\n\n\nDrinking from the Firehose – Data Analysis Workflow\n\n\n RA Ch 2\n\n\n KH Ch 1\n\n\n \n\n\n\n\n04\n\n\n1/16/25\n\n\nDrinking from the Firehose – Data Analysis Workflow\n\n\n RA Ch 2\n\n\n KH Ch 1\n\n\n \n\n\n\n\n05\n\n\n1/21/25\n\n\nReproducible Research with R and RStudio\n\n\n RA CH 3\n\n\n KH Ch 2\n\n\n \n\n\n\n\n06\n\n\n1/23/25\n\n\nReproducible Research with R and RStudio\n\n\n RA CH 3\n\n\n KH Ch 2\n\n\nData Stories 1\n\n\n\n\n07\n\n\n1/28/25\n\n\nWriting and Developing Research Questions\n\n\n\n\n\n \n\n\n \n\n\n\n\n08\n\n\n1/30/25\n\n\nWriting and Developing Research Questions\n\n\n\n\n\n \n\n\n \n\n\n\n\n09\n\n\n2/4/25\n\n\nCreating Graphs, Tables, & Maps\n\n\n\n\n\n + \n\n\n \n\n\n\n\n10\n\n\n2/6/25\n\n\nCreating Graphs, Tables, & Maps\n\n\n\n\n\n) + \n\n\n \n\n\n\n\n11\n\n\n2/11/25\n\n\nMeasurement, Censuses, and Sampling\n\n\n\n\n\n \n\n\n \n\n\n\n\n12\n\n\n2/13/25\n\n\nMeasurement, Censuses, and Sampling\n\n\n\n\n\n \n\n\n \n\n\n\n\n13\n\n\n2/18/25\n\n\nAPIs, Scraping, and Parsing\n\n\n[](https://tellingstorieswithdata.com/07-gather.html\n\n\n \n\n\n \n\n\n\n\n14\n\n\n2/20/25\n\n\nAPIs, Scraping, and Parsing\n\n\n\n\n\n \n\n\nData Stories 2\n\n\n\n\n15\n\n\n3/4/25\n\n\nExperiments and Surveys\n\n\n\n\n\n \n\n\n \n\n\n\n\n16\n\n\n3/6/25\n\n\nExperiments and Surveys\n\n\n\n\n\n \n\n\n \n\n\n\n\n17\n\n\n3/11/25\n\n\nCleaning, Preparing, and Testing\n\n\n RA Ch 9\n\n\n \n\n\n \n\n\n\n\n18\n\n\n3/13/25\n\n\nCleaning, Preparing, and Testing\n\n\n RA Ch 9\n\n\n \n\n\n \n\n\n\n\n19\n\n\n3/18/25\n\n\nExploratory Data Analysis (EDA)\n\n\n RA Ch 11\n\n\n \n\n\n \n\n\n\n\n20\n\n\n3/20/25\n\n\nExploratory Data Analysis (EDA)\n\n\n RA Ch 11\n\n\n \n\n\nData Stories 3\n\n\n\n\n21\n\n\n3/25/25\n\n\nLinear Models\n\n\n RA Ch 12\n\n\n \n\n\n \n\n\n\n\n22\n\n\n3/27/25\n\n\nLinear Models\n\n\n RA Ch 12\n\n\n \n\n\n \n\n\n\n\n23\n\n\n4/1/25\n\n\nGeneralized Linear Models (GLMs)\n\n\n\n\n\n \n\n\n \n\n\n\n\n24\n\n\n4/3/25\n\n\nGeneralized Linear Models (GLMs)\n\n\n\n\n\n \n\n\n \n\n\n\n\n25\n\n\n4/8/25\n\n\nProject Work\n\n\n \n\n\n \n\n\nData Stories 4\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Course Schedule"
    ]
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-2.html",
    "href": "deliverables/submissions/assignment-1/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit.\nMaecenas turpis velit, ultricies non elementum vel, luctus nec nunc. Nulla a diam interdum, faucibus sapien viverra, finibus metus. Donec non tortor diam. In ut elit aliquet, bibendum sem et, aliquam tortor. Donec congue, sem at rhoncus ultrices, nunc augue cursus erat, quis porttitor mauris libero ut ex. Nullam quis leo urna. Donec faucibus ligula eget pellentesque interdum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rhoncus interdum erat ut ultricies. Aenean tempus ex non elit suscipit, quis dignissim enim efficitur. Proin laoreet enim massa, vitae laoreet nulla mollis quis."
  },
  {
    "objectID": "deliverables/submissions/assignment-1/example-submission-4.html",
    "href": "deliverables/submissions/assignment-1/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-1.html",
    "href": "deliverables/submissions/assignment-2/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nVestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-3.html",
    "href": "deliverables/submissions/assignment-2/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nVestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "deliverables/submissions/assignment-2/example-submission-5.html",
    "href": "deliverables/submissions/assignment-2/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit.\nMaecenas turpis velit, ultricies non elementum vel, luctus nec nunc. Nulla a diam interdum, faucibus sapien viverra, finibus metus. Donec non tortor diam. In ut elit aliquet, bibendum sem et, aliquam tortor. Donec congue, sem at rhoncus ultrices, nunc augue cursus erat, quis porttitor mauris libero ut ex. Nullam quis leo urna. Donec faucibus ligula eget pellentesque interdum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rhoncus interdum erat ut ultricies. Aenean tempus ex non elit suscipit, quis dignissim enim efficitur. Proin laoreet enim massa, vitae laoreet nulla mollis quis."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-2.html",
    "href": "deliverables/submissions/assignment-3/example-submission-2.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\nDuis urna urna, pellentesque eu urna ut, malesuada bibendum dolor. Suspendisse potenti. Vivamus ornare, arcu quis molestie ultrices, magna est accumsan augue, auctor vulputate erat quam quis neque. Nullam scelerisque odio vel ultricies facilisis. Ut porta arcu non magna sagittis lacinia. Cras ornare vulputate lectus a tristique. Pellentesque ac arcu congue, rhoncus mi id, dignissim ligula."
  },
  {
    "objectID": "deliverables/submissions/assignment-3/example-submission-4.html",
    "href": "deliverables/submissions/assignment-3/example-submission-4.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nNulla eget cursus ipsum. Vivamus porttitor leo diam, sed volutpat lectus facilisis sit amet. Maecenas et pulvinar metus. Ut at dignissim tellus. In in tincidunt elit. Etiam vulputate lobortis arcu, vel faucibus leo lobortis ac. Aliquam erat volutpat. In interdum orci ac est euismod euismod. Nunc eleifend tristique risus, at lacinia odio commodo in. Sed aliquet ligula odio, sed tempor neque ultricies sit amet.\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-1.html",
    "href": "deliverables/submissions/assignment-4/example-submission-1.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nNulla eget cursus ipsum. Vivamus porttitor leo diam, sed volutpat lectus facilisis sit amet. Maecenas et pulvinar metus. Ut at dignissim tellus. In in tincidunt elit. Etiam vulputate lobortis arcu, vel faucibus leo lobortis ac. Aliquam erat volutpat. In interdum orci ac est euismod euismod. Nunc eleifend tristique risus, at lacinia odio commodo in. Sed aliquet ligula odio, sed tempor neque ultricies sit amet.\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-3.html",
    "href": "deliverables/submissions/assignment-4/example-submission-3.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\n\nEtiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut.\nEtiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos."
  },
  {
    "objectID": "deliverables/submissions/assignment-4/example-submission-5.html",
    "href": "deliverables/submissions/assignment-4/example-submission-5.html",
    "title": "This is an Example Submission",
    "section": "",
    "text": "Example Submission\nNulla eget cursus ipsum. Vivamus porttitor leo diam, sed volutpat lectus facilisis sit amet. Maecenas et pulvinar metus. Ut at dignissim tellus. In in tincidunt elit. Etiam vulputate lobortis arcu, vel faucibus leo lobortis ac. Aliquam erat volutpat. In interdum orci ac est euismod euismod. Nunc eleifend tristique risus, at lacinia odio commodo in. Sed aliquet ligula odio, sed tempor neque ultricies sit amet.\nEtiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit."
  },
  {
    "objectID": "lectures/lecture-08-content.html",
    "href": "lectures/lecture-08-content.html",
    "title": "DAGs?",
    "section": "",
    "text": "DAGs?\nRohan brings them up in this chapter but doesn’t get into it. Maybe that;s best?"
  },
  {
    "objectID": "lectures/lecture-12-content.html#references",
    "href": "lectures/lecture-12-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-14-content.html#references",
    "href": "lectures/lecture-14-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-16-content.html#references",
    "href": "lectures/lecture-16-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-18-content.html#references",
    "href": "lectures/lecture-18-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-20-content.html#references",
    "href": "lectures/lecture-20-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-22-content.html#references",
    "href": "lectures/lecture-22-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-24-content.html#references",
    "href": "lectures/lecture-24-content.html#references",
    "title": "🔥 Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus/assessment.html",
    "href": "syllabus/assessment.html",
    "title": " Assessment",
    "section": "",
    "text": "Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\n\n\n\n\n\nDue (On or Before)\n\n\nBy\n\n\nAssignment\n\n\nGrade Weight\n\n\n\n\n\n\nTuesday, January 7, 2025\n\n\n11:59 pm\n\n\n Data Stories 1\n\n\n10%\n\n\n\n\nWednesday, January 8, 2025\n\n\n11:59 pm\n\n\n Data Stories 2\n\n\n15%\n\n\n\n\nThursday, January 9, 2025\n\n\n11:59 pm\n\n\n Data Stories 3\n\n\n25%\n\n\n\n\nFriday, January 10, 2025\n\n\n11:59 pm\n\n\n Data Stories 4\n\n\n25%\n\n\n\n\nFriday, January 10, 2025\n\n\n11:59 pm\n\n\n Final Exam\n\n\n25%\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Assessment"
    ]
  },
  {
    "objectID": "deliverables/assignment-2.html",
    "href": "deliverables/assignment-2.html",
    "title": " Data Stories 2",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Praesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 2"
    ]
  },
  {
    "objectID": "deliverables/assignment-2.html#instructions",
    "href": "deliverables/assignment-2.html#instructions",
    "title": " Data Stories 2",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Praesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 2"
    ]
  },
  {
    "objectID": "deliverables/assignment-2.html#grading-rubric",
    "href": "deliverables/assignment-2.html#grading-rubric",
    "title": " Data Stories 2",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)\n\nComing soon… Nullam dapibus cursus dolor sit amet consequat. Nulla facilisi. Curabitur vel nulla non magna lacinia tincidunt. Duis porttitor quam leo, et blandit velit efficitur ut. Etiam auctor tincidunt porttitor. Phasellus sed accumsan mi. Fusce ut erat dui. Suspendisse eu augue eget turpis condimentum finibus eu non lorem. Donec finibus eros eu ante condimentum, sed pharetra sapien sagittis. Phasellus non dolor ac ante mollis auctor nec et sapien. Pellentesque vulputate at nisi eu tincidunt. Vestibulum at dolor aliquam, hendrerit purus eu, eleifend massa. Morbi consectetur eros id tincidunt gravida. Fusce ut enim quis orci hendrerit lacinia sed vitae enim.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 2"
    ]
  },
  {
    "objectID": "deliverables/assignment-2.html#student-work",
    "href": "deliverables/assignment-2.html#student-work",
    "title": " Data Stories 2",
    "section": "Student Work",
    "text": "Student Work\nComing soon… Etiam non efficitur urna, quis elementum nisi. Mauris posuere a augue vel gravida. Praesent luctus erat et ex iaculis interdum. Nulla vestibulum quam ac nunc consequat vulputate. Nullam iaculis lobortis sem sit amet fringilla. Aliquam semper, metus ut blandit semper, nulla velit fermentum sapien, fermentum ultrices dolor sapien sed leo. Vestibulum molestie faucibus magna, at feugiat nulla ullamcorper a. Aliquam erat volutpat. Praesent scelerisque magna a justo maximus, sit amet suscipit mauris tempor. Nulla nec dolor eget ipsum pellentesque lobortis a in ipsum. Morbi turpis turpis, fringilla a eleifend maximus, viverra nec neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.\n\n\n\n\n\n\n\n\nThis is a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\nFigure 2: A selection of visualizations from the first data stories assignment. Click an image to open a lightbox gallery.\n\n\n\nComing soon… Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories 2"
    ]
  },
  {
    "objectID": "deliverables/assignment-4.html",
    "href": "deliverables/assignment-4.html",
    "title": " Data Stories 4",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Vestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories  4"
    ]
  },
  {
    "objectID": "deliverables/assignment-4.html#instructions",
    "href": "deliverables/assignment-4.html#instructions",
    "title": " Data Stories 4",
    "section": "",
    "text": "See Figure 1 for the workflow diagram.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) Telling Stories with Data workflow\n\n\n\nComing soon… Vestibulum ultrices, tortor at mattis porta, odio nisi rutrum nulla, sit amet tincidunt eros quam facilisis tellus. Fusce eleifend lectus in elementum lacinia. Nam auctor nunc in massa ullamcorper, sit amet auctor ante accumsan. Nam ut varius metus. Curabitur eget tristique leo. Cras finibus euismod erat eget elementum. Integer vel placerat ex. Ut id eros quis lectus lacinia venenatis hendrerit vel ante.\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories  4"
    ]
  },
  {
    "objectID": "deliverables/assignment-4.html#grading-rubric",
    "href": "deliverables/assignment-4.html#grading-rubric",
    "title": " Data Stories 4",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\nPlan (XX%)\nSimulate (XX%)\nAcquire (XX%)\nExplore (XX%)\nShare (XX%)\n\nComing soon… Aenean placerat luctus tortor vitae molestie. Nulla at aliquet nulla. Sed efficitur tellus orci, sed fringilla lectus laoreet eget. Vivamus maximus quam sit amet arcu dignissim, sed accumsan massa ullamcorper. Sed iaculis tincidunt feugiat. Nulla in est at nunc ultricies dictum ut vitae nunc. Aenean convallis vel diam at malesuada. Suspendisse arcu libero, vehicula tempus ultrices a, placerat sit amet tortor. Sed dictum id nulla commodo mattis. Aliquam mollis, nunc eu tristique faucibus, purus lacus tincidunt nulla, ac pretium lorem nunc ut enim. Curabitur eget mattis nisl, vitae sodales augue. Nam felis massa, bibendum sit amet nulla vel, vulputate rutrum lacus. Aenean convallis odio pharetra nulla mattis consequat.",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories  4"
    ]
  },
  {
    "objectID": "deliverables/assignment-4.html#student-work",
    "href": "deliverables/assignment-4.html#student-work",
    "title": " Data Stories 4",
    "section": "Student Work",
    "text": "Student Work\nComing soon… Praesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.\n\n\n\n\n\n\n\n\nThis is a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\n\nThis is also a featured student assignment.\n\n\n\n\n\n\nFigure 2: A selection of visualizations from the first data stories assignment. Click an image to open a lightbox gallery.\n\n\n\nComing soon… Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an Example Submission\n\n\n\n\n\n\nStudent Name\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>ASSIGNMENTS</strong>",
      "&nbsp;&nbsp;Data Stories  4"
    ]
  },
  {
    "objectID": "lectures/inputs/quarto_example-tables_crossreferences.html",
    "href": "lectures/inputs/quarto_example-tables_crossreferences.html",
    "title": "🔥 Quantitative Research Methods",
    "section": "",
    "text": "Table 1: Number of visits to the doctor in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\nDoctorVisits |&gt;\n    count(visits) |&gt;\n    knitr::kable()"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan",
    "href": "lectures/lecture-02-slides.html#plan",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "2.1 Plan",
    "text": "2.1 Plan\n\nTODO: Introduction to R\nEither KH and RA would be ideal, Tidyverse first"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#references",
    "href": "lectures/lecture-02-slides.html#references",
    "title": "Telling Stories with Data + R and the Tidyverse",
    "section": "3.1 References",
    "text": "3.1 References"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#key-concepts-and-skills",
    "href": "lectures/lecture-07-slides.html#key-concepts-and-skills",
    "title": "Writing and Developing Research Questions",
    "section": "1.1 Key concepts and skills",
    "text": "1.1 Key concepts and skills\n\nWriting is a critical skill—perhaps the most important—of all the skills required to analyze data. The only way to get better at writing is to write, ideally every day.\nWhen we write, although the benefits typically accrue to ourselves, we must nonetheless write for the reader. This means having one main message that we want to communicate, and thinking about where they are, rather than where we are.\nWe want to get to a first draft as quickly as possible. Even if it is horrible, the difference between a first draft existing and not is enormous. At that point we start to rewrite. When doing so we aim to maximize clarity, often by removing unnecessary words.\nWe typically begin with some area of interest and then develop research questions, datasets, and analysis in an iterative way. Through this process we come to a better understanding of what we are doing."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#software-and-packages",
    "href": "lectures/lecture-07-slides.html#software-and-packages",
    "title": "Writing and Developing Research Questions",
    "section": "1.2 Software and packages",
    "text": "1.2 Software and packages\n\nknitr (Xie 2023)\ntidyverse (Wickham et al. 2019)\n\n\nlibrary(knitr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#writing-1",
    "href": "lectures/lecture-07-slides.html#writing-1",
    "title": "Writing and Developing Research Questions",
    "section": "2.1 Writing",
    "text": "2.1 Writing\n\nThe way to do a piece of writing is three or four times over, never once. For me, the hardest part comes first, getting something—anything—out in front of me. Sometimes in a nervous frenzy I just fling words as if I were flinging mud at a wall. Blurt out, heave out, babble out something—anything—as a first draft.\nMcPhee (2017, 159)\n\nThe process of writing is a process of rewriting. The critical task is to get to a first draft as quickly as possible. Until that complete first draft exists, it is useful to try to not to delete, or even revise, anything that was written, regardless of how bad it may seem. Just write. (This advice is directed at less-experienced writers. As you get more experience, you may find that your approach changes.)\nOne of the most intimidating stages is a blank page, and we deal with this by immediately adding headings such as: “Introduction”, “Data”, “Model”, “Results”, and “Discussion”. And then adding fields in the top matter for the various bits and pieces that are needed, such as “title”, “date”, “author”, and “abstract”. This creates a generic outline, which will play the role of mise en place for the paper. By way of background, mise en place is a preparatory phase in a professional kitchen when ingredients are sorted, prepared, and arranged for easy access. This ensures that everything that is needed is available without unnecessary delay. Putting together an outline plays the same role when writing quantitative papers, and is akin to placing on the counter, the ingredients that we will use to prepare dinner (McPhee 2017)."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section",
    "href": "lectures/lecture-07-slides.html#section",
    "title": "Writing and Developing Research Questions",
    "section": "2.2 ",
    "text": "2.2 \nHaving established this generic outline, we need to develop an understanding of what we are exploring through thinking deeply about our research question. In theory, we develop a research question, answer it, and then do all the writing; but that rarely actually happens (Franklin 2005). Instead, we typically have some idea of the question and the shape of an answer, and these become less vague as we write. This is because it is through the process of writing that we refine our thinking (King 2000, 131). Having put down some thoughts about the research question, we can start to add dot points in each of the sections, adding sub-sections with informative sub-headings as needed. We then go back and expand those dot points into paragraphs. While we do this our thinking is influenced by a web of other researchers, but also other aspects such as our circumstances and environment (Latour 1996).\nWhile writing the first draft you should ignore the feeling that you are not good enough, or that it is impossible. Just write. You need words on paper, even if they are bad, and the first draft is when you accomplish this. Remove distractions and focus on writing. Perfectionism is the enemy, and should be set aside. Sometimes this can be accomplished by getting up very early to write, by creating a deadline, or forming a writing group. Creating a sense of urgency can be useful and one option is to not bother with adding proper citations as you go, which could slow you down, and instead just add something like “[TODO: CITE R HERE]”. Do similar with graphs and tables. That is, include textual descriptions such as “[TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE]” instead of actual graphs and tables. Focus on adding content, even if it is bad. When this is all done, a first draft exists."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-1",
    "href": "lectures/lecture-07-slides.html#section-1",
    "title": "Writing and Developing Research Questions",
    "section": "2.3 ",
    "text": "2.3 \nThis first draft will be poorly written and far from great. But it is by writing a bad first draft that you can get to a good second draft, a great third draft, and eventually excellence (Lamott 1994, 20). That first draft will be too long, it will not make sense, it will contain claims that cannot be supported, and some claims that should not be. If you are not embarrassed by your first draft, then you have not written it quickly enough.\nUse the “delete” key extensively, as well as “cut” and “paste”, to turn that first draft into a second. Print the draft and using a red pen to move or remove words, sentences, and entire paragraphs, is especially helpful. The process of going from a first draft to a second draft is best done in one sitting, to help with the flow and consistency of the story. One aspect of this first rewrite is enhancing the story that we want to tell. Another aspect is taking out everything that is not the story (King 2000, 57).\nIt can be painful to remove work that seems good even if it does not quite fit into what the draft is becoming. One way to make this less painful is to make a temporary document, perhaps named “debris.qmd”, to save these unwanted paragraphs instead of immediately deleting them. Another strategy is to comment out the paragraphs. That way you can still look at the raw file and notice aspects that could be useful."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-2",
    "href": "lectures/lecture-07-slides.html#section-2",
    "title": "Writing and Developing Research Questions",
    "section": "2.4 ",
    "text": "2.4 \nAs you go through what was written in each of the sections try to bring some sense to it with special consideration to how it supports the story that is developing. This revision process is the essence of writing (McPhee 2017, 160). You should also fix the references, and add the real graphs and tables. As part of this rewriting process, the paper’s central message tends to develop, and the answers to the research questions tend to become clearer. At this point, aspects such as the introduction can be returned to and, finally, the abstract. Typos and other issues affect the credibility of the work. So these should be fixed as part of the second draft."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-3",
    "href": "lectures/lecture-07-slides.html#section-3",
    "title": "Writing and Developing Research Questions",
    "section": "2.5 ",
    "text": "2.5 \nAt this point the draft is starting to become sensible. The job is to now make it brilliant. Print it and again go through it on paper. Try to remove everything that does not contribute to the story. At about this stage, you may start to get too close to the paper. This is a great opportunity to give it to someone else for their comments. Ask for feedback about what is weak about the story. After addressing these, it can be helpful to go through the paper once more, this time reading it aloud. A paper is never “done” and it is more that at a certain point you either run out of time or become sick of the sight of it."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-4",
    "href": "lectures/lecture-07-slides.html#section-4",
    "title": "Writing and Developing Research Questions",
    "section": "3.1 ",
    "text": "3.1 \nConsider two examples:\n\nMok et al. (2022) examine eight billion unique listening events from 100,000 Spotify users to understand how users explore content. They find a clear relationship between age and behavior, with younger users exploring unknown content less than older users, despite having more diverse consumption. While it is clear that research questions around discovery and exploration drive this paper, it would not have been possible without access to this dataset. There likely would have been an iterative process where potential research questions and potential datasets were considered, before the ultimate match.\nThink of wanting to explore the neonatal mortality rate (NMR), which was introduced in ?@sec-fire-hose. One might be interested in what NMR could look like in Sub-Saharan Africa in 20 years. This would be question-first. But within this, there could be: theory-driven aspects, such as what do we expect based on biological relationships with other quantities; or data-driven aspects such as collecting as much data as possible to make forecasts. An alternative, purely data-driven approach would be having access to the NMR and then working out what is possible."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#data-first",
    "href": "lectures/lecture-07-slides.html#data-first",
    "title": "Writing and Developing Research Questions",
    "section": "3.2 Data-first",
    "text": "3.2 Data-first\nWhen being data-first, the main issue is working out the questions that can be reasonably answered with the available data. When deciding what these are, it is useful to consider:\n\nTheory: Is there a reasonable expectation that there is something causal that could be determined? For instance, Mark Christensen used to joke that if the question involved charting the stock market, then it might be better to hark back to The Odyssey and read bull entrails on a fire, because at least that way you would have something to eat at the end of the day. Questions usually need to have some plausible theoretical underpinning to help avoid spurious relationships. One way to develop theory, given data, is to consider “of what is this an instance?” (Rosenau 1999, 7). Following that approach, one tries to generalize beyond the specific setting. For instance, thinking of some particular civil war as an instance of all civil wars. The benefit of this is it focuses attention on the general attributes needed for building theory.\nImportance: There are plenty of trivial questions that can be answered, but it is important to not waste our time or that of the reader. Having an important question can also help with motivation when we find ourselves in, say, the fourth straight week of cleaning data and debugging code. In industry it can also make it easier to attract talented employees and funding. That said, a balance is needed; the question needs to have a decent chance of being answered. Attacking a generation-defining question might be best broken into chunks.\nAvailability: Is there a reasonable expectation of additional data being available in the future? This could allow us to answer related questions and turn one paper into a research agenda.\nIteration: Is this something that could be run multiple times, or is it a once-off analysis? If it is the former, then it becomes possible to start answering specific research questions and then iterate. But if we can only get access to the data once then we need to think about broader questions."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-5",
    "href": "lectures/lecture-07-slides.html#section-5",
    "title": "Writing and Developing Research Questions",
    "section": "3.3 ",
    "text": "3.3 \nThere is a saying, sometimes attributed to Xiao-Li Meng, that all of statistics is a missing data problem. And so paradoxically, another way to ask data-first questions is to think about the data we do not have. For instance, returning to the neonatal and maternal mortality examples discussed earlier one problem is that we do not have complete cause of death data. If we did, then we could count the number of relevant deaths. (Castro et al. (2023) remind us that this simplistic hypothetical would be complicated in reality because there are sometimes causes of death that are not independent of other causes.) Having established some missing data problem, we can take a data-driven approach. We look at the data we do have, and then ask research questions that speak to the extent that we can use that to approximate our hypothetical dataset.\nOne way that some researchers are data-first is that they develop a particular expertise in the data of some geographical or historical circumstance. For instance, they may be especially knowledgeable about, say, the present-day United Kingdom, or late nineteenth century Japan. They then look at the questions that other researchers are asking in other circumstances, and bring their data to that question. For instance, it is common to see a particular question initially asked for the United States, and then a host of researchers answer that same question for the United Kingdom, Canada, Australia, and many other countries.\nThere are a number of negatives to data-first research, including the fact that it can be especially uncertain. It can also struggle for external validity because there is always a worry about a selection effect.\nA variant of data-driven research is model-driven research. Here a researcher becomes an expert on some particular statistical approach and then applies that approach to appropriate contexts."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#question-first",
    "href": "lectures/lecture-07-slides.html#question-first",
    "title": "Writing and Developing Research Questions",
    "section": "3.4 Question-first",
    "text": "3.4 Question-first\nWhen trying to be question-first, there is the inverse issue of being concerned about data availability. The “FINER framework” is used in medicine to help guide the development of research questions. It recommends asking questions that are: Feasible, Interesting, Novel, Ethical, and Relevant (Hulley et al. 2007). Farrugia et al. (2010) build on FINER with PICOT, which recommends additional considerations: Population, Intervention, Comparison group, Outcome of interest, and Time."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-6",
    "href": "lectures/lecture-07-slides.html#section-6",
    "title": "Writing and Developing Research Questions",
    "section": "3.5 ",
    "text": "3.5 \nIt can feel overwhelming trying to write out a question. One way to go about it is to ask a very specific question. Another is to decide whether we are interested in descriptive, predictive, inferential, or causal analysis. These then lead to different types of questions. For instance:\n\ndescriptive analysis: “What does \\(x\\) look like?”;\npredictive analysis: “What will happen to \\(x\\)?”;\ninferential: “How can we explain \\(x\\)?”; and\ncausal: “What impact does \\(x\\) have on \\(y\\)?”.\n\nEach of these have a role to play. Since the credibility revolution (Angrist and Pischke 2010), causal questions answered with a particular approach have been predominant. This has brought some benefit, but not without cost. Descriptive analysis can be just as, indeed sometimes more, illuminating, and is critical (Sen 1980). The nature of the question being asked matters less than being genuinely interested in answering it."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-7",
    "href": "lectures/lecture-07-slides.html#section-7",
    "title": "Writing and Developing Research Questions",
    "section": "3.6 ",
    "text": "3.6 \nTime will often be constrained, possibly in an interesting way and this can guide the specifics of the research question. If we are interested in the effect of a celebrity’s announcements on the stock market, then that can be done by looking at stock prices before and after the announcement. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we must either wait a while, or we need to look at people who were treated twenty years ago. We then have selection effects and different circumstances compared to if we were to administer the drug today. Often the only reasonable thing to do is to build a statistical model, but that brings other issues."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#counterfactuals-and-bias",
    "href": "lectures/lecture-07-slides.html#counterfactuals-and-bias",
    "title": "Writing and Developing Research Questions",
    "section": "4.1 Counterfactuals and bias",
    "text": "4.1 Counterfactuals and bias\nThe creation of a counterfactual is often crucial when answering questions. A counterfactual is an if-then statement in which the “if” is false. Consider the example of Humpty Dumpty in Through the Looking-Glass by Lewis Carroll:\n\n“What tremendously easy riddles you ask!” Humpty Dumpty growled out. “Of course I don’t think so! Why, if ever I did fall off—which there’s no chance of—but if I did—” Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. “If I did fall,” he went on, “The King has promised me—with his very own mouth-to-to-” “To send all his horses and all his men,” Alice interrupted, rather unwisely.\nCarroll (1871)\n\nHumpty is satisfied with what would happen if he were to fall off, even though he is convinced that this would never happen. It is this comparison group that often determines the answer to a question. For instance, in ?@sec-causality-from-observational-data we consider the effect of VO2 max on a cyclist’s chance of winning a race. If we compare over the general population then it is an important variable. But if we only compare over well-trained athletes, then it is less important, because of selection.\n4.1.1 Selection bias and measurement bias\nTwo aspects of the data to be especially aware of when deciding on a research question are selection bias and measurement bias.\nSelection bias occurs when the results depend on who is in the sample. One of the pernicious aspects of selection bias is that we need to know about its existence in order to do anything about it. But many default diagnostics will not identify selection bias. In A/B testing, which we discuss in ?@sec-hunt-data, A/A testing is a slight variant where we create groups and compare them before imposing a treatment (hence the A/A nomenclature). This effort to check whether the groups are initially the same, can help to identify selection bias. More generally, comparing the properties of the sample, such as age-group, gender, and education, with characteristics of the population can assist as well. But the fundamental problem with selection bias and observational data is that we know people about whom we have data are different in at least one way to those about whom we do not! But we do not know in what other ways they may be different.\nSelection bias can pervade many aspects of our analysis. Even a sample that is initially representative may become biased over time. For instance, survey panels, that we discuss in ?@sec-farm-data, need to be updated from time to time because the people who do not get anything out of it stop responding.\nAnother bias to be aware of is measurement bias, which occurs when the results are affected by how the data were collected. A common example of this is if we were to ask respondents their income, then we may get different answers in-person compared with an online survey."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#estimands",
    "href": "lectures/lecture-07-slides.html#estimands",
    "title": "Writing and Developing Research Questions",
    "section": "4.2 Estimands",
    "text": "4.2 Estimands\nWe will typically be interested in using data to answer our question and it is important that we are clear about specifics. For instance, we might be interested in the effect of smoking on life expectancy. In that case, there is some true effect, which we can never know, and that true effect is called the “estimand” (Little and Lewis 2021). Defining the estimand at some point in the paper, ideally in the introduction, is critical (Lundberg, Johnson, and Stewart 2021). This is because it is easy to slightly change some specific aspect of the analysis plan and end up accidentally estimating something different (Kahan et al. 2022). They are beginning to be required by some medicine regulators (Kahan et al. 2024). For an estimand we are looking for a clear description of what the effect represents (Kahan et al. 2023). An “estimator” is a process by which we use the data that we have available to generate an “estimate” of the “estimand”. Efron and Morris (1977) provide a discussion of estimators and related concerns.\nBueno de Mesquita and Fowler (2021, 94) describe the relationship between an estimate and an estimand as:\n\\[\n\\mbox{Estimate = Estimand + Bias + Noise}\n\\]"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-8",
    "href": "lectures/lecture-07-slides.html#section-8",
    "title": "Writing and Developing Research Questions",
    "section": "4.3 ",
    "text": "4.3 \nBias refers to issues with an estimator systematically providing estimates that are different from the estimand, while noise refers to non-systematic differences. For instance, consider a standard Normal distribution. We might be interested in understanding the average, which would be our estimand. We know (in a way that we can never with real data) that the estimand is zero. Let us draw ten times from that distribution. One estimator we could use to produce an estimate is: sum the draws and divide by the number of draws. Another is to order the draws and find the middle observation. To be more specific, we will simulate this situation (Table 1).\n\nset.seed(853)\n\ntibble(\n    num_draws = c(\n        rep(10, times = 10),\n        rep(100, times = 100),\n        rep(1000, times = 1000),\n        rep(10000, times = 10000)\n    ),\n    draw = rnorm(\n        n = length(num_draws),\n        mean = 0,\n        sd = 1\n    )\n) |&gt;\n    summarise(\n        estimator_one = sum(draw) / unique(num_draws),\n        estimator_two = sort(draw)[round(unique(num_draws) / 2, 0)],\n        .by = num_draws\n    ) |&gt;\n    kable(\n        col.names = c(\"Number of draws\", \"Estimator one\", \"Estimator two\"),\n        digits = 2,\n        format.args = list(big.mark = \",\")\n    )\n\n\n\nTable 1: Comparing two estimators of the average of random draws as the number of draws increases\n\n\n\n\n\n\nNumber of draws\nEstimator one\nEstimator two\n\n\n\n\n10\n-0.58\n-0.82\n\n\n100\n-0.06\n-0.07\n\n\n1,000\n0.06\n0.04\n\n\n10,000\n-0.01\n-0.01\n\n\n\n\n\n\n\n\nAs the number of draws increases, the effect of noise is removed, and our estimates illustrate the bias of our estimators. In this example, we know what the truth is, but when considering real data it can be more difficult to know what to do. Hence the importance of being clear about what the estimand is, before turning to generating estimates."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#directed-acyclic-graphs",
    "href": "lectures/lecture-07-slides.html#directed-acyclic-graphs",
    "title": "Writing and Developing Research Questions",
    "section": "4.4 Directed Acyclic Graphs",
    "text": "4.4 Directed Acyclic Graphs\nWhen we are thinking about the variables we will use to answer our question, it can help to be specific about what we mean. It is easy to get caught up in observational data and trick ourselves. We should think hard, and to use all the tools available to us. One framework that can help with thinking hard about our data is the use of directed acyclic graphs (DAG). DAGs are a fancy name for a flow diagram and involve drawing arrows and lines between the variables to indicate the relationship between them.\nTo construct them we use Graphviz, which is an open-source package for graph visualization and is built into Quarto. The code needs to be wrapped in a “dot” chunk rather than “R”, and the chunk options are set with “//|” instead of “#|”. Alternatives that do not require this include the use of DiagrammeR (Iannone 2022) and ggdag (Barrett 2021). We provide the whole chunk for the first DAG, but then, only provide the code for the others.\n\n```{dot}\n//| label: fig-dot-firstdag-quarto\n//| fig-cap: \"We expect a causal relationship between x and y, where x influences y\"\n//| fig-width: 4\ndigraph D {\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  {rank=same x y};\n  \n  x -&gt; y;\n}\n```\n\n\n\n\n\n\n\nD\n\n\n\nx\nx\n\n\n\ny\ny\n\n\n\nx-&gt;y\n\n\n\n\n\n\n\n\nFigure 1: We expect a causal relationship between x and y, where x influences y\n\n\n\n\n\nIn Figure 1, we are saying that we think x causes y."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-12",
    "href": "lectures/lecture-07-slides.html#section-12",
    "title": "Writing and Developing Research Questions",
    "section": "4.5 ",
    "text": "4.5 \nWe could build another DAG where the situation is less clear. To make the examples a little easier to follow, we will switch to thinking about a hypothetical relationship between income and happiness, with consideration of variables that could affect that relationship. In this first one we consider the relationship between income and happiness, along with education (Figure 2).\n\ndigraph D {\n  \n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Education\"];\n  \n  { rank=same a b};\n  \n  a-&gt;b;\n  c-&gt;{a, b};\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nEducation\n\n\n\nc-&gt;a\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\nFigure 2: Education is a confounder that affects the relationship between income and happiness\n\n\n\n\n\nIn Figure 2, we think income causes happiness. But we also think that education causes happiness, and that education also causes income. That relationship is a “backdoor path”, and failing to adjust for education in a regression could overstate the extent of the relationship, or even create a spurious relationship, between income and happiness in our analysis. That is, we may think that changes in income are causing changes in happiness, but it could be that education is changing them both. That variable, in this case, education, is called a “confounder”.\nHernán and Robins (2023, 83) discuss an interesting case where a researcher was interested in whether one person looking up at the sky makes others look up at the sky also. There was a clear relationship between the responses of both people. But it was also the case that there was noise in the sky. It was unclear whether the second person looked up because the first person looked up, or they both looked up because of the noise. When using experimental data, randomization allows us to avoid this concern, but with observational data we cannot rely on that. It is also not the case that bigger data necessarily get around this problem for us. Instead, we should think carefully about the situation, and DAGs can help with that.\nIf there are confounders, but we are still interested in causal effects, then we need to adjust for them. One way is to include them in the regression. But the validity of this requires several assumptions. In particular, Gelman and Hill (2007, 169) warn that our estimate will only correspond to the average causal effect in the sample if we include all of the confounders and have the right model. Putting the second requirement to one side, and focusing only on the first, if we do not think about and observe a confounder, then it can be difficult to adjust for it. And this is an area where both domain expertise and theory can bring considerable weight to an analysis.\nIn Figure 3 we again consider that income causes happiness. But, if income also causes children, and children also cause happiness, then we have a situation where it would be tricky to understand the effect of income on happiness.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Children\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b, c};\n  c-&gt;b;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nChildren\n\n\n\na-&gt;c\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\nFigure 3: Children as a mediator between income and happiness\n\n\n\n\n\nIn Figure 3, children is called a “mediator” and we would not adjust for it if we were interested in the effect of income on happiness. If we were to adjust for it, then some of what we are attributing to income, would be due to children.\nFinally, in Figure 4 we have yet another similar situation, where we think that income causes happiness. But this time both income and happiness also cause exercise. For instance, if you have more money then it may be easier to exercise, but also it may be easier to exercise if you are happier.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Exercise\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b c};\n  b-&gt;c;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nExercise\n\n\n\na-&gt;c\n\n\n\n\n\nb-&gt;c\n\n\n\n\n\n\n\n\nFigure 4: Exercise as a collider affecting the relationship between income and happiness\n\n\n\n\n\nIn this case, exercise is called a “collider” and if we were to condition on it, then we would create a misleading relationship. Income influences exercise, but a person’s happiness also affects this. Exercise is a collider because both the predictor and outcome variable of interest influence it."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-13",
    "href": "lectures/lecture-07-slides.html#section-13",
    "title": "Writing and Developing Research Questions",
    "section": "4.6 ",
    "text": "4.6 \nWe will be clear about this: we must create the DAG ourselves, in the same way that we must put together the model ourselves. There is nothing that will create it for us. This means that we need to think carefully about the situation. Because it is one thing to see something in the DAG and then do something about it, but it is another to not even know that it is there. McElreath ([2015] 2020, 180) describes these as haunted DAGs. DAGs are helpful, but they are just a tool to help us think deeply about our situation.\nWhen we are building models, it can be tempting to include as many predictor variables as possible. DAGs show clearly why we need to be more thoughtful. For instance, if a variable is a confounder, then we would want to adjust for it, whereas if a variable was a collider then we would not. We can never know the truth, and we are informed by aspects such as theory, what we are interested in, research design, limitations of the data, or our own limitations as researchers, to name a few. Knowing the limits is as important as reporting the model. Data and models with flaws are still useful, if you acknowledge those flaws. The work of thinking about a situation is never done, and relies on others, which is why we need to make all our work as reproducible as possible."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-14",
    "href": "lectures/lecture-07-slides.html#section-14",
    "title": "Writing and Developing Research Questions",
    "section": "5.1 ",
    "text": "5.1 \nWe discuss the following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, and technical terms.1 Throughout the paper try to be as brief and specific as possible. Most readers will not get past the title. Almost no one will read more than the abstract. Section and sub-section headings, as well as graph and table captions should work on their own, without the surrounding text, because that type of skimming is how many people read papers (Keshav 2007).\nWhile there is sometimes a need for a separate literature review section, another approach is to discuss relevant literature throughout the paper as appropriate. For instance, when there is literature relevant to the data then it should be discussed in this section, while literature relevant to the model, results, or discussion should be mentioned as appropriate in those sections."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#title",
    "href": "lectures/lecture-07-slides.html#title",
    "title": "Writing and Developing Research Questions",
    "section": "5.2 Title",
    "text": "5.2 Title\nA title is the first opportunity that we have to engage our reader in our story. Ideally, we are able to tell our reader exactly what we found. Effective titles are critical because otherwise papers could be ignored by readers. While a title does not have to be “cute”, it does need to be meaningful. This means it needs to make the story clear.\nOne example of a title that is good enough is “On the 2016 Brexit referendum”. This title is useful because the reader knows what the paper is about. But it is not particularly informative or enticing. A slightly better title could be “On the Vote Leave outcome in the 2016 Brexit referendum”. This variant adds informative specificity. We argue the best title would be something like “Vote Leave outperforms in rural areas in the 2016 Brexit referendum: Evidence from a Bayesian hierarchical model”. Here the reader knows the approach of the paper and also the main take-away.\nWe will consider a few examples of particularly effective titles. Hug et al. (2019) use “National, regional, and global levels and trends in neonatal mortality between 1990 and 2017, with scenario-based projections to 2030: a systematic analysis”. Here it is clear what the paper is about and the methods that are used. R. Alexander and Alexander (2021) use “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018”. The main finding is, along with a good deal of information about what the content will be, clear from the title. M. Alexander, Kiang, and Barbieri (2018) use “Trends in Black and White Opioid Mortality in the United States, 1979–2015”; Frei and Welsh (2022) use “How the closure of a US tax loophole may affect investor portfolios”. Possibly one of the best titles ever is Bickel, Hammel, and O’Connell (1975) “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation”, which we return to in ?@sec-causality-from-observational-data.\nA title is often among the last aspects of a paper to be finalized. While getting through the first draft, we typically use a working title that gets the job done. We then refine it over the course of redrafting. The title needs to reflect the final story of the paper, and this is not usually something that we know at the start. We must strike a balance between getting our reader interested enough to read the paper, and conveying enough of the content so as to be useful (Hayot 2014). Two excellent examples are The History of England from the Accession of James the Second by Thomas Babington Macaulay, and A History of the English-Speaking Peoples by Winston Churchill. Both are clear about what the content is, and, for their target audience, spark interest.\nOne specific approach is the form: “Exciting content: Specific content”, for instance, “Returning to their roots: Examining the performance of Vote Leave in the 2016 Brexit referendum”. Kennedy and Gelman (2021) provide a particularly nice example of this approach with “Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample”, as does Craiu (2019) with “The Hiring Gambit: In Search of the Twofer Data Scientist”. A close variant of this is “A question? And an approach”. For instance, Cahill, Weinberger, and Alkema (2020) with “What increase in modern contraceptive use is needed in FP2020 countries to reach 75% demand satisfied by 2030? An assessment using the Accelerated Transition Method and Family Planning Estimation Model”. As you gain experience with this variant, it becomes possible to know when it is appropriate to drop the answer part yet remain effective, such as Briggs (2021) with “Why Does Aid Not Target the Poorest?”. Another specific approach is “Specific content then broad content” or the inverse. For instance, “Rurality, elites, and support for Vote Leave in the 2016 Brexit referendum” or “Support for Vote Leave in the 2016 Brexit referendum, rurality and elites”. This approach is used by Tolley and Paquet (2021) with “Gender, municipal party politics, and Montreal’s first woman mayor”.\nSometimes it is possible to include a subtitle. When this is possible, a great way to take advantage of this is to use it to include some detail of the main quantitative result that you found. Getting the right level of detail and abstraction about that result is difficult and will require re-writing and getting other’s opinions."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#abstract",
    "href": "lectures/lecture-07-slides.html#abstract",
    "title": "Writing and Developing Research Questions",
    "section": "5.3 Abstract",
    "text": "5.3 Abstract\nFor a ten-to-fifteen-page paper, a good abstract is a three-to-five sentence paragraph. For a longer paper the abstract can be slightly longer. The abstract needs to specify the story of the paper. It must also convey what was done and why it matters. To do so, an abstract typically touches on the context of the work, its objectives, approach, and findings.\nMore specifically, a good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.\nWe see this pattern in a variety of abstracts. For instance, Tolley and Paquet (2021) draw in the reader with their first sentence by mentioning the election of the first woman mayor in 400 years. The second sentence is clear about what is done in the paper. The third sentence tells the reader how it is done i.e. a survey, and the fourth sentence adds some detail. The fifth and final sentence makes the main take-away clear.\n\nIn 2017, Montreal elected Valérie Plante, the first woman mayor in the city’s 400-year history. Using this election as a case study, we show how gender did and did not influence the outcome. A survey of Montreal electors suggests that gender was not a salient factor in vote choice. Although gender did not matter much for voters, it did shape the organization of the campaign and party. We argue that Plante’s victory can be explained in part by a strategy that showcased a less leader-centric party and a degendered campaign that helped counteract stereotypes about women’s unsuitability for positions of political leadership.\n\nSimilarly, Beauregard and Sheppard (2021) make the broader environment clear within the first two sentences, and the specific contribution of this paper to that environment. The third and fourth sentences make the data source and main findings clear. The fifth and sixth sentences add specificity that would be of interest to likely readers of this abstract i.e. academic political scientists. In the final sentence, the position of the authors is made clear.\n\nPrevious research on support for gender quotas focuses on attitudes toward gender equality and government intervention as explanations. We argue the role of attitudes toward women in understanding support for policies aiming to increase the presence of women in politics is ambivalent—both hostile and benevolent forms of sexism contribute in understanding support, albeit in different ways. Using original data from a survey conducted on a probability-based sample of Australian respondents, our findings demonstrate that hostile sexists are more likely to oppose increasing of women’s presence in politics through the adoption of gender quotas. Benevolent sexists, on the other hand, are more likely to support these policies than respondents exhibiting low levels of benevolent sexism. We argue this is because benevolent sexism holds that women are pure and need protection; they do not have what it takes to succeed in politics without the assistance of quotas. Finally, we show that while women are more likely to support quotas, ambivalent sexism has the same relationship with support among both women and men. These findings suggest that aggregate levels of public support for gender quotas do not necessarily represent greater acceptance of gender equality generally."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-15",
    "href": "lectures/lecture-07-slides.html#section-15",
    "title": "Writing and Developing Research Questions",
    "section": "5.4 ",
    "text": "5.4 \nAnother excellent example of an abstract is Sides, Vavreck, and Warshaw (2021). In just five sentences, they make it clear what they do, how they do it, what they find, and why it is important.\n\nWe provide a comprehensive assessment of the influence of television advertising on United States election outcomes from 2000–2018. We expand on previous research by including presidential, Senate, House, gubernatorial, Attorney General, and state Treasurer elections and using both difference-in-differences and border-discontinuity research designs to help identify the causal effect of advertising. We find that televised broadcast campaign advertising matters up and down the ballot, but it has much larger effects in down-ballot elections than in presidential elections. Using survey and voter registration data from multiple election cycles, we also show that the primary mechanism for ad effects is persuasion, not the mobilization of partisans. Our results have implications for the study of campaigns and elections as well as voter decision making and information processing."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-16",
    "href": "lectures/lecture-07-slides.html#section-16",
    "title": "Writing and Developing Research Questions",
    "section": "5.5 ",
    "text": "5.5 \nThe best abstracts will have such a high content to words ratio that they may even feel a little terse. For instance, in the abstract of Touvron et al. (2023), there is not a word that is wasted and they communicate a large amount of information in only four sentences.\n\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\n\nKasy and Teytelboym (2023) provide an excellent example of a more statistical abstract. They clearly identify what they do and why it is important.\n\nWe consider an experimental setting in which a matching of resources to participants has to be chosen repeatedly and returns from the individual chosen matches are unknown but can be learned. Our setting covers two-sided and one-sided matching with (potentially complex) capacity constraints, such as refugee resettlement, social housing allocation, and foster care. We propose a variant of the Thompson sampling algorithm to solve such adaptive combinatorial allocation problems. We give a tight, prior-independent, finite-sample bound on the expected regret for this algorithm. Although the number of allocations grows exponentially in the number of matches, our bound does not. In simulations based on refugee resettlement data using a Bayesian hierarchical model, we find that the algorithm achieves half of the employment gains (relative to the status quo) that could be obtained in an optimal matching based on perfect knowledge of employment probabilities."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-17",
    "href": "lectures/lecture-07-slides.html#section-17",
    "title": "Writing and Developing Research Questions",
    "section": "5.6 ",
    "text": "5.6 \nFinally, Briggs (2021) begins with a claim that seems unquestionably true. In the second sentence he then says that it is false! The third sentence specifies the extent of this claim, and the fourth sentence details how he comes to this position, before providing more detail. The final two sentences speak broader implications and importance.\n\nForeign-aid projects typically have local effects, so they need to be placed close to the poor if they are to reduce poverty. I show that, conditional on local population levels, World Bank (WB) project aid targets richer parts of countries. This relationship holds over time and across world regions. I test five donor-side explanations for pro-rich targeting using a pre-registered conjoint experiment on WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments as most interested in targeting aid politically and controlling implementation. They also believe that aid works better in poorer or more remote areas, but that implementation in these areas is uniquely difficult. These results speak to debates in distributive politics, international bargaining over aid, and principal-agent issues in international organizations. The results also suggest that tweaks to WB incentive structures to make ease of project implementation less important may encourage aid to flow to poorer parts of countries."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-18",
    "href": "lectures/lecture-07-slides.html#section-18",
    "title": "Writing and Developing Research Questions",
    "section": "5.7 ",
    "text": "5.7 \nNature, a scientific journal, provides a guide for constructing an abstract. They recommend a structure that results in an abstract of six parts and adds up to around 200 words:\n\nAn introductory sentence that is comprehensible to a wide audience.\nA more detailed background sentence that is relevant to likely readers.\nA sentence that states the general problem.\nSentences that summarize and then explain the main results.\nA sentence about general context.\nAnd finally, a sentence about the broader perspective.\n\nThe first sentence of an abstract should not be vacuous. Assuming the reader continued past the title, this first sentence is the next opportunity that we have to implore them to keep reading our paper. And then the second sentence of the abstract, and so on. Work and re-work the abstract until it is so good that you would be fine if that was the only thing that was read; because that will often be the case."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-19",
    "href": "lectures/lecture-07-slides.html#section-19",
    "title": "Writing and Developing Research Questions",
    "section": "5.8 ",
    "text": "5.8 \n5.8.1 Introduction\nAn introduction needs to be self-contained and convey everything that a reader needs to know. We are not writing a mystery story. Instead, we want to give away the most important points in the introduction. For a ten-to-fifteen-page paper, an introduction may be two or three paragraphs of main content. Hayot (2014, 90) says the goal of an introduction is to engage the reader, locate them in some discipline and background, and then tell them what happens in the rest of the paper. It should be completely reader-focused.\nThe introduction should set the scene and give the reader some background. For instance, we typically start a little broader. This provides some context to the paper. We then describe how the paper fits into that context, and give some high-level results, especially focused on the one key result that is the main part of the story. We provide more detail here than we provided in the abstract, but not the full extent. And we broadly discuss next steps in a sentence or two. Finally, we finish the introduction with an additional short final paragraph that highlights the structure of the paper."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-20",
    "href": "lectures/lecture-07-slides.html#section-20",
    "title": "Writing and Developing Research Questions",
    "section": "5.9 ",
    "text": "5.9 \nAs an example (with made-up details):\n\nThe UK Conservative Party has always done well in rural electorates. And the 2016 Brexit vote was no different with a significant difference in support between rural and urban areas. But even by the standard of rural support for conservative issues, support for “Vote Leave” was unusually strong with “Vote Leave” being most heavily supported in the East Midlands and the East of England, while the strongest support for “Remain” was in Greater London.\nIn this paper we look at why the performance of “Vote Leave” in the 2016 Brexit referendum was so correlated with rurality. We construct a model in which support for “Vote Leave” at a voting area level is explained by the number of farms in the area, the average internet connectivity, and the median age. We find that as the median age of an area increases, the likelihood that an area supported “Vote Leave” decreases by 14 percentage points. Future work could look at the effect of having a Conservative MP which would allow a more nuanced understanding of these effects.\nThe remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.\n\nThe introduction needs to be self-contained and tell the reader almost everything that they need to know. A reader should be able to only read the introduction and have an accurate picture of all the major aspects of the whole paper. It would be rare to include graphs or tables in the introduction. An introduction should close by telegraphing the structure of the paper."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-21",
    "href": "lectures/lecture-07-slides.html#section-21",
    "title": "Writing and Developing Research Questions",
    "section": "5.10 ",
    "text": "5.10 \n5.10.1 Data\nRobert Caro, Lyndon Johnson’s biographer, describes the importance of conveying “a sense of place” when writing a biography (Caro 2019, 141). He defines this as “the physical setting in which a book’s action is occurring: to see it clearly enough, in sufficient detail, so that he feels as if he himself were present while the action is occurring.” He provides the following example:\n\nWhen Rebekah walked out the front door of that little house, there was nothing—a roadrunner streaking behind some rocks with something long and wet dangling from his beak, perhaps, or a rabbit disappearing around a bush so fast that all she really saw was the flash of a white tail—but otherwise nothing. There was no movement except for the ripple of the leaves in the scattered trees, no sound except for the constant whisper of the wind\\(\\dots\\) If Rebekah climbed, almost in desperation, the hill in the back of the house, what she saw from its crest was more hills, an endless vista of hills, hills on which there was visible not a single house\\(\\dots\\) hills on which nothing moved, empty hills with, above them, empty sky; a hawk circling silently overhead was an event. But most of all, there was nothing human, no one to talk to.\nCaro (2019, 146)\n\nHow thoroughly we can imagine the circumstances of Johnson’s mother, Rebekah Baines Johnson. When writing our papers, we need to achieve that same sense of place, for our data, as Caro provides for the Hill County. We do this by being as explicit as possible. We typically have a whole section about it and this is designed to show the reader, as closely as possible, the actual data that underpin our story.\nWhen writing the data section, we are beginning our answer to the critical question about our claim, which is, how is it possible to know this? (McPhee 2017, 78). An excellent example of a data section is provided by Doll and Hill (1950). They are interested in the effect of smoking between control and treatment groups. After clearly describing their dataset they use tables to display relevant cross-tabs and graphs to contrast groups.\nIn the data section we need to thoroughly discuss the variables in the dataset that we are using. If there are other datasets that could have been used, but were not, then this should be mentioned and the choice justified. If variables were constructed or combined, then this process and motivation should be explained.\nWe want the reader to understand what the data that underpin the results look like. This means that we should graph the data that are used in our analysis, or as close to them as possible. And we should also include tables of summary statistics. If the dataset was created from some other source, then it can also help to include an example of that original source. For instance, if the dataset was created from survey responses then the underlying survey questions should be included in an appendix.\nSome judgment is required when it comes to the figures and tables in the data section. The reader should have the opportunity to understand the details, but it may be that some are better placed in an appendix. Figures and tables are a critical aspect of convincing people of a story. In a graph we can show the data and then let the reader decide for themselves. And using a table, we can summarize a dataset. At the very least, every variable should be shown in a graph and summarized in a table. If there are too many, then some of these could be relegated to an appendix, with the critical relationships shown in the main body. Figures and tables should be numbered and then cross-referenced in the text, for instance, “Figure 1 shows\\(\\dots\\)”, “Table 1 describes\\(\\dots\\)”. For every graph and table there should be accompanying text that describes their main aspects, and adds additional detail."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-22",
    "href": "lectures/lecture-07-slides.html#section-22",
    "title": "Writing and Developing Research Questions",
    "section": "5.11 ",
    "text": "5.11 \nWe discuss the components of graphs and tables, including titles and labels, in ?@sec-static-communication. But here we will discuss captions, as they are between the text and the graph or table. Captions need to be informative and self-contained. Borkin et al. (2015) use eye-tracking to understand how visualizations are recognized and recalled. They find that captions need to make the central message of the figure clear, and that there should be redundancy. As Cleveland ([1985] 1994, 57) says, the “interplay between graph, caption, and text is a delicate one”, however the reader should be able to read only the caption and understand what the graph or table shows. A caption that is two lines long is not necessarily inappropriate. And all aspects of the graph or table should be explained. For instance, consider Figure 5 (a) and Figure 5 (b), both from Bowley (1901, 151). They are clear, and self-contained.\n\n\n\n\n\n\n\n\n\n\n\n(a) Example of a well-captioned figure\n\n\n\n\n\n\n\n\n\n\n\n(b) Example of a well-captioned table\n\n\n\n\n\n\n\nFigure 5: Examples of a graph and table from Bowley (1901)\n\n\n\nThe choice between a table and a graph comes down to how much information is to be conveyed. In general, if there is specific information that should be considered, such as a summary statistic, then a table is a good option. If we are interested in the reader making comparisons and understanding trends, then a graph is a good option (Gelman, Pasarica, and Dodhia 2002)."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-23",
    "href": "lectures/lecture-07-slides.html#section-23",
    "title": "Writing and Developing Research Questions",
    "section": "5.12 ",
    "text": "5.12 \n5.12.1 Model\nWe often build a statistical model that we will use to explore the data, and it is normal to have a specific section about this. At a minimum you should specify the equations that describe the model being used and explain their components with plain language and cross-references.\nThe model section typically begins with the model being written out, explained, and justified. Depending on the expected reader, some background may be needed. After specifying the model with appropriate mathematical notation and cross-referencing it, the components of the model should then be defined and explained. Try to define each aspect of the notation. This helps convince the reader that the model was well-chosen and enhances the credibility of the paper. The model’s variables should correspond to those that were discussed in the data section, making a clear link between the two sections.\nThere should be some discussion of how features enter the model and why. Some examples could include:\n\nWhy use age rather than age-groups?\nWhy does state/province have a levels effect?\nWhy is gender a categorical variable? In general, we are trying to convey a sense that this is the appropriate model for the situation. We want the reader to understand how the aspects that were discussed in the data section assert themselves in the modeling decisions that were made.\n\nThe model section should close with some discussion of the assumptions that underpin the model. It should also have a brief discussion of alternative models or variants. You want the strengths and weaknesses to be clear and for the reader to know why this particular model was chosen.\nAt some point in this section, it is usually appropriate to specify the software that was used to run the model, and to provide some evidence of thought about the circumstances in which the model may not be appropriate. That second point would typically be expanded on in the discussion section. And there should be evidence of model validation and checking, model convergence, and/or diagnostic issues. Again, there is a balance needed here, and some of this content may be more appropriately placed in appendices.\nWhen technical terms are used, they should be briefly explained in plain language for readers who might not be familiar with it. For instance, M. Alexander (2019) integrates an explanation of the Gini coefficient that brings the reader along.\n\nTo look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.\n\nThere may be papers that do not include a statistical model. In that case, this “Model” section should be replaced by a broader “Methodology” section. It might describe the simulation that was conducted, or contain more general details about the approach."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-24",
    "href": "lectures/lecture-07-slides.html#section-24",
    "title": "Writing and Developing Research Questions",
    "section": "5.13 ",
    "text": "5.13 \n5.13.1 Results\nTwo excellent examples of results sections are provided by Kharecha and Hansen (2013) and Kiang et al. (2021). In the results section, we want to communicate the outcomes of the analysis in a clear way and without too much focus on the discussion of implications. The results section likely requires summary statistics, tables, and graphs. Each of those aspects should be cross-referenced and have text associated with them that details what is seen in each figure. This section should relay results; that is, we are interested in what the results are, rather than what they mean.\nThis section would also typically include tables of graphs of coefficient estimates based on the modeling. Various features of the estimates should be discussed, and differences between the models explained. It may be that different subsets of the data are considered separately. Again, all graphs and tables need to have text in plain language accompany them. A rough guide is that the amount of text should be at least equal to the amount of space taken up by the tables and graphs. For instance, if a full page is used to display a table of coefficient estimates, then that should be cross-referenced and accompanied by about a full page of text about that table."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-25",
    "href": "lectures/lecture-07-slides.html#section-25",
    "title": "Writing and Developing Research Questions",
    "section": "5.14 ",
    "text": "5.14 \n5.14.1 Discussion\nA discussion section may be the final section of a paper and would typically have four or five sub-sections.\nThe discussion section would typically begin with a sub-section that comprises a brief summary of what was done in the paper. This would be followed by two or three sub-sections that are devoted to the key things that we learn about the world from this paper. These sub-sections are the main opportunity to justify or detail the implications of the story being told in the paper. Typically, these sub-sections do not see newly introduced graphs or tables, but are instead focused on what we learn from those that were introduced in earlier sections. It may be that some of the results are discussed in relation to what others have found, and differences could be attempted to be reconciled here.\nFollowing these sub-sections of what we learn about the world, we would typically have a sub-section focused on some of the weaknesses of what was done. This could concern aspects such as the data that were used, the approach, and the model. In the case of the model we are especially concerned with those aspects that might affect the findings. This can be especially difficult in the case of machine learning models and Smith et al. (2022) provide guidance for aspects to consider. And the final sub-section is typically a few paragraphs that specify what is left to learn, and how future work could proceed.\nIn general, we would expect this section to take at least 25 per cent of the total paper. This means that in an eight-page paper we would expect at least two pages of discussion."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#section-26",
    "href": "lectures/lecture-07-slides.html#section-26",
    "title": "Writing and Developing Research Questions",
    "section": "5.15 ",
    "text": "5.15 \n5.15.1 Brevity, typos, and grammar\nBrevity is important. This is partly because we write for the reader, and the reader has other priorities. But it is also because as the writer it forces us to consider what our most important points are, how we can best support them, and where our arguments are weakest. Jean Chrétien, is a former Canadian prime minister. In Chrétien (2007, 105) he wrote that he used to ask “\\(\\dots\\)the officials to summarize their documents in two or three pages and attach the rest of the materials as background information. I soon discovered that this was a problem only for those who didn’t really know what they were talking about” .\nThis experience is not unique to Canada and it is not new. In Hughes and Rutter (2016) Oliver Letwin, the former British cabinet member, describes there being “a huge amount of terrible guff, at huge, colossal, humongous length coming from some departments” and how he asked “for them to be one quarter of the length”. He found that the departments were able to accommodate this request without losing anything important. Winston Churchill asked for brevity during the Second World War, saying “the discipline of setting out the real points concisely will prove an aid to clearer thinking.” The letter from Szilard and Einstein to FDR that was the catalyst for the Manhattan Project was only two pages!\nZinsser (1976) goes further and describes “the secret of good writing” being “to strip every sentence to its cleanest components.” Every sentence should be simplified to its essence. And every word that does not contribute should be removed.\nUnnecessary words, typos, and grammatical issues should be removed from papers. These mistakes affect the credibility of claims. If the reader cannot trust you to use a spell-checker, then why should they trust you to use logistic regression? RStudio has a spell-checker built in, but Microsoft Word and Google Docs are useful additional checks. Copy from the Quarto document and paste into Word, then look for the red and green lines, and fix them in the Quarto document.\nWe are not worried about the n-th degree of grammatical content. Instead, we are interested in grammar and sentence structure that occurs in conversational language use (King 2000, 118). The way to develop comfort is by reading widely and asking others to also read your work. Another useful tactic is to read your writing aloud, which can be useful for detecting odd sentences based on how they sound. One small aspect to check that will regularly come up is that any number from one to ten should be written as words, while 11 and over should be written as numbers."
  },
  {
    "objectID": "lectures/lecture-07-slides.html#rules-for-writing",
    "href": "lectures/lecture-07-slides.html#rules-for-writing",
    "title": "Writing and Developing Research Questions",
    "section": "5.16 Rules for writing",
    "text": "5.16 Rules for writing\nA variety of authors have established rules for writing. This famously includes those of Orwell (1946) which were reimagined by The Economist (2013). A further reimagining of rules for writing, focused on telling stories with data, could be:\n\nFocus on the reader and their needs. Everything else is commentary.\nEstablish a structure and then rely on that to tell the story.\nWrite a first draft as quickly as possible.\nRewrite that draft extensively.\nBe concise and direct. Remove as many words as possible.\nUse words precisely. For instance, stock prices rise or fall, rather than improve or worsen.\nUse short sentences where possible.\nAvoid jargon.\nWrite as though your work will be on the front page of a newspaper.\nNever claim novelty or that you are the “first to study X”—there is always someone else who got there first.\n\nFiske and Kuriwaki (2021) have a list of rules for scientific papers and the appendix of Pineau et al. (2021) provides a checklist for machine learning papers. But perhaps the last word should be from Savage and Yeh (2019):\n\n[T]ry to write the best version of your paper: the one that you like. You can’t please an anonymous reader, but you should be able to please yourself. Your paper—you hope—is for posterity.\nSavage and Yeh (2019, 442)"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#references",
    "href": "lectures/lecture-07-slides.html#references",
    "title": "Writing and Developing Research Questions",
    "section": "5.17 References",
    "text": "5.17 References"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#references-1",
    "href": "lectures/lecture-07-slides.html#references-1",
    "title": "Writing and Developing Research Questions",
    "section": "5.18 References",
    "text": "5.18 References\n\n\nAlexander, Monica. 2019. “The Concentration and Uniqueness of Baby Names in Australia and the US,” January. https://www.monicaalexander.com/posts/2019-20-01-babynames/.\n\n\nAlexander, Monica, Mathew Kiang, and Magali Barbieri. 2018. “Trends in Black and White Opioid Mortality in the United States, 1979–2015.” Epidemiology 29 (5): 707–15. https://doi.org/10.1097/EDE.0000000000000858.\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” https://doi.org/10.48550/arXiv.2111.09299.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2010. “The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con Out of Econometrics.” Journal of Economic Perspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nBarrett, Malcolm. 2021. ggdag: Analyze and Create Elegant Directed Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBeauregard, Katrine, and Jill Sheppard. 2021. “Antiwomen but Proquota: Disaggregating Sexism and Support for Gender Quota Policies.” Political Psychology 42 (2): 219–37. https://doi.org/10.1111/pops.12696.\n\n\nBickel, Peter, Eugene Hammel, and William O’Connell. 1975. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring Bias Is Harder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary to Expectation.” Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBorkin, Michelle, Zoya Bylinskii, Nam Wook Kim, Constance May Bainbridge, Chelsea Yeh, Daniel Borkin, Hanspeter Pfister, and Aude Oliva. 2015. “Beyond Memorability: Visualization Recognition and Recall.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 519–28. https://doi.org/10.1109/TVCG.2015.2467732.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P. S. King.\n\n\nBriggs, Ryan. 2021. “Why Does Aid Not Target the Poorest?” International Studies Quarterly 65 (3): 739–52. https://doi.org/10.1093/isq/sqab035.\n\n\nBueno de Mesquita, Ethan, and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. New Jersey: Princeton University Press.\n\n\nCahill, Niamh, Michelle Weinberger, and Leontine Alkema. 2020. “What Increase in Modern Contraceptive Use Is Needed in FP2020 Countries to Reach 75% Demand Satisfied by 2030? An Assessment Using the Accelerated Transition Method and Family Planning Estimation Model.” Gates Open Research 4. https://doi.org/10.12688/gatesopenres.13125.1.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan. https://www.gutenberg.org/files/12/12-h/12-h.htm.\n\n\nCastro, Marcia, Susie Gurzenda, Cassio Turra, Sun Kim, Theresa Andrasfay, and Noreen Goldman. 2023. “Research Note: COVID-19 Is Not an Independent Cause of Death.” Demography, February. https://doi.org/10.1215/00703370-10575276.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. 1st ed. Toronto: Knopf Canada.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data. 2nd ed. New Jersey: Hobart Press.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nDoll, Richard, and Bradford Hill. 1950. “Smoking and Carcinoma of the Lung.” British Medical Journal 2 (4682): 739–48. https://doi.org/10.1136/bmj.2.4682.739.\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in Statistics.” Scientific American 236 (May): 119–27. https://doi.org/10.1038/scientificamerican0577-119.\n\n\nFarrugia, Patricia, Bradley Petrisor, Forough Farrokhyar, and Mohit Bhandari. 2010. “Research Questions, Hypotheses and Objectives.” Canadian Journal of Surgery 53 (4): 278.\n\n\nFiske, Susan, and Shiro Kuriwaki. 2021. “Words to the Wise on Writing Scientific Papers,” November. https://doi.org/10.31234/osf.io/n32qw.\n\n\nFranklin, Laura. 2005. “Exploratory Experiments.” Philosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nFrei, Christoph, and Liam Welsh. 2022. “How the Closure of a U.S. Tax Loophole May Affect Investor Portfolios.” Journal of Risk and Financial Management 15 (5): 209. https://doi.org/10.3390/jrfm15050209.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” The American Statistician 56 (2): 121–30. https://doi.org/10.1198/000313002317572790.\n\n\nGraham, Paul. 2020. “How to Write Usefully,” February. http://paulgraham.com/useful.html.\n\n\nGustafsson, Karl, and Linus Hagström. 2017. “What Is the Point? Teaching Graduate Students How to Construct Political Science Research Puzzles.” European Political Science 17 (4): 634–48. https://doi.org/10.1057/s41304-017-0130-y.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. New York: Columbia University Press.\n\n\nHernán, Miguel, and James Robins. 2023. What If. 1st ed. Boca Raton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN Inter-agency Group for Child. 2019. “National, Regional, and Global Levels and Trends in Neonatal Mortality Between 1990 and 2017, with Scenario-Based Projections to 2030: A Systematic Analysis.” Lancet Global Health 7 (6): e710–20. https://doi.org/10.1016/S2214-109X(19)30163-9.\n\n\nHughes, Nicola, and Jill Rutter. 2016. “Ministers Reflect: Interview with Oliver Letwin,” December. https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/.\n\n\nHulley, Stephen, Steven Cummings, Warren Browner, Deborah Grady, and Thomas Newman. 2007. Designing Clinical Research. 3rd ed. Lippincott Williams & Wilkins.\n\n\nIannone, Richard. 2022. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nKahan, Brennan, Suzie Cro, Fan Li, and Michael Harhay. 2023. “Eliminating Ambiguous Treatment Effects Using Estimands.” American Journal of Epidemiology, February. https://doi.org/10.1093/aje/kwad036.\n\n\nKahan, Brennan, Joanna Hindley, Mark Edwards, Suzie Cro, and Tim Morris. 2024. “The estimands framework: a primer on the ICH E9(R1) addendum.” BMJ, January, e076316. https://doi.org/10.1136/bmj-2023-076316.\n\n\nKahan, Brennan, Fan Li, Andrew Copas, and Michael Harhay. 2022. “Estimands in Cluster-Randomized Trials: Choosing Analyses That Answer the Right Question.” International Journal of Epidemiology, July. https://doi.org/10.1093/ije/dyac131.\n\n\nKasy, Maximilian, and Alexander Teytelboym. 2023. “Matching with Semi-Bandits.” The Econometrics Journal 26 (1): 45–66. https://doi.org/10.1093/ectj/utac021.\n\n\nKennedy, Lauren, and Andrew Gelman. 2021. “Know Your Population and Know Your Model: Using Model-Based Regression and Poststratification to Generalize Findings Beyond the Observed Sample.” Psychological Methods 26 (5): 547–58. https://doi.org/10.1037/met0000362.\n\n\nKeshav, Srinivasan. 2007. “How to Read a Paper.” ACM SIGCOMM Computer Communication Review 37 (3): 83–84. https://doi.org/10.1145/1273445.1273458.\n\n\nKharecha, Pushker, and James Hansen. 2013. “Prevented Mortality and Greenhouse Gas Emissions from Historical and Projected Nuclear Power.” Environmental Science & Technology 47 (9): 4889–95. https://doi.org/10.1021/es3051197.\n\n\nKiang, Mathew, Alexander Tsai, Monica Alexander, David Rehkopf, and Sanjay Basu. 2021. “Racial/Ethnic Disparities in Opioid-Related Mortality in the USA, 1999–2019: The Extreme Case of Washington DC.” Journal of Urban Health 98 (5): 589–95. https://doi.org/10.1007/s11524-021-00573-8.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and Life. Anchor Books.\n\n\nLatour, Bruno. 1996. “On Actor-Network Theory: A Few Clarifications.” Soziale Welt 47 (4): 369–81. http://www.jstor.org/stable/40878163.\n\n\nLight, Richard, Judith Singer, and John Willett. 1990. By Design: Planning Research on Higher Education. 1st ed. Cambridge: Harvard University Press.\n\n\nLittle, Roderick, and Roger Lewis. 2021. “Estimands, Estimators, and Estimates.” JAMA 326 (10): 967. https://doi.org/10.1001/jama.2021.2886.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus; Giroux.\n\n\nMok, Lillio, Samuel Way, Lucas Maystre, and Ashton Anderson. 2022. “The Dynamics of Exploration on Spotify.” In Proceedings of the International AAAI Conference on Web and Social Media, 16:663–74. https://doi.org/10.1609/icwsm.v16i1.19324.\n\n\nOrwell, George. 1946. Politics and the English Language. https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research (a Report from the NeurIPS 2019 Reproducibility Program).” Journal of Machine Learning Research 22 (164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nRosenau, James N. 1999. “A Transformed Observer in a Transforming World.” Studia Diplomatica 52 (1/2): 5–14. http://www.jstor.org/stable/44838096.\n\n\nSavage, Van, and Pamela Yeh. 2019. “Novelist Cormac McCarthy’s Tips on How to Write a Great Science Paper.” Nature 574 (7778): 441–42. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nSen, Amartya. 1980. “Description as Choice.” Oxford Economic Papers 32 (3): 353–69. https://doi.org/10.1093/oxfordjournals.oep.a041484.\n\n\nSides, John, Lynn Vavreck, and Christopher Warshaw. 2021. “The Effect of Television Advertising in United States Elections.” American Political Science Review, 1–17. https://doi.org/10.1017/s000305542100112x.\n\n\nSmith, Jessie, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. “REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research.” 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22). https://doi.org/10.1145/3531146.3533122.\n\n\nThe Economist. 2013. “Johnson: Those Six Little Rules: George Orwell on Writing,” July. https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules.\n\n\nTolley, Erin, and Mireille Paquet. 2021. “Gender, Municipal Party Politics, and Montreal’s First Woman Mayor.” Canadian Journal of Urban Research 30 (1): 40–52. https://cjur.uwinnipeg.ca/index.php/cjur/article/view/323.\n\n\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv. https://doi.org/10.48550/ARXIV.2302.13971.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nZinsser, William. 1976. On Writing Well. New York: HarperCollins."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#key-concepts-and-skills",
    "href": "lectures/lecture-09-slides.html#key-concepts-and-skills",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.1 Key concepts and skills",
    "text": "1.1 Key concepts and skills\n\nVisualization is one way to get a sense of our data and to communicate this to the reader. Plotting the observations in a dataset is important.\nWe need to be comfortable with a variety of graph types, including: bar charts, scatterplots, line plots, and histograms. We can even consider a map to be a type of graph, especially after geocoding our data.\nWe should also summarize data using tables. Typical use cases for this include showing part of a dataset, summary statistics, and regression results."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#software-and-packages",
    "href": "lectures/lecture-09-slides.html#software-and-packages",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.2 Software and packages",
    "text": "1.2 Software and packages\n\n\n\nBase R (R Core Team 2023)\ncarData (Fox, Weisberg, and Price 2022)\ndatasauRus (Davies, Locke, and D’Agostino McGowan 2022)\nggmap (Kahle and Wickham 2013)\njanitor (Firke 2023)\nknitr (Xie 2023)\nmaps (Becker et al. 2022)\nmapproj (McIlroy et al. 2023)\nmodelsummary (Arel-Bundock 2022)\nopendatatoronto (Gelfand 2022)\npatchwork (Pedersen 2022)\ntidygeocoder (Cambon and Belanger 2021)\ntidyverse (Wickham et al. 2019)\ntroopdata (Flynn 2022)\nWDI (Arel-Bundock 2021)\n\n\n\n\n\nlibrary(carData)\nlibrary(datasauRus)\nlibrary(ggmap)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(modelsummary)\nlibrary(opendatatoronto)\nlibrary(patchwork)\nlibrary(tidygeocoder)\nlibrary(tidyverse)\nlibrary(troopdata)\nlibrary(WDI)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#introduction",
    "href": "lectures/lecture-09-slides.html#introduction",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.3 Introduction",
    "text": "1.3 Introduction\nWhen telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.\nTry to show the observations that underpin our analysis. For instance, if your dataset consists of 2,500 responses to a survey, then at some point in the paper you should have a plot/s that contains each of the 2,500 observations, for every variable of interest. To do this we build graphs using ggplot2 which is part of the core tidyverse and so does not have to be installed or loaded separately. In this chapter we go through a variety of different options including bar charts, scatterplots, line plots, and histograms.\nIn contrast to the role of graphs, which is to show each observation, the role of tables is typically to show an extract of the dataset or to convey various summary statistics, or regression results. We will build tables primarily using knitr. Later we will use modelsummary to build tables related to regression output.\nFinally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using ggmap after having obtained geocoded data using tidygeocoder."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#graphs",
    "href": "lectures/lecture-09-slides.html#graphs",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.4 Graphs",
    "text": "1.4 Graphs\nGraphs are a critical aspect of compelling data stories. They allow us to see both broad patterns and details (Cleveland [1985] 1994, 5). Graphs enable a familiarity with our data that is hard to get from any other method. Every variable of interest should be graphed.\nThe most important objective of a graph is to convey as much of the actual data, and its context, as possible. In a way, graphing is an information encoding process where we construct a deliberate representation to convey information to our audience. The audience must decode that representation. The success of our graph depends on how much information is lost in this process so the decoding is a critical aspect (Cleveland [1985] 1994, 221). This means that we must focus on creating effective graphs that are suitable for our specific audience."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section",
    "href": "lectures/lecture-09-slides.html#section",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.5 ",
    "text": "1.5 \nTo see why graphing the actual data is important, after installing and loading datasauRus consider the datasaurus_dozen dataset.\n\ndatasaurus_dozen\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# ℹ 1,836 more rows\n\n\nThe dataset consists of values for “x” and “y”, which should be plotted on the x-axis and y-axis, respectively. There are 13 different values in the variable “dataset” including: “dino”, “star”, “away”, and “bullseye”. We focus on those four and generate summary statistics for each (Table 1).\n\n# Based on: https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  summarise(across(c(x, y), list(mean = mean, sd = sd)),\n            .by = dataset) |&gt;\n  kable(col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n        booktabs = TRUE, digits = 1)\n\n\n\nTable 1: Mean and standard deviation for four datasauRus datasets\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\ndino\n54.3\n16.8\n47.8\n26.9\n\n\naway\n54.3\n16.8\n47.8\n26.9\n\n\nstar\n54.3\n16.8\n47.8\n26.9\n\n\nbullseye\n54.3\n16.8\n47.8\n26.9"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-1",
    "href": "lectures/lecture-09-slides.html#section-1",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.6 ",
    "text": "1.6 \nNotice that the summary statistics are similar (Table 1). Despite this it turns out that the different datasets are actually very different beasts. This becomes clear when we plot the data (Figure 1).\n\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  ggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(color = \"Dataset\")\n\n\n\nFigure 1: Graph of four datasauRus datasets"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-2",
    "href": "lectures/lecture-09-slides.html#section-2",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.7 ",
    "text": "1.7 \nWe get a similar lesson—always plot your data—from “Anscombe’s Quartet”, created by the twentieth century statistician Frank Anscombe. The key takeaway is that it is important to plot the actual data and not rely solely on summary statistics.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-3",
    "href": "lectures/lecture-09-slides.html#section-3",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.8 ",
    "text": "1.8 \nAnscombe’s Quartet consists of eleven observations for four different datasets, with x and y values for each observation. We need to manipulate this dataset with pivot_longer() to get it into the “tidy” format discussed in ?@sec-r-essentials.\n\n# From: https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# And the pivot_longer() vignette.\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(\n    everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-4",
    "href": "lectures/lecture-09-slides.html#section-4",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.9 ",
    "text": "1.9 \nWe can first create summary statistics (Table 2) and then plot the data (Figure 2). This again illustrates the importance of graphing the actual data, rather than relying on summary statistics.\n\ntidy_anscombe |&gt;\n  summarise(\n    across(c(x, y), list(mean = mean, sd = sd)),\n    .by = set\n    ) |&gt;\n  kable(\n    col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n    digits = 1, booktabs = TRUE\n  )\n\n\n\nTable 2: Mean and standard deviation for Anscombe’s quartet\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\n1\n9\n3.3\n7.5\n2\n\n\n2\n9\n3.3\n7.5\n2\n\n\n3\n9\n3.3\n7.5\n2\n\n\n4\n9\n3.3\n7.5\n2"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-5",
    "href": "lectures/lecture-09-slides.html#section-5",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.10 ",
    "text": "1.10 \n\ntidy_anscombe |&gt;\n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\") +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 2: Recreation of Anscombe’s Quartet"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#bar-charts",
    "href": "lectures/lecture-09-slides.html#bar-charts",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.11 Bar charts",
    "text": "1.11 Bar charts\nWe typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in ?@sec-fire-hose when we constructed a graph of the number of occupied beds. The geometric object—a “geom”—that we primarily use is geom_bar(), but there are many variants to cater for specific situations. To illustrate the use of bar charts, we use a dataset from the 1997-2001 British Election Panel Study that was put together by Fox and Andersen (2006) and made available with BEPS, after installing and loading carData.\n\nbeps &lt;- \n  BEPS |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  select(age, vote, gender, political_knowledge)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-6",
    "href": "lectures/lecture-09-slides.html#section-6",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.12 ",
    "text": "1.12 \nThe dataset consists of which party the respondent supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondent. We begin by creating age-groups from the ages, and making a bar chart showing the frequency of each age-group using geom_bar() (Figure 3 (a)).\n\nbeps &lt;-\n  beps |&gt;\n  mutate(\n    age_group =\n      case_when(\n        age &lt; 35 ~ \"&lt;35\",\n        age &lt; 50 ~ \"35-49\",\n        age &lt; 65 ~ \"50-64\",\n        age &lt; 80 ~ \"65-79\",\n        age &lt; 100 ~ \"80-99\"\n      ),\n    age_group = \n      factor(age_group, levels = c(\"&lt;35\", \"35-49\", \"50-64\", \"65-79\", \"80-99\"))\n  )"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-7",
    "href": "lectures/lecture-09-slides.html#section-7",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.13 ",
    "text": "1.13 \nbeps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\nbeps |&gt; \n  count(age_group) |&gt; \n  ggplot(mapping = aes(x = age_group, y = n)) +\n  geom_col() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using count() and geom_col()\n\n\n\n\n\n\n\nFigure 3: Distribution of age-groups in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-8",
    "href": "lectures/lecture-09-slides.html#section-8",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.14 ",
    "text": "1.14 \nThe default axis label used by ggplot2 is the name of the relevant variable, so it is often useful to add more detail. We do this using labs() by specifying a variable and a name. In the case of Figure 3 (a) we have specified labels for the x-axis and y-axis.\nBy default, geom_bar() creates a count of the number of times each age-group appears in the dataset. It does this because the default statistical transformation—a “stat”—for geom_bar() is “count”, which saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with beps |&gt; count(age_group)), then we could specify a variable for the y-axis and then use geom_col() (Figure 3 (b))."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-9",
    "href": "lectures/lecture-09-slides.html#section-9",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.15 ",
    "text": "1.15 \nWe may also like to consider various groupings of the data to get a different insight. For instance, we can use color to look at which party the respondent supports, by age-group (Figure 4 (a)).\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge2\") +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using geom_bar() with dodge2\n\n\n\n\n\n\n\nFigure 4: Distribution of age-group, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\nBy default, these different groups are stacked, but they can be placed side by side with position = \"dodge2\" (Figure 4 (b)). (Using “dodge2” rather than “dodge” adds a little space between the bars.)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#themes",
    "href": "lectures/lecture-09-slides.html#themes",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.16 Themes",
    "text": "1.16 Themes\nAt this point, we may like to address the general look of the graph. There are various themes that are built into ggplot2. These include: theme_bw(), theme_classic(), theme_dark(), and theme_minimal(). A full list is available in the ggplot2 cheat sheet. We can use these themes by adding them as a layer (Figure 5). We could also install more themes from other packages, including ggthemes (Arnold 2021), and hrbrthemes (Rudis 2020). We could even build our own!\n\ntheme_bw &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\n\n\n\nFigure 5: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes and the use of patchwork\nIn Figure 5 we use patchwork to bring together multiple graphs. To do this, after installing and loading the package, we assign the graph to a variable. We then use “+” to signal which should be next to each other, “/” to signal which should be on top, and use brackets to indicate precedence"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#facets",
    "href": "lectures/lecture-09-slides.html#facets",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.17 Facets",
    "text": "1.17 Facets\nWe use facets to show variation, based on one or more variables (Wilkinson 2005, 219). Facets are especially useful when we have already used color to highlight variation in some other variable. For instance, we may be interested to explain vote, by age and gender (Figure 6). We rotate the x-axis with guides(x = guide_axis(angle = 90)) to avoid overlapping. We also change the position of the legend with theme(legend.position = \"bottom\").\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 6: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-10",
    "href": "lectures/lecture-09-slides.html#section-10",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.18 ",
    "text": "1.18 \nWe could change facet_wrap() to wrap vertically instead of horizontally with dir = \"v\". Alternatively, we could specify a few rows, say nrow = 2, or a number of columns, say ncol = 2.\nBy default, both facets will have the same x-axis and y-axis. We could enable both facets to have different scales with scales = \"free\", or just the x-axis with scales = \"free_x\", or just the y-axis with scales = \"free_y\" (Figure 7).\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote), scales = \"free\") +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 7: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-11",
    "href": "lectures/lecture-09-slides.html#section-11",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.19 ",
    "text": "1.19 \nFinally, we can change the labels of the facets using labeller() (Figure 8).\n\nnew_labels &lt;- \n  c(\"0\" = \"No knowledge\", \"1\" = \"Low knowledge\",\n    \"2\" = \"Moderate knowledge\", \"3\" = \"High knowledge\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(\n    vars(political_knowledge),\n    scales = \"free\",\n    labeller = labeller(political_knowledge = new_labels)\n  ) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 8: Distribution of age-group by political knowledge, and vote preference, in the 1997-2001 British Election Panel Study\nWe now have three ways to combine multiple graphs: sub-figures, facets, and patchwork. They are useful in different circumstances:\n\nsub-figures—which we covered in ?@sec-reproducible-workflows—for when we are considering different variables;\nfacets for when we are considering a categorical variable; and\npatchwork for when we are interested in bringing together entirely different graphs."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#colors",
    "href": "lectures/lecture-09-slides.html#colors",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.20 Colors",
    "text": "1.20 Colors\nWe now turn to the colors used in the graph. There are a variety of different ways to change the colors. The many palettes available from RColorBrewer (Neuwirth 2022) can be specified using scale_fill_brewer(). In the case of viridis (Garnier et al. 2021) we can specify the palettes using scale_fill_viridis_d(). Additionally, viridis is particularly focused on color-blind palettes (Figure 9). Neither RColorBrewer nor viridis need to be explicitly installed or loaded because ggplot2, which is part of the tidyverse, takes care of that for us."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-12",
    "href": "lectures/lecture-09-slides.html#section-12",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.21 ",
    "text": "1.21 \n# Panel (a)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Blues\")\n\n# Panel (b)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Panel (c)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d()\n\n# Panel (d)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 9: Distribution of age-group and vote preference, in the 1997-2001 British Election Panel Study, illustrating different colors"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#scatterplots",
    "href": "lectures/lecture-09-slides.html#scatterplots",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.22 Scatterplots",
    "text": "1.22 Scatterplots\nWe are often interested in the relationship between two numeric or continuous variables. We can use scatterplots to show this. A scatterplot may not always be the best choice, but it is rarely a bad one (Weissgerber et al. 2015). Some consider it the most versatile and useful graph option (Friendly and Wainer 2021, 121). To illustrate scatterplots, we install and load WDI and then use that to download some economic indicators from the World Bank. In particular, we use WDIsearch() to find the unique key that we need to pass to WDI() to facilitate the download."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-13",
    "href": "lectures/lecture-09-slides.html#section-13",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.23 ",
    "text": "1.23 \n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\nFrom OECD (2014, 15) Gross Domestic Product (GDP) “combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country’s economic territory.” The modern concept was developed by the twentieth century economist Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the economic activity of a country. It is useful and informative that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and disaggregated differences can be important (Moyer and Dunn 2020). It highlights short term economic progress over longer term improvements. And “the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable” (Kuznets, Epstein, and Jenks 1941, xxvi). Summary measures of economic performance shows only one side of a country’s economy. While there are many strengths there are also well-known areas where GDP is weak."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-14",
    "href": "lectures/lecture-09-slides.html#section-14",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.24 ",
    "text": "1.24 \n\nWDIsearch(\"gdp growth\")\nWDIsearch(\"inflation\")\nWDIsearch(\"population, total\")\nWDIsearch(\"Unemployment, total\")"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-15",
    "href": "lectures/lecture-09-slides.html#section-15",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.25 ",
    "text": "1.25 \n\nworld_bank_data &lt;-\n  WDI(\n    indicator =\n      c(\"FP.CPI.TOTL.ZG\", \"NY.GDP.MKTP.KD.ZG\", \"SP.POP.TOTL\",\"SL.UEM.TOTL.NE.ZS\"),\n    country = c(\"AU\", \"ET\", \"IN\", \"US\")\n  )"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-16",
    "href": "lectures/lecture-09-slides.html#section-16",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.26 ",
    "text": "1.26 \nWe may like to change the variable names to be more meaningful, and only keep those that we need.\n\nworld_bank_data &lt;-\n  world_bank_data |&gt;\n  rename(\n    inflation = FP.CPI.TOTL.ZG,\n    gdp_growth = NY.GDP.MKTP.KD.ZG,\n    population = SP.POP.TOTL,\n    unem_rate = SL.UEM.TOTL.NE.ZS\n  ) |&gt;\n  select(country, year, inflation, gdp_growth, population, unem_rate)\n\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unem_rate\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia  1960     3.73       NA      10276477        NA\n2 Australia  1961     2.29        2.48   10483000        NA\n3 Australia  1962    -0.319       1.29   10742000        NA\n4 Australia  1963     0.641       6.22   10950000        NA\n5 Australia  1964     2.87        6.98   11167000        NA\n6 Australia  1965     3.41        5.98   11388000        NA"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-17",
    "href": "lectures/lecture-09-slides.html#section-17",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.27 ",
    "text": "1.27 \nTo get started we can use geom_point() to make a scatterplot showing GDP growth and inflation, by country (Figure 10 (a)).\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default settings\n\n\n\n\n\n\n\n\n\n\n\n(b) With the addition of a theme and labels\n\n\n\n\n\n\n\nFigure 10: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nAs with bar charts, we can change the theme, and update the labels (Figure 10 (b))."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-18",
    "href": "lectures/lecture-09-slides.html#section-18",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.28 ",
    "text": "1.28 \nFor scatterplots we use “color” instead of “fill”, as we did for bar charts, because they use dots rather than bars. This also then slightly affects how we change the palette (Figure 11). That said, with particular types of dots, for instance shape = 21, it is possible to have both fill and color aesthetics.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Blues\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d()\n\n# Panel (d)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 11: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-19",
    "href": "lectures/lecture-09-slides.html#section-19",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.29 ",
    "text": "1.29 \nThe points of a scatterplot sometimes overlap. We can address this situation in a variety of ways (Figure 12):\n\nAdding a degree of transparency to our dots with “alpha” (Figure 12 (a)). The value for “alpha” can vary between 0, which is fully transparent, and 1, which is completely opaque.\nAdding a small amount of noise, which slightly moves the points, using geom_jitter() (Figure 12 (b)). By default, the movement is uniform in both directions, but we can specify which direction movement occurs with “width” or “height”. The decision between these two options turns on the degree to which accuracy matters, and the number of points: it is often useful to use geom_jitter() when you want to highlight the relative density of points and not necessarily the exact value of individual points. When using geom_jitter() it is a good idea to set a seed, as introduced in ?@sec-fire-hose, for reproducibility."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-20",
    "href": "lectures/lecture-09-slides.html#section-20",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.30 ",
    "text": "1.30 \nset.seed(853)\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country )) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter(width = 1, height = 1) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Changing the alpha setting\n\n\n\n\n\n\n\n\n\n\n\n(b) Using jitter\n\n\n\n\n\n\n\nFigure 12: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-21",
    "href": "lectures/lecture-09-slides.html#section-21",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.31 ",
    "text": "1.31 \nWe often use scatterplots to illustrate a relationship between two continuous variables. It can be useful to add a “summary” line using geom_smooth() (Figure 13). We can specify the relationship using “method”, change the color with “color”, and add or remove standard errors with “se”. A commonly used “method” is lm, which computes and plots a simple linear regression line similar to using the lm() function. Using geom_smooth() adds a layer to the graph, and so it inherits aesthetics from ggplot(). For instance, that is why we have one line for each country in Figure 13 (a) and Figure 13 (b). We could overwrite that by specifying a particular color (Figure 13 (c)). There are situation where other types of fitted lines such as splines might be preferred.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default line of best fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Specifying a linear relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Specifying only one color\n\n\n\n\n\n\n\nFigure 13: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#line-plots",
    "href": "lectures/lecture-09-slides.html#line-plots",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.32 Line plots",
    "text": "1.32 Line plots\nWe can use a line plot when we have variables that should be joined together, for instance, an economic time series. We will continue with the dataset from the World Bank and focus on GDP growth in the United States using geom_line() (Figure 14 (a)). The source of the data can be added to the graph using “caption” within labs().\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using a line plot\n\n\n\n\n\n\n\n\n\n\n\n(b) Using a stairstep line plot\n\n\n\n\n\n\n\nFigure 14: United States GDP growth (1961-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-22",
    "href": "lectures/lecture-09-slides.html#section-22",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.33 ",
    "text": "1.33 \nWe can use geom_step(), a slight variant of geom_line(), to focus attention on the change from year to year (Figure 14 (b)).\nThe Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the United Kingdom between 1861 and 1957 (Phillips 1958). We have a variety of ways to investigate this relationship in our data, including:\n\nAdding a second line to our graph. For instance, we could add inflation (Figure 15 (a)). This requires us to use pivot_longer(), which is discussed in ?@sec-r-essentials, to ensure that the data are in a tidy format.\nUsing geom_path() to link values in the order they appear in the dataset. In Figure 15 (b) we show a Phillips curve for the United States between 1960 and 2020. Figure 15 (b) does not appear to show any clear relationship between unemployment and inflation.\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  select(-population, -gdp_growth) |&gt;\n  pivot_longer(\n    cols = c(\"inflation\", \"unem_rate\"),\n    names_to = \"series\",\n    values_to = \"value\"\n  ) |&gt;\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Year\", y = \"Value\", color = \"Economic indicator\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = unem_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(\n    x = \"Unemployment rate\", y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparing the two time series over time\n\n\n\n\n\n\n\n\n\n\n\n(b) Plotting the two time series against each other\n\n\n\n\n\n\n\nFigure 15: Unemployment and inflation for the United States (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#histograms",
    "href": "lectures/lecture-09-slides.html#histograms",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.34 Histograms",
    "text": "1.34 Histograms\nA histogram is useful to show the shape of the distribution of a continuous variable. The full range of the data values is split into intervals called “bins” and the histogram counts how many observations fall into which bin. In Figure 16 we examine the distribution of GDP in Ethiopia.\n\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\nFigure 16: Distribution of GDP growth in Ethiopia (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-23",
    "href": "lectures/lecture-09-slides.html#section-23",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.35 ",
    "text": "1.35 \nThe key component that determines the shape of a histogram is the number of bins. This can be specified in one of two ways (Figure 17):\n\nspecifying the number of “bins” to include; or\nspecifying their “binwidth”.\n\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (d)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Five bins\n\n\n\n\n\n\n\n\n\n\n\n(b) 20 bins\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Binwidth of two\n\n\n\n\n\n\n\n\n\n\n\n(d) Binwidth of five\n\n\n\n\n\n\n\nFigure 17: Distribution of GDP growth in Ethiopia (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-24",
    "href": "lectures/lecture-09-slides.html#section-24",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.36 ",
    "text": "1.36 \nHistograms can be thought of as locally averaging data, and the number of bins affects how much of this occurs. When there are only two bins then there is considerable smoothing, but we lose a lot of accuracy. Too few bins results in more bias, while too many bins results in more variance (Wasserman 2005, 303). Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal (Cleveland [1985] 1994, 135). This is one of the reasons that Denby and Mallows (2009) consider histograms to be especially valuable as exploratory tools."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-25",
    "href": "lectures/lecture-09-slides.html#section-25",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.37 ",
    "text": "1.37 \nFinally, while we can use “fill” to distinguish between different types of observations, it can get quite messy. It is usually better to:\n\ntrace the outline of the distribution with geom_freqpoly() (Figure 18 (a))\nbuild stack of dots with geom_dotplot() (Figure 18 (b)); or\nadd transparency, especially if the differences are more stark (Figure 18 (c)).\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = \"histodot\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country %in% c(\"India\", \"United States\")) |&gt;\n  ggplot(mapping = aes(x = gdp_growth, fill = country)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tracing the outline\n\n\n\n\n\n\n\n\n\n\n\n(b) Using dots\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding transparency\n\n\n\n\n\n\n\nFigure 18: Distribution of GDP growth across various countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-26",
    "href": "lectures/lecture-09-slides.html#section-26",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.38 ",
    "text": "1.38 \nAn interesting alternative to a histogram is the empirical cumulative distribution function (ECDF). The choice between this and a histogram is tends to be audience-specific. It may not appropriate for less-sophisticated audiences, but if the audience is quantitatively comfortable, then it can be a great choice because it does less smoothing than a histogram. We can build an ECDF with stat_ecdf(). For instance, Figure 19 shows an ECDF equivalent to Figure 16.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  stat_ecdf(geom = \"point\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Proportion\", color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\nFigure 19: Distribution of GDP growth in four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#boxplots",
    "href": "lectures/lecture-09-slides.html#boxplots",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.39 Boxplots",
    "text": "1.39 Boxplots\nA boxplot typically shows five aspects: 1) the median, 2) the 25th, and 3) 75th percentiles. The fourth and fifth elements differ depending on specifics. One option is the minimum and maximum values. Another option is to determine the difference between the 75th and 25th percentiles, which is the interquartile range (IQR). The fourth and fifth elements are then the extreme observations within \\(1.5\\times\\mbox{IQR}\\) from the 25th and 75th percentiles. That latter approach is used, by default, in geom_boxplot from ggplot2. Spear (1952, 166) introduced the notion of a chart that focused on the range and various summary statistics including the median and the range, while Tukey (1977) focused on which summary statistics and popularized it (Wickham and Stryjewski 2011).\nOne reason for using graphs is that they help us understand and embrace how complex our data are, rather than trying to hide and smooth it away (Armstrong 2022). One appropriate use case for boxplots is to compare the summary statistics of many variables at once, such as in Bethlehem et al. (2022). But boxplots alone are rarely the best choice because they hide the distribution of data, rather than show it. The same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. The first contains draws from two beta distributions: one that is right skewed and another that is left skewed. The second contains draws from a beta distribution with no skew, noting that \\(\\mbox{Beta}(1, 1)\\) is equivalent to \\(\\mbox{Uniform}(0, 1)\\)."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-27",
    "href": "lectures/lecture-09-slides.html#section-27",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.40 ",
    "text": "1.40 \n\nset.seed(853)\n\nnumber_of_draws &lt;- 10000\n\nboth_left_and_right_skew &lt;-\n  c(\n    rbeta(number_of_draws / 2, 5, 2),\n    rbeta(number_of_draws / 2, 2, 5)\n  )\n\nno_skew &lt;-\n  rbeta(number_of_draws, 1, 1)\n\nbeta_distributions &lt;-\n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(\n      rep(\"Left and right skew\", number_of_draws),\n      rep(\"No skew\", number_of_draws)\n    )\n  )"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-28",
    "href": "lectures/lecture-09-slides.html#section-28",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.41 ",
    "text": "1.41 \nWe can first compare the boxplots of the two series (Figure 20 (a)). But if we plot the actual data then we can see how different they are (Figure 20 (b)).\nbeta_distributions |&gt;\n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\n\nbeta_distributions |&gt;\n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Illustrated with a boxplot\n\n\n\n\n\n\n\n\n\n\n\n(b) Actual data\n\n\n\n\n\n\n\nFigure 20: Data drawn from beta distributions with different parameters"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-29",
    "href": "lectures/lecture-09-slides.html#section-29",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.42 ",
    "text": "1.42 \nOne way forward, if a boxplot is to be used, is to include the actual data as a layer on top of the boxplot. For instance, in Figure 21 we show the distribution of inflation across the four countries. The reason that this works well is that it shows the actual observations, as well as the summary statistics.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Country\",\n    y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\nFigure 21: Distribution of inflation data for four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-30",
    "href": "lectures/lecture-09-slides.html#section-30",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.1 ",
    "text": "2.1 \nTables are an important part of telling a compelling story. Tables can communicate less information than a graph, but they do so at a high fidelity. They are especially useful to highlight a few specific values (Andersen and Armstrong 2021). In this book, we primarily use tables in three ways:\n\nTo show an extract of the dataset.\nTo communicate summary statistics.\nTo display regression results."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#showing-part-of-a-dataset",
    "href": "lectures/lecture-09-slides.html#showing-part-of-a-dataset",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.2 Showing part of a dataset",
    "text": "2.2 Showing part of a dataset\nWe illustrate showing part of a dataset using kable() from knitr. We use the World Bank dataset that we downloaded earlier and focus on inflation, GDP growth, and population as unemployment data are not available for every year for every country.\n\nworld_bank_data &lt;- \n  world_bank_data |&gt; \n  select(-unem_rate)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-31",
    "href": "lectures/lecture-09-slides.html#section-31",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.3 ",
    "text": "2.3 \nTo begin, after installing and loading knitr, we can display the first ten rows with the default kable() settings.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable()\n\n\n\n\ncountry\nyear\ninflation\ngdp_growth\npopulation\n\n\n\n\nAustralia\n1960\n3.7288136\nNA\n10276477\n\n\nAustralia\n1961\n2.2875817\n2.482656\n10483000\n\n\nAustralia\n1962\n-0.3194888\n1.294611\n10742000\n\n\nAustralia\n1963\n0.6410256\n6.216107\n10950000\n\n\nAustralia\n1964\n2.8662420\n6.980061\n11167000\n\n\nAustralia\n1965\n3.4055728\n5.980438\n11388000\n\n\nAustralia\n1966\n3.2934132\n2.379040\n11651000\n\n\nAustralia\n1967\n3.4782609\n6.304945\n11799000\n\n\nAustralia\n1968\n2.5210084\n5.094034\n12009000\n\n\nAustralia\n1969\n3.2786885\n7.045584\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-32",
    "href": "lectures/lecture-09-slides.html#section-32",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.4 ",
    "text": "2.4 \nTo be able to cross-reference a table in the text, we need to add a table caption and label to the R chunk as shown in ?@sec-quartocrossreferences of ?@sec-reproducible-workflows. We can also make the column names more informative with “col.names” and specify the number of digits to be displayed (Table 3).\n\n```{r}\n#| label: tbl-gdpfirst\n#| message: false\n#| tbl-cap: \"A dataset of economic indicators for four countries\"\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1\n  )\n```\n\n\n\nTable 3: A dataset of economic indicators for four countries\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-33",
    "href": "lectures/lecture-09-slides.html#section-33",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.5 ",
    "text": "2.5 \n2.5.1 Improving the formatting\nWhen producing PDFs, the “booktabs” option makes a host of small changes to the default display and results in tables that look better (Table 4). (This should not have an effect for HTML output.) By default a small space will be added every five lines. We can additionally specify “linesep” to stop that.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 4: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-34",
    "href": "lectures/lecture-09-slides.html#section-34",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.6 ",
    "text": "2.6 \nWe can specify the alignment of the columns using a character vector of “l” (left), “c” (center), and “r” (right) (Table 5). Additionally, we can change the formatting. For instance, we could specify groupings for numbers that are at least 1,000 using format.args = list(big.mark = \",\").\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  mutate(year = as.factor(year)) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\",\n    align = c(\"l\", \"l\", \"c\", \"c\", \"r\", \"r\"),\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 5: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10,276,477\n\n\nAustralia\n1961\n2.3\n2.5\n10,483,000\n\n\nAustralia\n1962\n-0.3\n1.3\n10,742,000\n\n\nAustralia\n1963\n0.6\n6.2\n10,950,000\n\n\nAustralia\n1964\n2.9\n7.0\n11,167,000\n\n\nAustralia\n1965\n3.4\n6.0\n11,388,000\n\n\nAustralia\n1966\n3.3\n2.4\n11,651,000\n\n\nAustralia\n1967\n3.5\n6.3\n11,799,000\n\n\nAustralia\n1968\n2.5\n5.1\n12,009,000\n\n\nAustralia\n1969\n3.3\n7.0\n12,263,000"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#communicating-summary-statistics",
    "href": "lectures/lecture-09-slides.html#communicating-summary-statistics",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.7 Communicating summary statistics",
    "text": "2.7 Communicating summary statistics\nAfter installing and loading modelsummary we can use datasummary_skim() to create tables of summary statistics from our dataset.\nWe can use this to get a table such as Table 6. That might be useful for exploratory data analysis, which we cover in ?@sec-exploratory-data-analysis. (Here we remove population to save space and do not include a histogram of each variable.)\n\nworld_bank_data |&gt;\n  select(-population) |&gt; \n  datasummary_skim(histogram = FALSE)\n\n\n\nTable 6: Summary of economic indicator variables for four countries\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n              \n        \n        \n        \n                \n                  year\n                  62\n                  0\n                  1990.5\n                  17.9\n                  1960.0\n                  1990.5\n                  2021.0\n                \n                \n                  inflation\n                  243\n                  2\n                  6.1\n                  6.5\n                  -9.8\n                  4.3\n                  44.4\n                \n                \n                  gdp_growth\n                  224\n                  10\n                  4.2\n                  3.7\n                  -11.1\n                  3.9\n                  13.9\n                \n                \n                  country\n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Australia\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Ethiopia\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  India\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  United States\n                  62\n                  25.0"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-35",
    "href": "lectures/lecture-09-slides.html#section-35",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.8 ",
    "text": "2.8 \nBy default, datasummary_skim() summarizes the numeric variables, but we can ask for the categorical variables (Table 7). Additionally we can add cross-references in the same way as kable(), that is, include a “tbl-cap” entry and then cross-reference the name of the R chunk.\n\nworld_bank_data |&gt;\n  datasummary_skim(type = \"categorical\")\n\n\n\nTable 7: Summary of categorical economic indicator variables for four countries\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                country\n                N\n                %\n              \n        \n        \n        \n                \n                  Australia    \n                  62\n                  25.0\n                \n                \n                  Ethiopia     \n                  62\n                  25.0\n                \n                \n                  India        \n                  62\n                  25.0\n                \n                \n                  United States\n                  62\n                  25.0"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-36",
    "href": "lectures/lecture-09-slides.html#section-36",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.9 ",
    "text": "2.9 \nWe can create a table that shows the correlation between variables using datasummary_correlation() (Table 8).\n\nworld_bank_data |&gt;\n  datasummary_correlation()\n\n\n\nTable 8: Correlation between the economic indicator variables for four countries (Australia, Ethiopia, India, and the United States)\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                year\n                inflation\n                gdp_growth\n                population\n              \n        \n        \n        \n                \n                  year      \n                  1  \n                  .  \n                  .  \n                  .\n                \n                \n                  inflation \n                  .03\n                  1  \n                  .  \n                  .\n                \n                \n                  gdp_growth\n                  .11\n                  .01\n                  1  \n                  .\n                \n                \n                  population\n                  .25\n                  .06\n                  .16\n                  1"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-37",
    "href": "lectures/lecture-09-slides.html#section-37",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.10 ",
    "text": "2.10 \nWe typically need a table of descriptive statistics that we could add to our paper (Table 9). This contrasts with Table 7 which would likely not be included in the main section of a paper, and is more to help us understand the data. We can add a note about the source of the data using notes.\n\ndatasummary_balance(\n  formula = ~country,\n  data = world_bank_data |&gt; \n    filter(country %in% c(\"Australia\", \"Ethiopia\")),\n  dinm = FALSE,\n  notes = \"Data source: World Bank.\"\n)\n\n\n\nTable 9: Descriptive statistics for the inflation and GDP dataset\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nAustralia (N=62)\nEthiopia (N=62)\n\n        \n              \n                 \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n              \n        \n        Data source: World Bank.\n        \n                \n                  year      \n                  1990.5    \n                  18.0     \n                  1990.5    \n                  18.0      \n                \n                \n                  inflation \n                  4.7       \n                  3.8      \n                  9.1       \n                  10.6      \n                \n                \n                  gdp_growth\n                  3.4       \n                  1.8      \n                  5.9       \n                  6.4       \n                \n                \n                  population\n                  17351313.1\n                  4407899.0\n                  57185292.0\n                  29328845.8"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#display-regression-results",
    "href": "lectures/lecture-09-slides.html#display-regression-results",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.11 Display regression results",
    "text": "2.11 Display regression results\nWe can report regression results using modelsummary() from modelsummary. For instance, we could display the estimates from a few different models (Table 10).\n\nfirst_model &lt;- lm(\n  formula = gdp_growth ~ inflation,\n  data = world_bank_data\n)\n\nsecond_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country,\n  data = world_bank_data\n)\n\nthird_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country + population,\n  data = world_bank_data\n)\n\nmodelsummary(list(first_model, second_model, third_model))\n\n\n\nTable 10: Explaining GDP as a function of inflation\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  4.147   \n                  3.676   \n                  3.611   \n                \n                \n                                      \n                  (0.343) \n                  (0.484) \n                  (0.482) \n                \n                \n                  inflation           \n                  0.006   \n                  -0.068  \n                  -0.065  \n                \n                \n                                      \n                  (0.039) \n                  (0.040) \n                  (0.039) \n                \n                \n                  countryEthiopia     \n                          \n                  2.896   \n                  2.716   \n                \n                \n                                      \n                          \n                  (0.740) \n                  (0.740) \n                \n                \n                  countryIndia        \n                          \n                  1.916   \n                  -0.730  \n                \n                \n                                      \n                          \n                  (0.642) \n                  (1.465) \n                \n                \n                  countryUnited States\n                          \n                  -0.436  \n                  -1.145  \n                \n                \n                                      \n                          \n                  (0.633) \n                  (0.722) \n                \n                \n                  population          \n                          \n                          \n                  0.000   \n                \n                \n                                      \n                          \n                          \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  223     \n                  223     \n                  223     \n                \n                \n                  R2                  \n                  0.000   \n                  0.111   \n                  0.127   \n                \n                \n                  R2 Adj.             \n                  -0.004  \n                  0.095   \n                  0.107   \n                \n                \n                  AIC                 \n                  1217.7  \n                  1197.5  \n                  1195.4  \n                \n                \n                  BIC                 \n                  1227.9  \n                  1217.9  \n                  1219.3  \n                \n                \n                  Log.Lik.            \n                  -605.861\n                  -592.752\n                  -590.704\n                \n                \n                  F                   \n                  0.024   \n                  6.806   \n                          \n                \n                \n                  RMSE                \n                  3.66    \n                  3.45    \n                  3.42    \n                \n        \n      \n    \n\n\n\n\n\n\nThe number of significant digits can be adjusted with “fmt” (Table 11). To help establish credibility you should generally not add as many significant digits as possible (Howes 2022). Instead, you should think carefully about the data-generating process and adjust based on that."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-38",
    "href": "lectures/lecture-09-slides.html#section-38",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.12 ",
    "text": "2.12 \n\nmodelsummary(\n  list(first_model, second_model, third_model),\n  fmt = 1\n)\n\n\n\nTable 11: Three models of GDP as a function of inflation\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  4.1     \n                  3.7     \n                  3.6     \n                \n                \n                                      \n                  (0.3)   \n                  (0.5)   \n                  (0.5)   \n                \n                \n                  inflation           \n                  0.0     \n                  -0.1    \n                  -0.1    \n                \n                \n                                      \n                  (0.0)   \n                  (0.0)   \n                  (0.0)   \n                \n                \n                  countryEthiopia     \n                          \n                  2.9     \n                  2.7     \n                \n                \n                                      \n                          \n                  (0.7)   \n                  (0.7)   \n                \n                \n                  countryIndia        \n                          \n                  1.9     \n                  -0.7    \n                \n                \n                                      \n                          \n                  (0.6)   \n                  (1.5)   \n                \n                \n                  countryUnited States\n                          \n                  -0.4    \n                  -1.1    \n                \n                \n                                      \n                          \n                  (0.6)   \n                  (0.7)   \n                \n                \n                  population          \n                          \n                          \n                  0.0     \n                \n                \n                                      \n                          \n                          \n                  (0.0)   \n                \n                \n                  Num.Obs.            \n                  223     \n                  223     \n                  223     \n                \n                \n                  R2                  \n                  0.000   \n                  0.111   \n                  0.127   \n                \n                \n                  R2 Adj.             \n                  -0.004  \n                  0.095   \n                  0.107   \n                \n                \n                  AIC                 \n                  1217.7  \n                  1197.5  \n                  1195.4  \n                \n                \n                  BIC                 \n                  1227.9  \n                  1217.9  \n                  1219.3  \n                \n                \n                  Log.Lik.            \n                  -605.861\n                  -592.752\n                  -590.704\n                \n                \n                  F                   \n                  0.024   \n                  6.806   \n                          \n                \n                \n                  RMSE                \n                  3.66    \n                  3.45    \n                  3.42"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-39",
    "href": "lectures/lecture-09-slides.html#section-39",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.1 ",
    "text": "3.1 \nIn many ways maps can be thought of as another type of graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or background image. It is possible that they are the oldest and best understood type of chart (Karsten 1923, 1). We can generate a map in a straight-forward manner. That said, it is not to be taken lightly; things quickly get complicated!\nThe first step is to get some data. There is some geographic data built into ggplot2 that we can access with map_data(). There are additional variables in the world.cities dataset from maps.\n\nfrance &lt;- map_data(map = \"france\")\n\nhead(france)\n\n      long      lat group order region subregion\n1 2.557093 51.09752     1     1   Nord      &lt;NA&gt;\n2 2.579995 51.00298     1     2   Nord      &lt;NA&gt;\n3 2.609101 50.98545     1     3   Nord      &lt;NA&gt;\n4 2.630782 50.95073     1     4   Nord      &lt;NA&gt;\n5 2.625894 50.94116     1     5   Nord      &lt;NA&gt;\n6 2.597699 50.91967     1     6   Nord      &lt;NA&gt;\n\nfrench_cities &lt;-\n  world.cities |&gt;\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n\n             name country.etc    pop   lat long capital\n1       Abbeville      France  26656 50.12 1.83       0\n2         Acheres      France  23219 48.97 2.06       0\n3            Agde      France  23477 43.33 3.46       0\n4            Agen      France  34742 44.20 0.62       0\n5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n6 Aix-en-Provence      France 148622 43.53 5.44       0"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-40",
    "href": "lectures/lecture-09-slides.html#section-40",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.2 ",
    "text": "3.2 \nUsing that information you can create a map of France that shows the larger cities (Figure 22). Use geom_polygon() from ggplot2 to draw shapes by connecting points within groups. And coord_map() adjusts for the fact that we are making a 2D map to represent a world that is 3D.\n\nggplot() +\n  geom_polygon(\n    data = france,\n    aes(x = long, y = lat, group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map() +\n  geom_point(\n    aes(x = french_cities$long, y = french_cities$lat),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  theme_minimal() +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n\nFigure 22: Map of France showing the largest cities"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-41",
    "href": "lectures/lecture-09-slides.html#section-41",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.3 ",
    "text": "3.3 \nAs is often the case with R, there are many ways to get started creating static maps. We have seen how they can be built using only ggplot2, but ggmap brings additional functionality.\nThere are two essential components to a map:\n\na border or background image (sometimes called a tile); and\nsomething of interest within that border, or on top of that tile.\n\nIn ggmap, we use an open-source option for our tile, Stamen Maps. And we use plot points based on latitude and longitude."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#australian-polling-places",
    "href": "lectures/lecture-09-slides.html#australian-polling-places",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.4 Australian polling places",
    "text": "3.4 Australian polling places\nIn Australia, people have to go to “booths” in order to vote. Because the booths have coordinates (latitude and longitude), we can plot them. One reason we may like to do that is to notice spatial voting patterns.\nTo get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap. The main argument to this function is to specify a bounding box. A bounding box is the coordinates of the edges that you are interested in. This requires two latitudes and two longitudes."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-42",
    "href": "lectures/lecture-09-slides.html#section-42",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.5 ",
    "text": "3.5 \nIt can be useful to use Google Maps, or other mapping platform, to find the coordinate values that you need. In this case we have provided it with coordinates such that it will be centered around Australia’s capital Canberra.\n\nbbox &lt;- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-43",
    "href": "lectures/lecture-09-slides.html#section-43",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.6 ",
    "text": "3.6 \nIt is free, but we need to register in order to get a map. To do this go to https://client.stadiamaps.com/signup/ and create an account. Then create a new property, then “Add API Key”. Copy the key and run (replacing PUT-KEY-HERE with the key) register_stadiamaps(key = \"PUT-KEY-HERE\", write = TRUE). Then once you have defined the bounding box, the function get_stadiamap() will get the tiles in that area (?@fig-heyitscanberra). The number of tiles that it needs depends on the zoom, and the type of tiles that it gets depends on the type of map. We have used “toner-lite”, which is black and white, but there are others including: “terrain”, “toner”, and “toner-lines”. We pass the tiles to ggmap() which will plot it. An internet connection is needed for this to work as get_stadiamap() downloads the tiles.\n\ncanberra_stamen_map &lt;- get_stadiamap(bbox, zoom = 11, maptype = \"stamen_toner_lite\")\n\nggmap(canberra_stamen_map)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-44",
    "href": "lectures/lecture-09-slides.html#section-44",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.7 ",
    "text": "3.7 \nOnce we have a map then we can use ggmap() to plot it. Now we want to get some data that we plot on top of our tiles. We will plot the location of the polling place based on its “division”. This is available from the Australian Electoral Commission (AEC).\n\nbooths &lt;-\n  read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-45",
    "href": "lectures/lecture-09-slides.html#section-45",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.8 ",
    "text": "3.8 \nThis dataset is for the whole of Australia, but as we are only plotting the area around Canberra, we will filter the data to only booths with a geography close to Canberra.\n\nbooths_reduced &lt;-\n    booths |&gt;\n    filter(State == \"ACT\") |&gt;\n    select(PollingPlaceID, DivisionNm, Latitude, Longitude) |&gt;\n    filter(!is.na(Longitude)) |&gt; # Remove rows without geography\n    filter(Longitude &lt; 165) # Remove Norfolk Island\n\nNow we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest.\n\nggmap(canberra_stamen_map, extent = \"normal\", maprange = FALSE) +\n    geom_point(\n        data = booths_reduced,\n        aes(x = Longitude, y = Latitude, colour = DivisionNm),\n        alpha = 0.7\n    ) +\n    scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n    coord_map(\n        projection = \"mercator\",\n        xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n        ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n    ) +\n    labs(\n        x = \"Longitude\",\n        y = \"Latitude\"\n    ) +\n    theme_minimal() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()```\n\n##\n\nWe may like to save the map so that we do not have to create it every time, and we can do that in the same way as any other graph, using `ggsave()`.\n\n\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")\n\nFinally, the reason that we used Stamen Maps and OpenStreetMap is because they are open source, but we could have also used Google Maps. This requires you to first register a credit card with Google, and specify a key, but with low usage the service should be free. Using Google Maps—by using get_googlemap() within ggmap—brings some advantages over get_stadiamap(). For instance it will attempt to find a place name rather than needing to specify a bounding box."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#united-states-military-bases",
    "href": "lectures/lecture-09-slides.html#united-states-military-bases",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.9 United States military bases",
    "text": "3.9 United States military bases\nTo see another example of a static map we will plot some United States military bases after installing and loading troopdata. We can access data about United States overseas military bases back to the start of the Cold War using get_basedata().\n\nbases &lt;- get_basedata()\n\nhead(bases)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-46",
    "href": "lectures/lecture-09-slides.html#section-46",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.10 ",
    "text": "3.10 \nWe will look at the locations of United States military bases in Germany, Japan, and Australia. The troopdata dataset already has the latitude and longitude of each base, and we will use that as our item of interest. The first step is to define a bounding box for each country.\n\n# Use: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany &lt;- c(left = 5.867, bottom = 45.967, right = 15.033, top = 55.133)\n\nbbox_japan &lt;- c(left = 127, bottom = 30, right = 146, top = 45)\n\nbbox_australia &lt;- c(left = 112.467, bottom = -45, right = 155, top = -9.133)"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-47",
    "href": "lectures/lecture-09-slides.html#section-47",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.11 ",
    "text": "3.11 \nThen we need to get the tiles using get_stadiamap() from ggmap.\n\ngerman_stamen_map &lt;- get_stadiamap(bbox_germany, zoom = 6, maptype = \"stamen_toner_lite\")\n\njapan_stamen_map &lt;- get_stadiamap(bbox_japan, zoom = 6, maptype = \"stamen_toner_lite\")\n\naus_stamen_map &lt;- get_stadiamap(bbox_australia, zoom = 5, maptype = \"stamen_toner_lite\")"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-48",
    "href": "lectures/lecture-09-slides.html#section-48",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.12 ",
    "text": "3.12 \nAnd finally, we can bring it all together with maps showing United States military bases in Germany (?@fig-mapbasesin-1), Japan (?@fig-mapbasesin-2), and Australia (?@fig-mapbasesin-3)."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#geocoding",
    "href": "lectures/lecture-09-slides.html#geocoding",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.13 Geocoding",
    "text": "3.13 Geocoding\nSo far we have assumed that we already have geocoded data. This means that we have latitude and longitude coordinates for each place. But sometimes we only have place names, such as “Sydney, Australia”, “Toronto, Canada”, “Accra, Ghana”, and “Guayaquil, Ecuador”. Before we can plot them, we need to get the latitude and longitude coordinates for each case. The process of going from names to coordinates is called geocoding."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-49",
    "href": "lectures/lecture-09-slides.html#section-49",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.14 ",
    "text": "3.14 \n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\nWhile you almost surely know where you live, it can be surprisingly difficult to specifically define the boundaries of many places. And this is made especially difficult when different levels of government have different definitions. Bronner (2021) illustrates this in the case of Atlanta, Georgia, where there are (at least) three official different definitions:\n\nthe metropolitan statistical area;\nthe urbanized area; and\nthe census place.\n\nWhich definition is used can have a substantial effect on the analysis, or even the data that are available, even though they are all “Atlanta”."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-50",
    "href": "lectures/lecture-09-slides.html#section-50",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.15 ",
    "text": "3.15 \nThere are a range of options to geocode data in R, but tidygeocoder is especially useful. We first need a dataframe of locations.\n\nplace_names &lt;-\n    tibble(\n        city = c(\"Sydney\", \"Toronto\", \"Accra\", \"Guayaquil\"),\n        country = c(\"Australia\", \"Canada\", \"Ghana\", \"Ecuador\")\n    )\n\nplace_names\n\n\nplace_names &lt;-\n    geo(\n        city = place_names$city,\n        country = place_names$country,\n        method = \"osm\"\n    )\n\nplace_names"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#section-51",
    "href": "lectures/lecture-09-slides.html#section-51",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.16 ",
    "text": "3.16 \nAnd we can now plot and label these cities (?@fig-mynicemap).\n\nworld &lt;- map_data(map = \"world\")\n\nggplot() +\n    geom_polygon(\n        data = world,\n        aes(x = long, y = lat, group = group),\n        fill = \"white\",\n        colour = \"grey\"\n    ) +\n    geom_point(\n        aes(x = place_names$long, y = place_names$lat),\n        color = \"black\"\n    ) +\n    geom_text(\n        aes(x = place_names$long, y = place_names$lat, label = place_names$city),\n        nudge_y = -5\n    ) +\n    theme_minimal() +\n    labs(\n        x = \"Longitude\",\n        y = \"Latitude\"\n    )"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#concluding-remarks",
    "href": "lectures/lecture-09-slides.html#concluding-remarks",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.17 Concluding remarks",
    "text": "3.17 Concluding remarks\nIn this chapter we considered many ways of communicating data. We spent substantial time on graphs, because of their ability to convey a large amount of information in an efficient way. We then turned to tables because of how they can specifically convey information. Finally, we discussed maps, which allow us to display geographic information. The most important task is to show the observations to the full extent possible."
  },
  {
    "objectID": "lectures/lecture-09-slides.html#references",
    "href": "lectures/lecture-09-slides.html#references",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.18 References",
    "text": "3.18 References"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#references-1",
    "href": "lectures/lecture-09-slides.html#references-1",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.19 References",
    "text": "3.19 References\n\n\nAndersen, Robert, and David Armstrong. 2021. Presenting Statistical Results Effectively. London: Sage.\n\n\nArel-Bundock, Vincent. 2021. WDI: World Development Indicators and Other World Bank Data. https://CRAN.R-project.org/package=WDI.\n\n\n———. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nArmstrong, Zan. 2022. “Stop Aggregating Away the Signal in Your Data.” The Overflow, March. https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/.\n\n\nArnold, Jeffrey. 2021. ggthemes: Extra Themes, Scales and Geoms for “ggplot2”. https://CRAN.R-project.org/package=ggthemes.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, and Alex Deckmyn. 2022. maps: Draw Geographical Maps. https://CRAN.R-project.org/package=maps.\n\n\nBethlehem, R. A. I., J. Seidlitz, S. R. White, J. W. Vogel, K. M. Anderson, C. Adamson, S. Adler, et al. 2022. “Brain Charts for the Human Lifespan.” Nature 604 (7906): 525–33. https://doi.org/10.1038/s41586-022-04554-y.\n\n\nBronner, Laura. 2021. “Quantitative Editing.” YouTube, June. https://youtu.be/LI5m9RzJgWc.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “tidygeocoder: Geocoding Made Easy.” Zenodo. https://doi.org/10.5281/zenodo.3981510.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data. 2nd ed. New Jersey: Hobart Press.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDenby, Lorraine, and Colin Mallows. 2009. “Variations on the Histogram.” Journal of Computational and Graphical Statistics 18 (1): 21–31. https://doi.org/10.1198/jcgs.2009.0002.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFlynn, Michael. 2022. troopdata: Tools for Analyzing Cross-National Military Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nFox, John, and Robert Andersen. 2006. “Effect Displays for Multinomial and Proportional-Odds Logit Models.” Sociological Methodology 36 (1): 225–55. https://doi.org/10.1111/j.1467-9531.2006.00180.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2022. carData: Companion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. 1st ed. Massachusetts: Harvard University Press.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Camargo, Marco Sciaini, and Cédric Scherer. 2021. viridis – Colorblind-Friendly Color Maps for R. https://doi.org/10.5281/zenodo.4679424.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nHowes, Adam. 2022. “Representing Uncertainty Using Significant Figures,” April. https://athowes.github.io/posts/2022-04-24-representing-uncertainty-using-significant-figures/.\n\n\nKahle, David, and Hadley Wickham. 2013. “ggmap: Spatial Visualization with ggplot2.” The R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKarsten, Karl. 1923. Charts and Graphs. New York: Prentice-Hall.\n\n\nKuznets, Simon, Lillian Epstein, and Elizabeth Jenks. 1941. National Income and Its Composition, 1919-1938. National Bureau of Economic Research.\n\n\nMcIlroy, Doug, Ray Brownrigg, Thomas Minka, and Roger Bivand. 2023. mapproj: Map Projections. https://CRAN.R-project.org/package=mapproj.\n\n\nMoyer, Brian, and Abe Dunn. 2020. “Measuring the Gross Domestic Product (GDP): The Ultimate Data Science Project.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.414caadb.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In Understanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPhillips, Alban. 1958. “The Relation Between Unemployment and the Rate of Change of Money Wage Rates in the United Kingdom, 1861-1957.” Economica 25 (100): 283–99. https://doi.org/10.1111/j.1468-0335.1958.tb00003.x.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRudis, Bob. 2020. hrbrthemes: Additional Themes, Theme Components and Utilities for “ggplot2”. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nSpear, Mary Eleanor. 1952. Charting Statistics. https://archive.org/details/ChartingStatistics_201801/.\n\n\nTukey, John. 1977. Exploratory Data Analysis.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWeissgerber, Tracey, Natasa Milic, Stacey Winham, and Vesna Garovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.” PLoS Biology 13 (4): e1002128. https://doi.org/10.1371/journal.pbio.1002128.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Lisa Stryjewski. 2011. “40 Years of Boxplots,” November. https://vita.had.co.nz/papers/boxplots.pdf.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "lectures/lecture-11-slides.html#coming-soon",
    "href": "lectures/lecture-11-slides.html#coming-soon",
    "title": "Measurement, Censuses, and Sampling",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-11-slides.html#references",
    "href": "lectures/lecture-11-slides.html#references",
    "title": "Measurement, Censuses, and Sampling",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-11-slides.html#references-1",
    "href": "lectures/lecture-11-slides.html#references-1",
    "title": "Measurement, Censuses, and Sampling",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#coming-soon",
    "href": "lectures/lecture-13-slides.html#coming-soon",
    "title": "APIs, Scraping, and Parsing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#references",
    "href": "lectures/lecture-13-slides.html#references",
    "title": "APIs, Scraping, and Parsing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#references-1",
    "href": "lectures/lecture-13-slides.html#references-1",
    "title": "APIs, Scraping, and Parsing",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#coming-soon",
    "href": "lectures/lecture-15-slides.html#coming-soon",
    "title": "Experiments and Surveys",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#references",
    "href": "lectures/lecture-15-slides.html#references",
    "title": "Experiments and Surveys",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#references-1",
    "href": "lectures/lecture-15-slides.html#references-1",
    "title": "Experiments and Surveys",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#coming-soon",
    "href": "lectures/lecture-17-slides.html#coming-soon",
    "title": "Cleaning, Preparing, and Testing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#references",
    "href": "lectures/lecture-17-slides.html#references",
    "title": "Cleaning, Preparing, and Testing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#references-1",
    "href": "lectures/lecture-17-slides.html#references-1",
    "title": "Cleaning, Preparing, and Testing",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#coming-soon",
    "href": "lectures/lecture-19-slides.html#coming-soon",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#references",
    "href": "lectures/lecture-19-slides.html#references",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#references-1",
    "href": "lectures/lecture-19-slides.html#references-1",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#coming-soon",
    "href": "lectures/lecture-21-slides.html#coming-soon",
    "title": "Linear Models",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#references",
    "href": "lectures/lecture-21-slides.html#references",
    "title": "Linear Models",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#references-1",
    "href": "lectures/lecture-21-slides.html#references-1",
    "title": "Linear Models",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#coming-soon",
    "href": "lectures/lecture-23-slides.html#coming-soon",
    "title": "Generalized Linear Models (GLMs)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#references",
    "href": "lectures/lecture-23-slides.html#references",
    "title": "Generalized Linear Models (GLMs)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#references-1",
    "href": "lectures/lecture-23-slides.html#references-1",
    "title": "Generalized Linear Models (GLMs)",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-25-slides.html#coming-soon",
    "href": "lectures/lecture-25-slides.html#coming-soon",
    "title": "Project Work",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-25-slides.html#references",
    "href": "lectures/lecture-25-slides.html#references",
    "title": "Project Work",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-25-slides.html#references-1",
    "href": "lectures/lecture-25-slides.html#references-1",
    "title": "Project Work",
    "section": "3 References",
    "text": "3 References"
  },
  {
    "objectID": "lectures/lecture-01-notes.html",
    "href": "lectures/lecture-01-notes.html",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "Required: Browse this site\nRecommended: \n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#reading-assignment",
    "href": "lectures/lecture-01-notes.html#reading-assignment",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "Required: Browse this site\nRecommended:"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#lecture-slides",
    "href": "lectures/lecture-01-notes.html#lecture-slides",
    "title": "Introduction + Telling Stories with Data",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-notes.html#welcome-to-soci-3040",
    "title": "Introduction + Telling Stories with Data",
    "section": "1 👋 Welcome to SOCI 3040!",
    "text": "1 👋 Welcome to SOCI 3040!\nThis is an introduction the knowledge and skills you need to tell credible stories with quantitative data…\n\nTODO: Add intro slides and overview\nThey read the chapter for Thursday, but Thursday will be intro to R in lab session"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#why-data-storytelling-matters",
    "href": "lectures/lecture-01-notes.html#why-data-storytelling-matters",
    "title": "Introduction + Telling Stories with Data",
    "section": "2 Why Data Storytelling Matters",
    "text": "2 Why Data Storytelling Matters\n\n“A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.” (Alexander 2023)\n\n\nCore foundation of quantitative research methods\nBridge between analysis and understanding\nEssential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#common-concerns",
    "href": "lectures/lecture-01-notes.html#common-concerns",
    "title": "Introduction + Telling Stories with Data",
    "section": "3 Common Concerns",
    "text": "3 Common Concerns\nFive Key Questions for Data Stories\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What’s missing?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data?\n\n\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components",
    "href": "lectures/lecture-01-notes.html#core-workflow-components",
    "title": "Introduction + Telling Stories with Data",
    "section": "4 Core Workflow Components",
    "text": "4 Core Workflow Components\n\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure 1: Rohan Alexander’s (2023) workflow for telling stories with data.\n\n\n\n\n\n\nPlan and sketch endpoint\nSimulate and consider data\nAcquire and prepare data\nExplore and understand data\nShare findings"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#plan-and-sketch",
    "href": "lectures/lecture-01-notes.html#plan-and-sketch",
    "title": "Introduction + Telling Stories with Data",
    "section": "5  Plan and Sketch",
    "text": "5  Plan and Sketch\n\n\n\ndeliberate, reasoned decisions\npurposeful adjustments\neven 10 minutes of planning is valuable\n\n\n\n\n\nThink of Alice’s conversation with the Cheshire Cat 😸. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.\n\n\n\n\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice’s conversation with the Cheshire Cat in Alice’s Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn’t require extensive time—often just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#simulate-data",
    "href": "lectures/lecture-01-notes.html#simulate-data",
    "title": "Introduction + Telling Stories with Data",
    "section": "6  Simulate Data",
    "text": "6  Simulate Data\n\nForces detailed thinking\nClarifies expected data structure and distributions.\nHelps with cleaning and preparation\nIdentifies potential issues beforehand.\nProvides clear testing framework\nEnsures data meets expectations.\n“Almost free” with modern computing\nProvides “an intimate feeling for the situation” (Hamming [1997] 2020)\n\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides “an intimate feeling for the situation” and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#acquire-and-prepare",
    "href": "lectures/lecture-01-notes.html#acquire-and-prepare",
    "title": "Introduction + Telling Stories with Data",
    "section": "7  Acquire and Prepare",
    "text": "7  Acquire and Prepare\n\nOften overlooked but crucial stage\nMany difficult decisions required: data sources, formats, permissions.\nCan significantly affect statistical results (Huntington-Klein et al. 2021)\nCommon challenges: quantity (too little or too much data) and quality\n\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed—either by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#explore-and-understand",
    "href": "lectures/lecture-01-notes.html#explore-and-understand",
    "title": "Introduction + Telling Stories with Data",
    "section": "8  Explore and Understand",
    "text": "8  Explore and Understand\n\nBegin with descriptive statistics\nMove to statistical models\nRemember: Models are tools, not truth\nModels reflect:\n\nEarlier decisions\nData acquisition choices\nCleaning procedures\n\n\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It’s important to remember that statistical models are tools—not absolute truths—and they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#share-findings",
    "href": "lectures/lecture-01-notes.html#share-findings",
    "title": "Introduction + Telling Stories with Data",
    "section": "9  Share Findings",
    "text": "9  Share Findings\n\nHigh-fidelity communication essential\nDocument all decisions\nBuild credibility through transparency\n\nInclude:\n\nWhat was done\nWhy it was done\nWhat was found\nWeaknesses of the approach\n\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it’s essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process—from data acquisition to analysis—builds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#the-foundation",
    "href": "lectures/lecture-01-notes.html#the-foundation",
    "title": "Introduction + Telling Stories with Data",
    "section": "10 The Foundation",
    "text": "10 The Foundation\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#communication-most-important",
    "href": "lectures/lecture-01-notes.html#communication-most-important",
    "title": "Introduction + Telling Stories with Data",
    "section": "11  Communication (Most Important)",
    "text": "11  Communication (Most Important)\n\n“Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly.” (Alexander 2023)\n\n\n“One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.” (Alexander 2023)\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience’s perspective"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#reproducibility",
    "href": "lectures/lecture-01-notes.html#reproducibility",
    "title": "Introduction + Telling Stories with Data",
    "section": "12  Reproducibility",
    "text": "12  Reproducibility\nEverything must be independently repeatable.\nRequirements:\n\nOpen access to code\nData availability or simulation\nAutomated testing\nClear documentation\nAim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#ethics",
    "href": "lectures/lecture-01-notes.html#ethics",
    "title": "Introduction + Telling Stories with Data",
    "section": "13  Ethics",
    "text": "13  Ethics\n\n“This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?” (Alexander 2023)\n\n\nConsider the full context of the dataset (D’Ignazio and Klein 2020)\nAcknowledge the social, cultural, and political forces (Crawford 2021)\nUse data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#questions",
    "href": "lectures/lecture-01-notes.html#questions",
    "title": "Introduction + Telling Stories with Data",
    "section": "14  Questions",
    "text": "14  Questions\n\nQuestions evolve through understanding\nChallenge of operationalizing variables\nCuriosity is essential, drives deeper exploration\nValue of “hybrid” knowledge that combines multiple disciplines\nComfort with asking “dumb” questions\n\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a “hybrid”—cultivating knowledge across various disciplines—and being comfortable with asking seemingly simple or “dumb” questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#measurement",
    "href": "lectures/lecture-01-notes.html#measurement",
    "title": "Introduction + Telling Stories with Data",
    "section": "15  Measurement",
    "text": "15  Measurement\n\n“The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.” (Alexander 2023)\n\n\n\nMeasuring even simple things is challenging\nExample: Measuring height\n\nShoes on or off?\nTime of day affects height.\nDifferent tools yield different results.\n\nMore complex measurements are even harder. How do we measure happiness or pain?\n\nMeasurement requires decisions and is not value-free\nContext and purpose guide all measurement choices\n\n\n\n\n\nPicasso’s dog and the challenges of reduction.\n\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person’s height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso’s minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we’re measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#data-collection-cleaning",
    "href": "lectures/lecture-01-notes.html#data-collection-cleaning",
    "title": "Introduction + Telling Stories with Data",
    "section": "16  &  Data Collection & Cleaning",
    "text": "16  &  Data Collection & Cleaning\n\n“Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.” (Alexander 2023)\n\n\nCollection determines possibilities\n\nWhat and how we measure matters.\n\nCleaning requires many decisions\n\nExample: Survey responses on gender\nOptions: “man”, “woman”, “prefer not to say”, “other”\nHandling “prefer not to say” and open-text responses.\n\nDocument every step\n\nEnsures transparency and reproducibility.\n\nConsider implications of choices\n\nEthical considerations and representation.\n\n\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like “man,” “woman,” “prefer not to say,” and “other” (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude “prefer not to say” responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents’ specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it’s vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#eda-modeling-scaling",
    "href": "lectures/lecture-01-notes.html#eda-modeling-scaling",
    "title": "Introduction + Telling Stories with Data",
    "section": "17 + EDA, Modeling, & Scaling",
    "text": "17 + EDA, Modeling, & Scaling\n\n17.1 Exploratory Data Analysis (EDA)\n\nIterative process\nNever truly complete\nShapes understanding\n\n\n\n17.2 Modeling\n\nTool for understanding\nNot a recipe to follow\nJust one representation of reality\nStatistical significance \\(\\neq\\) scientific significance\nStatistical models help us explore the shape of the data; are like echolocation\n\n\n\n17.3 Scaling\n\nUsing programming languages like R and Python\n\nHandle large datasets efficiently\nAutomate repetitive tasks\nShare work widely and quickly\n\nOutputs can reach many people easily\nAPIs can make analyses accessible in real-time\n\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there’s always more to explore. Although it’s challenging to delineate where EDA ends and formal statistical modeling begins—since our beliefs and understanding evolve continuously—EDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it’s crucial that all steps taken during EDA are recorded and shared.\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it’s a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference—the process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it’s essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly—whether it’s one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data",
    "title": "Introduction + Telling Stories with Data",
    "section": "18 How Do Our Worlds Become Data?",
    "text": "18 How Do Our Worlds Become Data?\n\n“There is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.” (Hamming [1997] 2020, 177)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-1",
    "title": "Introduction + Telling Stories with Data",
    "section": "19 How Do Our Worlds Become Data?",
    "text": "19 How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-2",
    "title": "Introduction + Telling Stories with Data",
    "section": "20 How Do Our Worlds Become Data?",
    "text": "20 How Do Our Worlds Become Data?\n Through skillfulreduction 👨‍🍳\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data—plausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it’s crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment—much like a chef monitoring a reduction to achieve the desired consistency without overcooking—to determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors—from selecting what to measure to deciding on the methods and accuracy required. Advances in instruments—from telescopes in astronomy to real-time internet data collection—have expanded our ability to gather data, much like new culinary techniques enhance a chef’s ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our “reduction” process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#what-is-data-science",
    "href": "lectures/lecture-01-notes.html#what-is-data-science",
    "title": "Introduction + Telling Stories with Data",
    "section": "21 What is Data Science?",
    "text": "21 What is Data Science?\n\n“Data science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict.” (Alexander 2023)\n\nKey Principles\n\nData are generated, and must be gathered, cleaned, and prepared\nThese decisions matter\nThe process will be difficult\nDevelop resilience and intrinsic motivation"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#the-power-of-multiple-perspectives",
    "href": "lectures/lecture-01-notes.html#the-power-of-multiple-perspectives",
    "title": "Introduction + Telling Stories with Data",
    "section": "22 The Power of Multiple Perspectives",
    "text": "22 The Power of Multiple Perspectives\n\n“The strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past.” (Alexander 2023)\n\n\nData science is multi-disciplinary\nCombines statistics, software engineering, subject-matter expertise, and more.\nDiversity enhances understanding\nDifferent perspectives lead to better questions and solutions.\nCollaboration is key\nRespect and integrate insights from various fields."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#embracing-the-challenge",
    "href": "lectures/lecture-01-notes.html#embracing-the-challenge",
    "title": "Introduction + Telling Stories with Data",
    "section": "23 Embracing the Challenge",
    "text": "23 Embracing the Challenge\nOur world is messy, and so are our data. Telling stories with data is difficult but rewarding.\n\nDevelop resilience and intrinsic motivation\nAccept that failure is part of the process.\nConsider possibilities and probabilities\nLearn to make trade-offs.\nNo perfect analysis exists\nAim for transparency and continuous improvement.\n\n\n“Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.” (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#key-takeaways",
    "href": "lectures/lecture-01-notes.html#key-takeaways",
    "title": "Introduction + Telling Stories with Data",
    "section": "24 Key Takeaways",
    "text": "24 Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#import-libraries",
    "href": "lectures/lecture-04-slides.html#import-libraries",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "1.1 import libraries",
    "text": "1.1 import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#plan-1",
    "href": "lectures/lecture-04-slides.html#plan-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.1  plan",
    "text": "3.1  plan\n\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure 1 (a) (next slide). We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure 1 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#plan-2",
    "href": "lectures/lecture-04-slides.html#plan-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.2  plan",
    "text": "3.2  plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\n\n\n\nFigure 1: Sketches of a dataset and graph about the neonatal mortality rate (NMR)"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-1",
    "href": "lectures/lecture-04-slides.html#simulate-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.1  simulate",
    "text": "4.1  simulate\n\n\nTo simulate some data that aligns with our plan, we will need three columns: country, year, and NMR. We can do this by repeating the name of each country 50 times with rep(), and enabling the passing of 50 years. Then we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-2",
    "href": "lectures/lecture-04-slides.html#simulate-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.2  simulate",
    "text": "4.2  simulate\n\n\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", 50), rep(\"Australia\", 50),\n                rep(\"Canada\", 50), rep(\"Kenya\", 50)\n            ),\n        year =\n            rep(c(1971:2020), 4),\n        nmr =\n            runif(n = 200, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-2-output",
    "href": "lectures/lecture-04-slides.html#simulate-2-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.2  simulate",
    "text": "4.2  simulate\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-3",
    "href": "lectures/lecture-04-slides.html#simulate-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.3  simulate",
    "text": "4.3  simulate\n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-4",
    "href": "lectures/lecture-04-slides.html#simulate-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.4  simulate",
    "text": "4.4  simulate\n\n\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n                rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)\n            ),\n        year =\n            rep(c(1:number_of_years + 1970), 4),\n        nmr =\n            runif(n = number_of_years * 4, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-4-output",
    "href": "lectures/lecture-04-slides.html#simulate-4-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.4  simulate",
    "text": "4.4  simulate\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-5",
    "href": "lectures/lecture-04-slides.html#simulate-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.5  simulate",
    "text": "4.5  simulate\n\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat “country” is exclusively one of these four: “Argentina”, “Australia”, “Canada”, or “Kenya”.\nConversely, “country” contains all those four countries.\nThat “year” is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat “nmr” is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-6",
    "href": "lectures/lecture-04-slides.html#simulate-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.6  simulate",
    "text": "4.6  simulate\n\nsimulated_nmr_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#simulate-7",
    "href": "lectures/lecture-04-slides.html#simulate-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.7  simulate",
    "text": "4.7  simulate\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section",
    "href": "lectures/lecture-04-slides.html#section",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.1 ",
    "text": "5.1 \nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\nigme_data_path &lt;- here(\"data\", \"igme.csv\")\nigme_data_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/igme.csv\"\n\n\n\nraw_igme_data &lt;-\n    read_csv(\n        file =\n            \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n        show_col_types = FALSE\n    )\n\nwrite_csv(x = raw_igme_data, file = igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-1",
    "href": "lectures/lecture-04-slides.html#section-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.2 ",
    "text": "5.2 \n\nraw_igme_data &lt;-\n    read_csv(\n        file = igme_data_path,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-2",
    "href": "lectures/lecture-04-slides.html#section-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.3 ",
    "text": "5.3 \nWith established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail()\n\nhead(raw_igme_data)\n\n# A tibble: 6 × 29\n  `Geographic area` Indicator              Sex   `Wealth Quintile` `Series Name`\n  &lt;chr&gt;             &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;        \n1 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n2 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n3 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n4 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n5 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n6 Afghanistan       Neonatal mortality ra… Total Total             Afghanistan …\n# ℹ 24 more variables: `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;, COUNTRY_NOTES &lt;chr&gt;, CONNECTION &lt;lgl&gt;,\n#   DEATH_CATEGORY &lt;lgl&gt;, CATEGORY &lt;chr&gt;, `Observation Status` &lt;chr&gt;,\n#   `Unit of measure` &lt;chr&gt;, `Series Category` &lt;chr&gt;, `Series Type` &lt;chr&gt;,\n#   STD_ERR &lt;dbl&gt;, REF_DATE &lt;dbl&gt;, `Age Group of Women` &lt;chr&gt;,\n#   `Time Since First Birth` &lt;chr&gt;, DEFINITION &lt;chr&gt;, INTERVAL &lt;dbl&gt;,\n#   `Series Method` &lt;chr&gt;, LOWER_BOUND &lt;dbl&gt;, UPPER_BOUND &lt;dbl&gt;, …"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-3",
    "href": "lectures/lecture-04-slides.html#section-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.4 ",
    "text": "5.4 \nand what the names of the columns are with names()\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\""
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-4",
    "href": "lectures/lecture-04-slides.html#section-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.5 ",
    "text": "5.5 \n\nWe would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where “Sex” is “Total”, “Series Name” is “UN IGME estimate”, “Geographic area” is one of “Argentina”, “Australia”, “Canada”, and “Kenya”, and the “Indicator” is “Neonatal mortality rate”. After this we are interested in just a few columns: “geographic_area”, “time_period”, and “obs_value”."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-5",
    "href": "lectures/lecture-04-slides.html#section-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.6 ",
    "text": "5.6 \n\ncleaned_igme_data &lt;-\n    clean_names(raw_igme_data) |&gt;\n    filter(\n        sex == \"Total\",\n        series_name == \"UN IGME estimate\",\n        geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n        indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n    select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-5-output",
    "href": "lectures/lecture-04-slides.html#section-5-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.6 ",
    "text": "5.6 \n\n# A tibble: 6 × 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-6",
    "href": "lectures/lecture-04-slides.html#section-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.7 ",
    "text": "5.7 \n\nWe need to fix two other aspects: the class of “time_period” is character when we need it to be a year, and the name of “obs_value” should be “nmr” to be more informative."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-7",
    "href": "lectures/lecture-04-slides.html#section-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.8 ",
    "text": "5.8 \n\ncleaned_igme_data &lt;-\n    cleaned_igme_data |&gt;\n    mutate(\n        time_period = str_remove(time_period, \"-06\"),\n        time_period = as.integer(time_period)\n    ) |&gt;\n    filter(time_period &gt;= 1971) |&gt;\n    rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-8",
    "href": "lectures/lecture-04-slides.html#section-8",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.9 ",
    "text": "5.9 \n\nFinally, we can check that our dataset passes the tests that we developed based on the simulated dataset."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-9",
    "href": "lectures/lecture-04-slides.html#section-9",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.10 ",
    "text": "5.10 \n\ncleaned_igme_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-10",
    "href": "lectures/lecture-04-slides.html#section-10",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.11 ",
    "text": "5.11 \nAll that remains is to save the nicely cleaned dataset.\n\ncleaned_igme_data_path &lt;- here(\"data\", \"cleaned_igme_data.csv\")\nwrite_csv(x = cleaned_igme_data, file = cleaned_igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-11",
    "href": "lectures/lecture-04-slides.html#section-11",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.1 ",
    "text": "6.1 \nWe would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\ncleaned_igme_data &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_igme_data.csv\"),\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-12",
    "href": "lectures/lecture-04-slides.html#section-12",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.2 ",
    "text": "6.2 \n\nWe can now make a graph of how NMR has changed over time and the differences between countries (Figure 2).\n\n\ncleaned_igme_data |&gt;\n    ggplot(aes(x = year, y = nmr, color = country)) +\n    geom_point() +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#section-12-output",
    "href": "lectures/lecture-04-slides.html#section-12-output",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.2 ",
    "text": "6.2 \n\n\n\n\n\n\n\nFigure 2: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#share-1",
    "href": "lectures/lecture-04-slides.html#share-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "7.1  share",
    "text": "7.1  share\nExample taken directly from Alexander (2023), here.\n\n\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries—Argentina, Australia, Canada, and Kenya—over the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023).\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure 2). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  },
  {
    "objectID": "lectures/lecture-04-slides.html#references",
    "href": "lectures/lecture-04-slides.html#references",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "9.1 References",
    "text": "9.1 References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality, 2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf."
  },
  {
    "objectID": "lectures/lecture-07-notes.html",
    "href": "lectures/lecture-07-notes.html",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#reading-assignment",
    "href": "lectures/lecture-07-notes.html#reading-assignment",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#lecture-slides",
    "href": "lectures/lecture-07-notes.html#lecture-slides",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#key-concepts-and-skills",
    "href": "lectures/lecture-07-notes.html#key-concepts-and-skills",
    "title": "Writing and Developing Research Questions",
    "section": "1.1 Key concepts and skills",
    "text": "1.1 Key concepts and skills\n\nWriting is a critical skill—perhaps the most important—of all the skills required to analyze data. The only way to get better at writing is to write, ideally every day.\nWhen we write, although the benefits typically accrue to ourselves, we must nonetheless write for the reader. This means having one main message that we want to communicate, and thinking about where they are, rather than where we are.\nWe want to get to a first draft as quickly as possible. Even if it is horrible, the difference between a first draft existing and not is enormous. At that point we start to rewrite. When doing so we aim to maximize clarity, often by removing unnecessary words.\nWe typically begin with some area of interest and then develop research questions, datasets, and analysis in an iterative way. Through this process we come to a better understanding of what we are doing."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#software-and-packages",
    "href": "lectures/lecture-07-notes.html#software-and-packages",
    "title": "Writing and Developing Research Questions",
    "section": "1.2 Software and packages",
    "text": "1.2 Software and packages\n\nknitr (Xie 2023)\ntidyverse (Wickham et al. 2019)\n\n\nlibrary(knitr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#writing-1",
    "href": "lectures/lecture-07-notes.html#writing-1",
    "title": "Writing and Developing Research Questions",
    "section": "2.1 Writing",
    "text": "2.1 Writing\n\nThe way to do a piece of writing is three or four times over, never once. For me, the hardest part comes first, getting something—anything—out in front of me. Sometimes in a nervous frenzy I just fling words as if I were flinging mud at a wall. Blurt out, heave out, babble out something—anything—as a first draft.\nMcPhee (2017, 159)\n\nThe process of writing is a process of rewriting. The critical task is to get to a first draft as quickly as possible. Until that complete first draft exists, it is useful to try to not to delete, or even revise, anything that was written, regardless of how bad it may seem. Just write. (This advice is directed at less-experienced writers. As you get more experience, you may find that your approach changes.)\nOne of the most intimidating stages is a blank page, and we deal with this by immediately adding headings such as: “Introduction”, “Data”, “Model”, “Results”, and “Discussion”. And then adding fields in the top matter for the various bits and pieces that are needed, such as “title”, “date”, “author”, and “abstract”. This creates a generic outline, which will play the role of mise en place for the paper. By way of background, mise en place is a preparatory phase in a professional kitchen when ingredients are sorted, prepared, and arranged for easy access. This ensures that everything that is needed is available without unnecessary delay. Putting together an outline plays the same role when writing quantitative papers, and is akin to placing on the counter, the ingredients that we will use to prepare dinner (McPhee 2017)."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section",
    "href": "lectures/lecture-07-notes.html#section",
    "title": "Writing and Developing Research Questions",
    "section": "2.2 ",
    "text": "2.2 \nHaving established this generic outline, we need to develop an understanding of what we are exploring through thinking deeply about our research question. In theory, we develop a research question, answer it, and then do all the writing; but that rarely actually happens (Franklin 2005). Instead, we typically have some idea of the question and the shape of an answer, and these become less vague as we write. This is because it is through the process of writing that we refine our thinking (King 2000, 131). Having put down some thoughts about the research question, we can start to add dot points in each of the sections, adding sub-sections with informative sub-headings as needed. We then go back and expand those dot points into paragraphs. While we do this our thinking is influenced by a web of other researchers, but also other aspects such as our circumstances and environment (Latour 1996).\nWhile writing the first draft you should ignore the feeling that you are not good enough, or that it is impossible. Just write. You need words on paper, even if they are bad, and the first draft is when you accomplish this. Remove distractions and focus on writing. Perfectionism is the enemy, and should be set aside. Sometimes this can be accomplished by getting up very early to write, by creating a deadline, or forming a writing group. Creating a sense of urgency can be useful and one option is to not bother with adding proper citations as you go, which could slow you down, and instead just add something like “[TODO: CITE R HERE]”. Do similar with graphs and tables. That is, include textual descriptions such as “[TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE]” instead of actual graphs and tables. Focus on adding content, even if it is bad. When this is all done, a first draft exists."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-1",
    "href": "lectures/lecture-07-notes.html#section-1",
    "title": "Writing and Developing Research Questions",
    "section": "2.3 ",
    "text": "2.3 \nThis first draft will be poorly written and far from great. But it is by writing a bad first draft that you can get to a good second draft, a great third draft, and eventually excellence (Lamott 1994, 20). That first draft will be too long, it will not make sense, it will contain claims that cannot be supported, and some claims that should not be. If you are not embarrassed by your first draft, then you have not written it quickly enough.\nUse the “delete” key extensively, as well as “cut” and “paste”, to turn that first draft into a second. Print the draft and using a red pen to move or remove words, sentences, and entire paragraphs, is especially helpful. The process of going from a first draft to a second draft is best done in one sitting, to help with the flow and consistency of the story. One aspect of this first rewrite is enhancing the story that we want to tell. Another aspect is taking out everything that is not the story (King 2000, 57).\nIt can be painful to remove work that seems good even if it does not quite fit into what the draft is becoming. One way to make this less painful is to make a temporary document, perhaps named “debris.qmd”, to save these unwanted paragraphs instead of immediately deleting them. Another strategy is to comment out the paragraphs. That way you can still look at the raw file and notice aspects that could be useful."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-2",
    "href": "lectures/lecture-07-notes.html#section-2",
    "title": "Writing and Developing Research Questions",
    "section": "2.4 ",
    "text": "2.4 \nAs you go through what was written in each of the sections try to bring some sense to it with special consideration to how it supports the story that is developing. This revision process is the essence of writing (McPhee 2017, 160). You should also fix the references, and add the real graphs and tables. As part of this rewriting process, the paper’s central message tends to develop, and the answers to the research questions tend to become clearer. At this point, aspects such as the introduction can be returned to and, finally, the abstract. Typos and other issues affect the credibility of the work. So these should be fixed as part of the second draft."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-3",
    "href": "lectures/lecture-07-notes.html#section-3",
    "title": "Writing and Developing Research Questions",
    "section": "2.5 ",
    "text": "2.5 \nAt this point the draft is starting to become sensible. The job is to now make it brilliant. Print it and again go through it on paper. Try to remove everything that does not contribute to the story. At about this stage, you may start to get too close to the paper. This is a great opportunity to give it to someone else for their comments. Ask for feedback about what is weak about the story. After addressing these, it can be helpful to go through the paper once more, this time reading it aloud. A paper is never “done” and it is more that at a certain point you either run out of time or become sick of the sight of it."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-4",
    "href": "lectures/lecture-07-notes.html#section-4",
    "title": "Writing and Developing Research Questions",
    "section": "3.1 ",
    "text": "3.1 \nConsider two examples:\n\nMok et al. (2022) examine eight billion unique listening events from 100,000 Spotify users to understand how users explore content. They find a clear relationship between age and behavior, with younger users exploring unknown content less than older users, despite having more diverse consumption. While it is clear that research questions around discovery and exploration drive this paper, it would not have been possible without access to this dataset. There likely would have been an iterative process where potential research questions and potential datasets were considered, before the ultimate match.\nThink of wanting to explore the neonatal mortality rate (NMR), which was introduced in ?@sec-fire-hose. One might be interested in what NMR could look like in Sub-Saharan Africa in 20 years. This would be question-first. But within this, there could be: theory-driven aspects, such as what do we expect based on biological relationships with other quantities; or data-driven aspects such as collecting as much data as possible to make forecasts. An alternative, purely data-driven approach would be having access to the NMR and then working out what is possible."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#data-first",
    "href": "lectures/lecture-07-notes.html#data-first",
    "title": "Writing and Developing Research Questions",
    "section": "3.2 Data-first",
    "text": "3.2 Data-first\nWhen being data-first, the main issue is working out the questions that can be reasonably answered with the available data. When deciding what these are, it is useful to consider:\n\nTheory: Is there a reasonable expectation that there is something causal that could be determined? For instance, Mark Christensen used to joke that if the question involved charting the stock market, then it might be better to hark back to The Odyssey and read bull entrails on a fire, because at least that way you would have something to eat at the end of the day. Questions usually need to have some plausible theoretical underpinning to help avoid spurious relationships. One way to develop theory, given data, is to consider “of what is this an instance?” (Rosenau 1999, 7). Following that approach, one tries to generalize beyond the specific setting. For instance, thinking of some particular civil war as an instance of all civil wars. The benefit of this is it focuses attention on the general attributes needed for building theory.\nImportance: There are plenty of trivial questions that can be answered, but it is important to not waste our time or that of the reader. Having an important question can also help with motivation when we find ourselves in, say, the fourth straight week of cleaning data and debugging code. In industry it can also make it easier to attract talented employees and funding. That said, a balance is needed; the question needs to have a decent chance of being answered. Attacking a generation-defining question might be best broken into chunks.\nAvailability: Is there a reasonable expectation of additional data being available in the future? This could allow us to answer related questions and turn one paper into a research agenda.\nIteration: Is this something that could be run multiple times, or is it a once-off analysis? If it is the former, then it becomes possible to start answering specific research questions and then iterate. But if we can only get access to the data once then we need to think about broader questions."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-5",
    "href": "lectures/lecture-07-notes.html#section-5",
    "title": "Writing and Developing Research Questions",
    "section": "3.3 ",
    "text": "3.3 \nThere is a saying, sometimes attributed to Xiao-Li Meng, that all of statistics is a missing data problem. And so paradoxically, another way to ask data-first questions is to think about the data we do not have. For instance, returning to the neonatal and maternal mortality examples discussed earlier one problem is that we do not have complete cause of death data. If we did, then we could count the number of relevant deaths. (Castro et al. (2023) remind us that this simplistic hypothetical would be complicated in reality because there are sometimes causes of death that are not independent of other causes.) Having established some missing data problem, we can take a data-driven approach. We look at the data we do have, and then ask research questions that speak to the extent that we can use that to approximate our hypothetical dataset.\nOne way that some researchers are data-first is that they develop a particular expertise in the data of some geographical or historical circumstance. For instance, they may be especially knowledgeable about, say, the present-day United Kingdom, or late nineteenth century Japan. They then look at the questions that other researchers are asking in other circumstances, and bring their data to that question. For instance, it is common to see a particular question initially asked for the United States, and then a host of researchers answer that same question for the United Kingdom, Canada, Australia, and many other countries.\nThere are a number of negatives to data-first research, including the fact that it can be especially uncertain. It can also struggle for external validity because there is always a worry about a selection effect.\nA variant of data-driven research is model-driven research. Here a researcher becomes an expert on some particular statistical approach and then applies that approach to appropriate contexts."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#question-first",
    "href": "lectures/lecture-07-notes.html#question-first",
    "title": "Writing and Developing Research Questions",
    "section": "3.4 Question-first",
    "text": "3.4 Question-first\nWhen trying to be question-first, there is the inverse issue of being concerned about data availability. The “FINER framework” is used in medicine to help guide the development of research questions. It recommends asking questions that are: Feasible, Interesting, Novel, Ethical, and Relevant (Hulley et al. 2007). Farrugia et al. (2010) build on FINER with PICOT, which recommends additional considerations: Population, Intervention, Comparison group, Outcome of interest, and Time."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-6",
    "href": "lectures/lecture-07-notes.html#section-6",
    "title": "Writing and Developing Research Questions",
    "section": "3.5 ",
    "text": "3.5 \nIt can feel overwhelming trying to write out a question. One way to go about it is to ask a very specific question. Another is to decide whether we are interested in descriptive, predictive, inferential, or causal analysis. These then lead to different types of questions. For instance:\n\ndescriptive analysis: “What does \\(x\\) look like?”;\npredictive analysis: “What will happen to \\(x\\)?”;\ninferential: “How can we explain \\(x\\)?”; and\ncausal: “What impact does \\(x\\) have on \\(y\\)?”.\n\nEach of these have a role to play. Since the credibility revolution (Angrist and Pischke 2010), causal questions answered with a particular approach have been predominant. This has brought some benefit, but not without cost. Descriptive analysis can be just as, indeed sometimes more, illuminating, and is critical (Sen 1980). The nature of the question being asked matters less than being genuinely interested in answering it."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-7",
    "href": "lectures/lecture-07-notes.html#section-7",
    "title": "Writing and Developing Research Questions",
    "section": "3.6 ",
    "text": "3.6 \nTime will often be constrained, possibly in an interesting way and this can guide the specifics of the research question. If we are interested in the effect of a celebrity’s announcements on the stock market, then that can be done by looking at stock prices before and after the announcement. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we must either wait a while, or we need to look at people who were treated twenty years ago. We then have selection effects and different circumstances compared to if we were to administer the drug today. Often the only reasonable thing to do is to build a statistical model, but that brings other issues."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#counterfactuals-and-bias",
    "href": "lectures/lecture-07-notes.html#counterfactuals-and-bias",
    "title": "Writing and Developing Research Questions",
    "section": "4.1 Counterfactuals and bias",
    "text": "4.1 Counterfactuals and bias\nThe creation of a counterfactual is often crucial when answering questions. A counterfactual is an if-then statement in which the “if” is false. Consider the example of Humpty Dumpty in Through the Looking-Glass by Lewis Carroll:\n\n“What tremendously easy riddles you ask!” Humpty Dumpty growled out. “Of course I don’t think so! Why, if ever I did fall off—which there’s no chance of—but if I did—” Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. “If I did fall,” he went on, “The King has promised me—with his very own mouth-to-to-” “To send all his horses and all his men,” Alice interrupted, rather unwisely.\nCarroll (1871)\n\nHumpty is satisfied with what would happen if he were to fall off, even though he is convinced that this would never happen. It is this comparison group that often determines the answer to a question. For instance, in ?@sec-causality-from-observational-data we consider the effect of VO2 max on a cyclist’s chance of winning a race. If we compare over the general population then it is an important variable. But if we only compare over well-trained athletes, then it is less important, because of selection.\n\n4.1.1 Selection bias and measurement bias\nTwo aspects of the data to be especially aware of when deciding on a research question are selection bias and measurement bias.\nSelection bias occurs when the results depend on who is in the sample. One of the pernicious aspects of selection bias is that we need to know about its existence in order to do anything about it. But many default diagnostics will not identify selection bias. In A/B testing, which we discuss in ?@sec-hunt-data, A/A testing is a slight variant where we create groups and compare them before imposing a treatment (hence the A/A nomenclature). This effort to check whether the groups are initially the same, can help to identify selection bias. More generally, comparing the properties of the sample, such as age-group, gender, and education, with characteristics of the population can assist as well. But the fundamental problem with selection bias and observational data is that we know people about whom we have data are different in at least one way to those about whom we do not! But we do not know in what other ways they may be different.\nSelection bias can pervade many aspects of our analysis. Even a sample that is initially representative may become biased over time. For instance, survey panels, that we discuss in ?@sec-farm-data, need to be updated from time to time because the people who do not get anything out of it stop responding.\nAnother bias to be aware of is measurement bias, which occurs when the results are affected by how the data were collected. A common example of this is if we were to ask respondents their income, then we may get different answers in-person compared with an online survey."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#estimands",
    "href": "lectures/lecture-07-notes.html#estimands",
    "title": "Writing and Developing Research Questions",
    "section": "4.2 Estimands",
    "text": "4.2 Estimands\nWe will typically be interested in using data to answer our question and it is important that we are clear about specifics. For instance, we might be interested in the effect of smoking on life expectancy. In that case, there is some true effect, which we can never know, and that true effect is called the “estimand” (Little and Lewis 2021). Defining the estimand at some point in the paper, ideally in the introduction, is critical (Lundberg, Johnson, and Stewart 2021). This is because it is easy to slightly change some specific aspect of the analysis plan and end up accidentally estimating something different (Kahan et al. 2022). They are beginning to be required by some medicine regulators (Kahan et al. 2024). For an estimand we are looking for a clear description of what the effect represents (Kahan et al. 2023). An “estimator” is a process by which we use the data that we have available to generate an “estimate” of the “estimand”. Efron and Morris (1977) provide a discussion of estimators and related concerns.\nBueno de Mesquita and Fowler (2021, 94) describe the relationship between an estimate and an estimand as:\n\\[\n\\mbox{Estimate = Estimand + Bias + Noise}\n\\]"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-8",
    "href": "lectures/lecture-07-notes.html#section-8",
    "title": "Writing and Developing Research Questions",
    "section": "4.3 ",
    "text": "4.3 \nBias refers to issues with an estimator systematically providing estimates that are different from the estimand, while noise refers to non-systematic differences. For instance, consider a standard Normal distribution. We might be interested in understanding the average, which would be our estimand. We know (in a way that we can never with real data) that the estimand is zero. Let us draw ten times from that distribution. One estimator we could use to produce an estimate is: sum the draws and divide by the number of draws. Another is to order the draws and find the middle observation. To be more specific, we will simulate this situation (Table 1).\n\nset.seed(853)\n\ntibble(\n    num_draws = c(\n        rep(10, times = 10),\n        rep(100, times = 100),\n        rep(1000, times = 1000),\n        rep(10000, times = 10000)\n    ),\n    draw = rnorm(\n        n = length(num_draws),\n        mean = 0,\n        sd = 1\n    )\n) |&gt;\n    summarise(\n        estimator_one = sum(draw) / unique(num_draws),\n        estimator_two = sort(draw)[round(unique(num_draws) / 2, 0)],\n        .by = num_draws\n    ) |&gt;\n    kable(\n        col.names = c(\"Number of draws\", \"Estimator one\", \"Estimator two\"),\n        digits = 2,\n        format.args = list(big.mark = \",\")\n    )\n\n\n\nTable 1: Comparing two estimators of the average of random draws as the number of draws increases\n\n\n\n\n\n\nNumber of draws\nEstimator one\nEstimator two\n\n\n\n\n10\n-0.58\n-0.82\n\n\n100\n-0.06\n-0.07\n\n\n1,000\n0.06\n0.04\n\n\n10,000\n-0.01\n-0.01\n\n\n\n\n\n\n\n\nAs the number of draws increases, the effect of noise is removed, and our estimates illustrate the bias of our estimators. In this example, we know what the truth is, but when considering real data it can be more difficult to know what to do. Hence the importance of being clear about what the estimand is, before turning to generating estimates."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#directed-acyclic-graphs",
    "href": "lectures/lecture-07-notes.html#directed-acyclic-graphs",
    "title": "Writing and Developing Research Questions",
    "section": "4.4 Directed Acyclic Graphs",
    "text": "4.4 Directed Acyclic Graphs\nWhen we are thinking about the variables we will use to answer our question, it can help to be specific about what we mean. It is easy to get caught up in observational data and trick ourselves. We should think hard, and to use all the tools available to us. One framework that can help with thinking hard about our data is the use of directed acyclic graphs (DAG). DAGs are a fancy name for a flow diagram and involve drawing arrows and lines between the variables to indicate the relationship between them.\nTo construct them we use Graphviz, which is an open-source package for graph visualization and is built into Quarto. The code needs to be wrapped in a “dot” chunk rather than “R”, and the chunk options are set with “//|” instead of “#|”. Alternatives that do not require this include the use of DiagrammeR (Iannone 2022) and ggdag (Barrett 2021). We provide the whole chunk for the first DAG, but then, only provide the code for the others.\n\n```{dot}\n//| label: fig-dot-firstdag-quarto\n//| fig-cap: \"We expect a causal relationship between x and y, where x influences y\"\n//| fig-width: 4\ndigraph D {\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  {rank=same x y};\n  \n  x -&gt; y;\n}\n```\n\n\n\n\n\n\n\nD\n\n\n\nx\nx\n\n\n\ny\ny\n\n\n\nx-&gt;y\n\n\n\n\n\n\n\n\nFigure 1: We expect a causal relationship between x and y, where x influences y\n\n\n\n\n\nIn Figure 1, we are saying that we think x causes y."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-12",
    "href": "lectures/lecture-07-notes.html#section-12",
    "title": "Writing and Developing Research Questions",
    "section": "4.5 ",
    "text": "4.5 \nWe could build another DAG where the situation is less clear. To make the examples a little easier to follow, we will switch to thinking about a hypothetical relationship between income and happiness, with consideration of variables that could affect that relationship. In this first one we consider the relationship between income and happiness, along with education (Figure 2).\n\ndigraph D {\n  \n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Education\"];\n  \n  { rank=same a b};\n  \n  a-&gt;b;\n  c-&gt;{a, b};\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nEducation\n\n\n\nc-&gt;a\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\nFigure 2: Education is a confounder that affects the relationship between income and happiness\n\n\n\n\n\nIn Figure 2, we think income causes happiness. But we also think that education causes happiness, and that education also causes income. That relationship is a “backdoor path”, and failing to adjust for education in a regression could overstate the extent of the relationship, or even create a spurious relationship, between income and happiness in our analysis. That is, we may think that changes in income are causing changes in happiness, but it could be that education is changing them both. That variable, in this case, education, is called a “confounder”.\nHernán and Robins (2023, 83) discuss an interesting case where a researcher was interested in whether one person looking up at the sky makes others look up at the sky also. There was a clear relationship between the responses of both people. But it was also the case that there was noise in the sky. It was unclear whether the second person looked up because the first person looked up, or they both looked up because of the noise. When using experimental data, randomization allows us to avoid this concern, but with observational data we cannot rely on that. It is also not the case that bigger data necessarily get around this problem for us. Instead, we should think carefully about the situation, and DAGs can help with that.\nIf there are confounders, but we are still interested in causal effects, then we need to adjust for them. One way is to include them in the regression. But the validity of this requires several assumptions. In particular, Gelman and Hill (2007, 169) warn that our estimate will only correspond to the average causal effect in the sample if we include all of the confounders and have the right model. Putting the second requirement to one side, and focusing only on the first, if we do not think about and observe a confounder, then it can be difficult to adjust for it. And this is an area where both domain expertise and theory can bring considerable weight to an analysis.\nIn Figure 3 we again consider that income causes happiness. But, if income also causes children, and children also cause happiness, then we have a situation where it would be tricky to understand the effect of income on happiness.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Children\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b, c};\n  c-&gt;b;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nChildren\n\n\n\na-&gt;c\n\n\n\n\n\nc-&gt;b\n\n\n\n\n\n\n\n\nFigure 3: Children as a mediator between income and happiness\n\n\n\n\n\nIn Figure 3, children is called a “mediator” and we would not adjust for it if we were interested in the effect of income on happiness. If we were to adjust for it, then some of what we are attributing to income, would be due to children.\nFinally, in Figure 4 we have yet another similar situation, where we think that income causes happiness. But this time both income and happiness also cause exercise. For instance, if you have more money then it may be easier to exercise, but also it may be easier to exercise if you are happier.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Exercise\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b c};\n  b-&gt;c;\n}\n\n\n\n\n\n\n\nD\n\n\n\na\nIncome\n\n\n\nb\nHappiness\n\n\n\na-&gt;b\n\n\n\n\n\nc\nExercise\n\n\n\na-&gt;c\n\n\n\n\n\nb-&gt;c\n\n\n\n\n\n\n\n\nFigure 4: Exercise as a collider affecting the relationship between income and happiness\n\n\n\n\n\nIn this case, exercise is called a “collider” and if we were to condition on it, then we would create a misleading relationship. Income influences exercise, but a person’s happiness also affects this. Exercise is a collider because both the predictor and outcome variable of interest influence it."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-13",
    "href": "lectures/lecture-07-notes.html#section-13",
    "title": "Writing and Developing Research Questions",
    "section": "4.6 ",
    "text": "4.6 \nWe will be clear about this: we must create the DAG ourselves, in the same way that we must put together the model ourselves. There is nothing that will create it for us. This means that we need to think carefully about the situation. Because it is one thing to see something in the DAG and then do something about it, but it is another to not even know that it is there. McElreath ([2015] 2020, 180) describes these as haunted DAGs. DAGs are helpful, but they are just a tool to help us think deeply about our situation.\nWhen we are building models, it can be tempting to include as many predictor variables as possible. DAGs show clearly why we need to be more thoughtful. For instance, if a variable is a confounder, then we would want to adjust for it, whereas if a variable was a collider then we would not. We can never know the truth, and we are informed by aspects such as theory, what we are interested in, research design, limitations of the data, or our own limitations as researchers, to name a few. Knowing the limits is as important as reporting the model. Data and models with flaws are still useful, if you acknowledge those flaws. The work of thinking about a situation is never done, and relies on others, which is why we need to make all our work as reproducible as possible."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-14",
    "href": "lectures/lecture-07-notes.html#section-14",
    "title": "Writing and Developing Research Questions",
    "section": "5.1 ",
    "text": "5.1 \nWe discuss the following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, and technical terms.1 Throughout the paper try to be as brief and specific as possible. Most readers will not get past the title. Almost no one will read more than the abstract. Section and sub-section headings, as well as graph and table captions should work on their own, without the surrounding text, because that type of skimming is how many people read papers (Keshav 2007)."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#title",
    "href": "lectures/lecture-07-notes.html#title",
    "title": "Writing and Developing Research Questions",
    "section": "5.2 Title",
    "text": "5.2 Title\nA title is the first opportunity that we have to engage our reader in our story. Ideally, we are able to tell our reader exactly what we found. Effective titles are critical because otherwise papers could be ignored by readers. While a title does not have to be “cute”, it does need to be meaningful. This means it needs to make the story clear.\nOne example of a title that is good enough is “On the 2016 Brexit referendum”. This title is useful because the reader knows what the paper is about. But it is not particularly informative or enticing. A slightly better title could be “On the Vote Leave outcome in the 2016 Brexit referendum”. This variant adds informative specificity. We argue the best title would be something like “Vote Leave outperforms in rural areas in the 2016 Brexit referendum: Evidence from a Bayesian hierarchical model”. Here the reader knows the approach of the paper and also the main take-away.\nWe will consider a few examples of particularly effective titles. Hug et al. (2019) use “National, regional, and global levels and trends in neonatal mortality between 1990 and 2017, with scenario-based projections to 2030: a systematic analysis”. Here it is clear what the paper is about and the methods that are used. R. Alexander and Alexander (2021) use “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018”. The main finding is, along with a good deal of information about what the content will be, clear from the title. M. Alexander, Kiang, and Barbieri (2018) use “Trends in Black and White Opioid Mortality in the United States, 1979–2015”; Frei and Welsh (2022) use “How the closure of a US tax loophole may affect investor portfolios”. Possibly one of the best titles ever is Bickel, Hammel, and O’Connell (1975) “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation”, which we return to in ?@sec-causality-from-observational-data.\nA title is often among the last aspects of a paper to be finalized. While getting through the first draft, we typically use a working title that gets the job done. We then refine it over the course of redrafting. The title needs to reflect the final story of the paper, and this is not usually something that we know at the start. We must strike a balance between getting our reader interested enough to read the paper, and conveying enough of the content so as to be useful (Hayot 2014). Two excellent examples are The History of England from the Accession of James the Second by Thomas Babington Macaulay, and A History of the English-Speaking Peoples by Winston Churchill. Both are clear about what the content is, and, for their target audience, spark interest.\nOne specific approach is the form: “Exciting content: Specific content”, for instance, “Returning to their roots: Examining the performance of Vote Leave in the 2016 Brexit referendum”. Kennedy and Gelman (2021) provide a particularly nice example of this approach with “Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample”, as does Craiu (2019) with “The Hiring Gambit: In Search of the Twofer Data Scientist”. A close variant of this is “A question? And an approach”. For instance, Cahill, Weinberger, and Alkema (2020) with “What increase in modern contraceptive use is needed in FP2020 countries to reach 75% demand satisfied by 2030? An assessment using the Accelerated Transition Method and Family Planning Estimation Model”. As you gain experience with this variant, it becomes possible to know when it is appropriate to drop the answer part yet remain effective, such as Briggs (2021) with “Why Does Aid Not Target the Poorest?”. Another specific approach is “Specific content then broad content” or the inverse. For instance, “Rurality, elites, and support for Vote Leave in the 2016 Brexit referendum” or “Support for Vote Leave in the 2016 Brexit referendum, rurality and elites”. This approach is used by Tolley and Paquet (2021) with “Gender, municipal party politics, and Montreal’s first woman mayor”.\nSometimes it is possible to include a subtitle. When this is possible, a great way to take advantage of this is to use it to include some detail of the main quantitative result that you found. Getting the right level of detail and abstraction about that result is difficult and will require re-writing and getting other’s opinions."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#abstract",
    "href": "lectures/lecture-07-notes.html#abstract",
    "title": "Writing and Developing Research Questions",
    "section": "5.3 Abstract",
    "text": "5.3 Abstract\nFor a ten-to-fifteen-page paper, a good abstract is a three-to-five sentence paragraph. For a longer paper the abstract can be slightly longer. The abstract needs to specify the story of the paper. It must also convey what was done and why it matters. To do so, an abstract typically touches on the context of the work, its objectives, approach, and findings.\nMore specifically, a good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.\nWe see this pattern in a variety of abstracts. For instance, Tolley and Paquet (2021) draw in the reader with their first sentence by mentioning the election of the first woman mayor in 400 years. The second sentence is clear about what is done in the paper. The third sentence tells the reader how it is done i.e. a survey, and the fourth sentence adds some detail. The fifth and final sentence makes the main take-away clear.\n\nIn 2017, Montreal elected Valérie Plante, the first woman mayor in the city’s 400-year history. Using this election as a case study, we show how gender did and did not influence the outcome. A survey of Montreal electors suggests that gender was not a salient factor in vote choice. Although gender did not matter much for voters, it did shape the organization of the campaign and party. We argue that Plante’s victory can be explained in part by a strategy that showcased a less leader-centric party and a degendered campaign that helped counteract stereotypes about women’s unsuitability for positions of political leadership.\n\nSimilarly, Beauregard and Sheppard (2021) make the broader environment clear within the first two sentences, and the specific contribution of this paper to that environment. The third and fourth sentences make the data source and main findings clear. The fifth and sixth sentences add specificity that would be of interest to likely readers of this abstract i.e. academic political scientists. In the final sentence, the position of the authors is made clear.\n\nPrevious research on support for gender quotas focuses on attitudes toward gender equality and government intervention as explanations. We argue the role of attitudes toward women in understanding support for policies aiming to increase the presence of women in politics is ambivalent—both hostile and benevolent forms of sexism contribute in understanding support, albeit in different ways. Using original data from a survey conducted on a probability-based sample of Australian respondents, our findings demonstrate that hostile sexists are more likely to oppose increasing of women’s presence in politics through the adoption of gender quotas. Benevolent sexists, on the other hand, are more likely to support these policies than respondents exhibiting low levels of benevolent sexism. We argue this is because benevolent sexism holds that women are pure and need protection; they do not have what it takes to succeed in politics without the assistance of quotas. Finally, we show that while women are more likely to support quotas, ambivalent sexism has the same relationship with support among both women and men. These findings suggest that aggregate levels of public support for gender quotas do not necessarily represent greater acceptance of gender equality generally."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-15",
    "href": "lectures/lecture-07-notes.html#section-15",
    "title": "Writing and Developing Research Questions",
    "section": "5.4 ",
    "text": "5.4 \nAnother excellent example of an abstract is Sides, Vavreck, and Warshaw (2021). In just five sentences, they make it clear what they do, how they do it, what they find, and why it is important.\n\nWe provide a comprehensive assessment of the influence of television advertising on United States election outcomes from 2000–2018. We expand on previous research by including presidential, Senate, House, gubernatorial, Attorney General, and state Treasurer elections and using both difference-in-differences and border-discontinuity research designs to help identify the causal effect of advertising. We find that televised broadcast campaign advertising matters up and down the ballot, but it has much larger effects in down-ballot elections than in presidential elections. Using survey and voter registration data from multiple election cycles, we also show that the primary mechanism for ad effects is persuasion, not the mobilization of partisans. Our results have implications for the study of campaigns and elections as well as voter decision making and information processing."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-16",
    "href": "lectures/lecture-07-notes.html#section-16",
    "title": "Writing and Developing Research Questions",
    "section": "5.5 ",
    "text": "5.5 \nThe best abstracts will have such a high content to words ratio that they may even feel a little terse. For instance, in the abstract of Touvron et al. (2023), there is not a word that is wasted and they communicate a large amount of information in only four sentences.\n\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\n\nKasy and Teytelboym (2023) provide an excellent example of a more statistical abstract. They clearly identify what they do and why it is important.\n\nWe consider an experimental setting in which a matching of resources to participants has to be chosen repeatedly and returns from the individual chosen matches are unknown but can be learned. Our setting covers two-sided and one-sided matching with (potentially complex) capacity constraints, such as refugee resettlement, social housing allocation, and foster care. We propose a variant of the Thompson sampling algorithm to solve such adaptive combinatorial allocation problems. We give a tight, prior-independent, finite-sample bound on the expected regret for this algorithm. Although the number of allocations grows exponentially in the number of matches, our bound does not. In simulations based on refugee resettlement data using a Bayesian hierarchical model, we find that the algorithm achieves half of the employment gains (relative to the status quo) that could be obtained in an optimal matching based on perfect knowledge of employment probabilities."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-17",
    "href": "lectures/lecture-07-notes.html#section-17",
    "title": "Writing and Developing Research Questions",
    "section": "5.6 ",
    "text": "5.6 \nFinally, Briggs (2021) begins with a claim that seems unquestionably true. In the second sentence he then says that it is false! The third sentence specifies the extent of this claim, and the fourth sentence details how he comes to this position, before providing more detail. The final two sentences speak broader implications and importance.\n\nForeign-aid projects typically have local effects, so they need to be placed close to the poor if they are to reduce poverty. I show that, conditional on local population levels, World Bank (WB) project aid targets richer parts of countries. This relationship holds over time and across world regions. I test five donor-side explanations for pro-rich targeting using a pre-registered conjoint experiment on WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments as most interested in targeting aid politically and controlling implementation. They also believe that aid works better in poorer or more remote areas, but that implementation in these areas is uniquely difficult. These results speak to debates in distributive politics, international bargaining over aid, and principal-agent issues in international organizations. The results also suggest that tweaks to WB incentive structures to make ease of project implementation less important may encourage aid to flow to poorer parts of countries."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-18",
    "href": "lectures/lecture-07-notes.html#section-18",
    "title": "Writing and Developing Research Questions",
    "section": "5.7 ",
    "text": "5.7 \nNature, a scientific journal, provides a guide for constructing an abstract. They recommend a structure that results in an abstract of six parts and adds up to around 200 words:\n\nAn introductory sentence that is comprehensible to a wide audience.\nA more detailed background sentence that is relevant to likely readers.\nA sentence that states the general problem.\nSentences that summarize and then explain the main results.\nA sentence about general context.\nAnd finally, a sentence about the broader perspective.\n\nThe first sentence of an abstract should not be vacuous. Assuming the reader continued past the title, this first sentence is the next opportunity that we have to implore them to keep reading our paper. And then the second sentence of the abstract, and so on. Work and re-work the abstract until it is so good that you would be fine if that was the only thing that was read; because that will often be the case."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-19",
    "href": "lectures/lecture-07-notes.html#section-19",
    "title": "Writing and Developing Research Questions",
    "section": "5.8 ",
    "text": "5.8 \n\n5.8.1 Introduction\nAn introduction needs to be self-contained and convey everything that a reader needs to know. We are not writing a mystery story. Instead, we want to give away the most important points in the introduction. For a ten-to-fifteen-page paper, an introduction may be two or three paragraphs of main content. Hayot (2014, 90) says the goal of an introduction is to engage the reader, locate them in some discipline and background, and then tell them what happens in the rest of the paper. It should be completely reader-focused.\nThe introduction should set the scene and give the reader some background. For instance, we typically start a little broader. This provides some context to the paper. We then describe how the paper fits into that context, and give some high-level results, especially focused on the one key result that is the main part of the story. We provide more detail here than we provided in the abstract, but not the full extent. And we broadly discuss next steps in a sentence or two. Finally, we finish the introduction with an additional short final paragraph that highlights the structure of the paper."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-20",
    "href": "lectures/lecture-07-notes.html#section-20",
    "title": "Writing and Developing Research Questions",
    "section": "5.9 ",
    "text": "5.9 \nAs an example (with made-up details):\n\nThe UK Conservative Party has always done well in rural electorates. And the 2016 Brexit vote was no different with a significant difference in support between rural and urban areas. But even by the standard of rural support for conservative issues, support for “Vote Leave” was unusually strong with “Vote Leave” being most heavily supported in the East Midlands and the East of England, while the strongest support for “Remain” was in Greater London.\nIn this paper we look at why the performance of “Vote Leave” in the 2016 Brexit referendum was so correlated with rurality. We construct a model in which support for “Vote Leave” at a voting area level is explained by the number of farms in the area, the average internet connectivity, and the median age. We find that as the median age of an area increases, the likelihood that an area supported “Vote Leave” decreases by 14 percentage points. Future work could look at the effect of having a Conservative MP which would allow a more nuanced understanding of these effects.\nThe remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.\n\nThe introduction needs to be self-contained and tell the reader almost everything that they need to know. A reader should be able to only read the introduction and have an accurate picture of all the major aspects of the whole paper. It would be rare to include graphs or tables in the introduction. An introduction should close by telegraphing the structure of the paper."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-21",
    "href": "lectures/lecture-07-notes.html#section-21",
    "title": "Writing and Developing Research Questions",
    "section": "5.10 ",
    "text": "5.10 \n\n5.10.1 Data\nRobert Caro, Lyndon Johnson’s biographer, describes the importance of conveying “a sense of place” when writing a biography (Caro 2019, 141). He defines this as “the physical setting in which a book’s action is occurring: to see it clearly enough, in sufficient detail, so that he feels as if he himself were present while the action is occurring.” He provides the following example:\n\nWhen Rebekah walked out the front door of that little house, there was nothing—a roadrunner streaking behind some rocks with something long and wet dangling from his beak, perhaps, or a rabbit disappearing around a bush so fast that all she really saw was the flash of a white tail—but otherwise nothing. There was no movement except for the ripple of the leaves in the scattered trees, no sound except for the constant whisper of the wind\\(\\dots\\) If Rebekah climbed, almost in desperation, the hill in the back of the house, what she saw from its crest was more hills, an endless vista of hills, hills on which there was visible not a single house\\(\\dots\\) hills on which nothing moved, empty hills with, above them, empty sky; a hawk circling silently overhead was an event. But most of all, there was nothing human, no one to talk to.\nCaro (2019, 146)\n\nHow thoroughly we can imagine the circumstances of Johnson’s mother, Rebekah Baines Johnson. When writing our papers, we need to achieve that same sense of place, for our data, as Caro provides for the Hill County. We do this by being as explicit as possible. We typically have a whole section about it and this is designed to show the reader, as closely as possible, the actual data that underpin our story.\nWhen writing the data section, we are beginning our answer to the critical question about our claim, which is, how is it possible to know this? (McPhee 2017, 78). An excellent example of a data section is provided by Doll and Hill (1950). They are interested in the effect of smoking between control and treatment groups. After clearly describing their dataset they use tables to display relevant cross-tabs and graphs to contrast groups.\nIn the data section we need to thoroughly discuss the variables in the dataset that we are using. If there are other datasets that could have been used, but were not, then this should be mentioned and the choice justified. If variables were constructed or combined, then this process and motivation should be explained.\nWe want the reader to understand what the data that underpin the results look like. This means that we should graph the data that are used in our analysis, or as close to them as possible. And we should also include tables of summary statistics. If the dataset was created from some other source, then it can also help to include an example of that original source. For instance, if the dataset was created from survey responses then the underlying survey questions should be included in an appendix.\nSome judgment is required when it comes to the figures and tables in the data section. The reader should have the opportunity to understand the details, but it may be that some are better placed in an appendix. Figures and tables are a critical aspect of convincing people of a story. In a graph we can show the data and then let the reader decide for themselves. And using a table, we can summarize a dataset. At the very least, every variable should be shown in a graph and summarized in a table. If there are too many, then some of these could be relegated to an appendix, with the critical relationships shown in the main body. Figures and tables should be numbered and then cross-referenced in the text, for instance, “Figure 1 shows\\(\\dots\\)”, “Table 1 describes\\(\\dots\\)”. For every graph and table there should be accompanying text that describes their main aspects, and adds additional detail."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-22",
    "href": "lectures/lecture-07-notes.html#section-22",
    "title": "Writing and Developing Research Questions",
    "section": "5.11 ",
    "text": "5.11 \nWe discuss the components of graphs and tables, including titles and labels, in ?@sec-static-communication. But here we will discuss captions, as they are between the text and the graph or table. Captions need to be informative and self-contained. Borkin et al. (2015) use eye-tracking to understand how visualizations are recognized and recalled. They find that captions need to make the central message of the figure clear, and that there should be redundancy. As Cleveland ([1985] 1994, 57) says, the “interplay between graph, caption, and text is a delicate one”, however the reader should be able to read only the caption and understand what the graph or table shows. A caption that is two lines long is not necessarily inappropriate. And all aspects of the graph or table should be explained. For instance, consider Figure 5 (a) and Figure 5 (b), both from Bowley (1901, 151). They are clear, and self-contained.\n\n\n\n\n\n\n\n\n\n\n\n(a) Example of a well-captioned figure\n\n\n\n\n\n\n\n\n\n\n\n(b) Example of a well-captioned table\n\n\n\n\n\n\n\nFigure 5: Examples of a graph and table from Bowley (1901)\n\n\n\nThe choice between a table and a graph comes down to how much information is to be conveyed. In general, if there is specific information that should be considered, such as a summary statistic, then a table is a good option. If we are interested in the reader making comparisons and understanding trends, then a graph is a good option (Gelman, Pasarica, and Dodhia 2002)."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-23",
    "href": "lectures/lecture-07-notes.html#section-23",
    "title": "Writing and Developing Research Questions",
    "section": "5.12 ",
    "text": "5.12 \n\n5.12.1 Model\nWe often build a statistical model that we will use to explore the data, and it is normal to have a specific section about this. At a minimum you should specify the equations that describe the model being used and explain their components with plain language and cross-references.\nThe model section typically begins with the model being written out, explained, and justified. Depending on the expected reader, some background may be needed. After specifying the model with appropriate mathematical notation and cross-referencing it, the components of the model should then be defined and explained. Try to define each aspect of the notation. This helps convince the reader that the model was well-chosen and enhances the credibility of the paper. The model’s variables should correspond to those that were discussed in the data section, making a clear link between the two sections.\nThere should be some discussion of how features enter the model and why. Some examples could include:\n\nWhy use age rather than age-groups?\nWhy does state/province have a levels effect?\nWhy is gender a categorical variable? In general, we are trying to convey a sense that this is the appropriate model for the situation. We want the reader to understand how the aspects that were discussed in the data section assert themselves in the modeling decisions that were made.\n\nThe model section should close with some discussion of the assumptions that underpin the model. It should also have a brief discussion of alternative models or variants. You want the strengths and weaknesses to be clear and for the reader to know why this particular model was chosen.\nAt some point in this section, it is usually appropriate to specify the software that was used to run the model, and to provide some evidence of thought about the circumstances in which the model may not be appropriate. That second point would typically be expanded on in the discussion section. And there should be evidence of model validation and checking, model convergence, and/or diagnostic issues. Again, there is a balance needed here, and some of this content may be more appropriately placed in appendices.\nWhen technical terms are used, they should be briefly explained in plain language for readers who might not be familiar with it. For instance, M. Alexander (2019) integrates an explanation of the Gini coefficient that brings the reader along.\n\nTo look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.\n\nThere may be papers that do not include a statistical model. In that case, this “Model” section should be replaced by a broader “Methodology” section. It might describe the simulation that was conducted, or contain more general details about the approach."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-24",
    "href": "lectures/lecture-07-notes.html#section-24",
    "title": "Writing and Developing Research Questions",
    "section": "5.13 ",
    "text": "5.13 \n\n5.13.1 Results\nTwo excellent examples of results sections are provided by Kharecha and Hansen (2013) and Kiang et al. (2021). In the results section, we want to communicate the outcomes of the analysis in a clear way and without too much focus on the discussion of implications. The results section likely requires summary statistics, tables, and graphs. Each of those aspects should be cross-referenced and have text associated with them that details what is seen in each figure. This section should relay results; that is, we are interested in what the results are, rather than what they mean.\nThis section would also typically include tables of graphs of coefficient estimates based on the modeling. Various features of the estimates should be discussed, and differences between the models explained. It may be that different subsets of the data are considered separately. Again, all graphs and tables need to have text in plain language accompany them. A rough guide is that the amount of text should be at least equal to the amount of space taken up by the tables and graphs. For instance, if a full page is used to display a table of coefficient estimates, then that should be cross-referenced and accompanied by about a full page of text about that table."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-25",
    "href": "lectures/lecture-07-notes.html#section-25",
    "title": "Writing and Developing Research Questions",
    "section": "5.14 ",
    "text": "5.14 \n\n5.14.1 Discussion\nA discussion section may be the final section of a paper and would typically have four or five sub-sections.\nThe discussion section would typically begin with a sub-section that comprises a brief summary of what was done in the paper. This would be followed by two or three sub-sections that are devoted to the key things that we learn about the world from this paper. These sub-sections are the main opportunity to justify or detail the implications of the story being told in the paper. Typically, these sub-sections do not see newly introduced graphs or tables, but are instead focused on what we learn from those that were introduced in earlier sections. It may be that some of the results are discussed in relation to what others have found, and differences could be attempted to be reconciled here.\nFollowing these sub-sections of what we learn about the world, we would typically have a sub-section focused on some of the weaknesses of what was done. This could concern aspects such as the data that were used, the approach, and the model. In the case of the model we are especially concerned with those aspects that might affect the findings. This can be especially difficult in the case of machine learning models and Smith et al. (2022) provide guidance for aspects to consider. And the final sub-section is typically a few paragraphs that specify what is left to learn, and how future work could proceed.\nIn general, we would expect this section to take at least 25 per cent of the total paper. This means that in an eight-page paper we would expect at least two pages of discussion."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#section-26",
    "href": "lectures/lecture-07-notes.html#section-26",
    "title": "Writing and Developing Research Questions",
    "section": "5.15 ",
    "text": "5.15 \n\n5.15.1 Brevity, typos, and grammar\nBrevity is important. This is partly because we write for the reader, and the reader has other priorities. But it is also because as the writer it forces us to consider what our most important points are, how we can best support them, and where our arguments are weakest. Jean Chrétien, is a former Canadian prime minister. In Chrétien (2007, 105) he wrote that he used to ask “\\(\\dots\\)the officials to summarize their documents in two or three pages and attach the rest of the materials as background information. I soon discovered that this was a problem only for those who didn’t really know what they were talking about” .\nThis experience is not unique to Canada and it is not new. In Hughes and Rutter (2016) Oliver Letwin, the former British cabinet member, describes there being “a huge amount of terrible guff, at huge, colossal, humongous length coming from some departments” and how he asked “for them to be one quarter of the length”. He found that the departments were able to accommodate this request without losing anything important. Winston Churchill asked for brevity during the Second World War, saying “the discipline of setting out the real points concisely will prove an aid to clearer thinking.” The letter from Szilard and Einstein to FDR that was the catalyst for the Manhattan Project was only two pages!\nZinsser (1976) goes further and describes “the secret of good writing” being “to strip every sentence to its cleanest components.” Every sentence should be simplified to its essence. And every word that does not contribute should be removed.\nUnnecessary words, typos, and grammatical issues should be removed from papers. These mistakes affect the credibility of claims. If the reader cannot trust you to use a spell-checker, then why should they trust you to use logistic regression? RStudio has a spell-checker built in, but Microsoft Word and Google Docs are useful additional checks. Copy from the Quarto document and paste into Word, then look for the red and green lines, and fix them in the Quarto document.\nWe are not worried about the n-th degree of grammatical content. Instead, we are interested in grammar and sentence structure that occurs in conversational language use (King 2000, 118). The way to develop comfort is by reading widely and asking others to also read your work. Another useful tactic is to read your writing aloud, which can be useful for detecting odd sentences based on how they sound. One small aspect to check that will regularly come up is that any number from one to ten should be written as words, while 11 and over should be written as numbers."
  },
  {
    "objectID": "lectures/lecture-07-notes.html#rules-for-writing",
    "href": "lectures/lecture-07-notes.html#rules-for-writing",
    "title": "Writing and Developing Research Questions",
    "section": "5.16 Rules for writing",
    "text": "5.16 Rules for writing\nA variety of authors have established rules for writing. This famously includes those of Orwell (1946) which were reimagined by The Economist (2013). A further reimagining of rules for writing, focused on telling stories with data, could be:\n\nFocus on the reader and their needs. Everything else is commentary.\nEstablish a structure and then rely on that to tell the story.\nWrite a first draft as quickly as possible.\nRewrite that draft extensively.\nBe concise and direct. Remove as many words as possible.\nUse words precisely. For instance, stock prices rise or fall, rather than improve or worsen.\nUse short sentences where possible.\nAvoid jargon.\nWrite as though your work will be on the front page of a newspaper.\nNever claim novelty or that you are the “first to study X”—there is always someone else who got there first.\n\nFiske and Kuriwaki (2021) have a list of rules for scientific papers and the appendix of Pineau et al. (2021) provides a checklist for machine learning papers. But perhaps the last word should be from Savage and Yeh (2019):\n\n[T]ry to write the best version of your paper: the one that you like. You can’t please an anonymous reader, but you should be able to please yourself. Your paper—you hope—is for posterity.\nSavage and Yeh (2019, 442)"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#footnotes",
    "href": "lectures/lecture-07-notes.html#footnotes",
    "title": "Writing and Developing Research Questions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile there is sometimes a need for a separate literature review section, another approach is to discuss relevant literature throughout the paper as appropriate. For instance, when there is literature relevant to the data then it should be discussed in this section, while literature relevant to the model, results, or discussion should be mentioned as appropriate in those sections.↩︎"
  },
  {
    "objectID": "lectures/lecture-09-notes.html",
    "href": "lectures/lecture-09-notes.html",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required: \nRecommended:  + \n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#reading-assignment",
    "href": "lectures/lecture-09-notes.html#reading-assignment",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "Required: \nRecommended:  +"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#lecture-slides",
    "href": "lectures/lecture-09-notes.html#lecture-slides",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#key-concepts-and-skills",
    "href": "lectures/lecture-09-notes.html#key-concepts-and-skills",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.1 Key concepts and skills",
    "text": "1.1 Key concepts and skills\n\nVisualization is one way to get a sense of our data and to communicate this to the reader. Plotting the observations in a dataset is important.\nWe need to be comfortable with a variety of graph types, including: bar charts, scatterplots, line plots, and histograms. We can even consider a map to be a type of graph, especially after geocoding our data.\nWe should also summarize data using tables. Typical use cases for this include showing part of a dataset, summary statistics, and regression results."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#software-and-packages",
    "href": "lectures/lecture-09-notes.html#software-and-packages",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.2 Software and packages",
    "text": "1.2 Software and packages\n\n\n\nBase R (R Core Team 2023)\ncarData (Fox, Weisberg, and Price 2022)\ndatasauRus (Davies, Locke, and D’Agostino McGowan 2022)\nggmap (Kahle and Wickham 2013)\njanitor (Firke 2023)\nknitr (Xie 2023)\nmaps (Becker et al. 2022)\nmapproj (McIlroy et al. 2023)\nmodelsummary (Arel-Bundock 2022)\nopendatatoronto (Gelfand 2022)\npatchwork (Pedersen 2022)\ntidygeocoder (Cambon and Belanger 2021)\ntidyverse (Wickham et al. 2019)\ntroopdata (Flynn 2022)\nWDI (Arel-Bundock 2021)\n\n\n\n\n\nlibrary(carData)\nlibrary(datasauRus)\nlibrary(ggmap)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(modelsummary)\nlibrary(opendatatoronto)\nlibrary(patchwork)\nlibrary(tidygeocoder)\nlibrary(tidyverse)\nlibrary(troopdata)\nlibrary(WDI)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#introduction",
    "href": "lectures/lecture-09-notes.html#introduction",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.3 Introduction",
    "text": "1.3 Introduction\nWhen telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.\nTry to show the observations that underpin our analysis. For instance, if your dataset consists of 2,500 responses to a survey, then at some point in the paper you should have a plot/s that contains each of the 2,500 observations, for every variable of interest. To do this we build graphs using ggplot2 which is part of the core tidyverse and so does not have to be installed or loaded separately. In this chapter we go through a variety of different options including bar charts, scatterplots, line plots, and histograms.\nIn contrast to the role of graphs, which is to show each observation, the role of tables is typically to show an extract of the dataset or to convey various summary statistics, or regression results. We will build tables primarily using knitr. Later we will use modelsummary to build tables related to regression output.\nFinally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using ggmap after having obtained geocoded data using tidygeocoder."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#graphs",
    "href": "lectures/lecture-09-notes.html#graphs",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.4 Graphs",
    "text": "1.4 Graphs\nGraphs are a critical aspect of compelling data stories. They allow us to see both broad patterns and details (Cleveland [1985] 1994, 5). Graphs enable a familiarity with our data that is hard to get from any other method. Every variable of interest should be graphed.\nThe most important objective of a graph is to convey as much of the actual data, and its context, as possible. In a way, graphing is an information encoding process where we construct a deliberate representation to convey information to our audience. The audience must decode that representation. The success of our graph depends on how much information is lost in this process so the decoding is a critical aspect (Cleveland [1985] 1994, 221). This means that we must focus on creating effective graphs that are suitable for our specific audience."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section",
    "href": "lectures/lecture-09-notes.html#section",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.5 ",
    "text": "1.5 \nTo see why graphing the actual data is important, after installing and loading datasauRus consider the datasaurus_dozen dataset.\n\ndatasaurus_dozen\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# ℹ 1,836 more rows\n\n\nThe dataset consists of values for “x” and “y”, which should be plotted on the x-axis and y-axis, respectively. There are 13 different values in the variable “dataset” including: “dino”, “star”, “away”, and “bullseye”. We focus on those four and generate summary statistics for each (Table 1).\n\n# Based on: https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  summarise(across(c(x, y), list(mean = mean, sd = sd)),\n            .by = dataset) |&gt;\n  kable(col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n        booktabs = TRUE, digits = 1)\n\n\n\nTable 1: Mean and standard deviation for four datasauRus datasets\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\ndino\n54.3\n16.8\n47.8\n26.9\n\n\naway\n54.3\n16.8\n47.8\n26.9\n\n\nstar\n54.3\n16.8\n47.8\n26.9\n\n\nbullseye\n54.3\n16.8\n47.8\n26.9"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-1",
    "href": "lectures/lecture-09-notes.html#section-1",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.6 ",
    "text": "1.6 \nNotice that the summary statistics are similar (Table 1). Despite this it turns out that the different datasets are actually very different beasts. This becomes clear when we plot the data (Figure 1).\n\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  ggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(color = \"Dataset\")\n\n\n\n\n\n\n\nFigure 1: Graph of four datasauRus datasets"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-2",
    "href": "lectures/lecture-09-notes.html#section-2",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.7 ",
    "text": "1.7 \nWe get a similar lesson—always plot your data—from “Anscombe’s Quartet”, created by the twentieth century statistician Frank Anscombe. The key takeaway is that it is important to plot the actual data and not rely solely on summary statistics.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-3",
    "href": "lectures/lecture-09-notes.html#section-3",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.8 ",
    "text": "1.8 \nAnscombe’s Quartet consists of eleven observations for four different datasets, with x and y values for each observation. We need to manipulate this dataset with pivot_longer() to get it into the “tidy” format discussed in ?@sec-r-essentials.\n\n# From: https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# And the pivot_longer() vignette.\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(\n    everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-4",
    "href": "lectures/lecture-09-notes.html#section-4",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.9 ",
    "text": "1.9 \nWe can first create summary statistics (Table 2) and then plot the data (Figure 2). This again illustrates the importance of graphing the actual data, rather than relying on summary statistics.\n\ntidy_anscombe |&gt;\n  summarise(\n    across(c(x, y), list(mean = mean, sd = sd)),\n    .by = set\n    ) |&gt;\n  kable(\n    col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n    digits = 1, booktabs = TRUE\n  )\n\n\n\nTable 2: Mean and standard deviation for Anscombe’s quartet\n\n\n\n\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\n1\n9\n3.3\n7.5\n2\n\n\n2\n9\n3.3\n7.5\n2\n\n\n3\n9\n3.3\n7.5\n2\n\n\n4\n9\n3.3\n7.5\n2"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-5",
    "href": "lectures/lecture-09-notes.html#section-5",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.10 ",
    "text": "1.10 \n\ntidy_anscombe |&gt;\n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Recreation of Anscombe’s Quartet"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#bar-charts",
    "href": "lectures/lecture-09-notes.html#bar-charts",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.11 Bar charts",
    "text": "1.11 Bar charts\nWe typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in ?@sec-fire-hose when we constructed a graph of the number of occupied beds. The geometric object—a “geom”—that we primarily use is geom_bar(), but there are many variants to cater for specific situations. To illustrate the use of bar charts, we use a dataset from the 1997-2001 British Election Panel Study that was put together by Fox and Andersen (2006) and made available with BEPS, after installing and loading carData.\n\nbeps &lt;- \n  BEPS |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  select(age, vote, gender, political_knowledge)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-6",
    "href": "lectures/lecture-09-notes.html#section-6",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.12 ",
    "text": "1.12 \nThe dataset consists of which party the respondent supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondent. We begin by creating age-groups from the ages, and making a bar chart showing the frequency of each age-group using geom_bar() (Figure 3 (a)).\n\nbeps &lt;-\n  beps |&gt;\n  mutate(\n    age_group =\n      case_when(\n        age &lt; 35 ~ \"&lt;35\",\n        age &lt; 50 ~ \"35-49\",\n        age &lt; 65 ~ \"50-64\",\n        age &lt; 80 ~ \"65-79\",\n        age &lt; 100 ~ \"80-99\"\n      ),\n    age_group = \n      factor(age_group, levels = c(\"&lt;35\", \"35-49\", \"50-64\", \"65-79\", \"80-99\"))\n  )"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-7",
    "href": "lectures/lecture-09-notes.html#section-7",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.13 ",
    "text": "1.13 \nbeps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\nbeps |&gt; \n  count(age_group) |&gt; \n  ggplot(mapping = aes(x = age_group, y = n)) +\n  geom_col() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using count() and geom_col()\n\n\n\n\n\n\n\nFigure 3: Distribution of age-groups in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-8",
    "href": "lectures/lecture-09-notes.html#section-8",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.14 ",
    "text": "1.14 \nThe default axis label used by ggplot2 is the name of the relevant variable, so it is often useful to add more detail. We do this using labs() by specifying a variable and a name. In the case of Figure 3 (a) we have specified labels for the x-axis and y-axis.\nBy default, geom_bar() creates a count of the number of times each age-group appears in the dataset. It does this because the default statistical transformation—a “stat”—for geom_bar() is “count”, which saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with beps |&gt; count(age_group)), then we could specify a variable for the y-axis and then use geom_col() (Figure 3 (b))."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-9",
    "href": "lectures/lecture-09-notes.html#section-9",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.15 ",
    "text": "1.15 \nWe may also like to consider various groupings of the data to get a different insight. For instance, we can use color to look at which party the respondent supports, by age-group (Figure 4 (a)).\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge2\") +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n(b) Using geom_bar() with dodge2\n\n\n\n\n\n\n\nFigure 4: Distribution of age-group, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\nBy default, these different groups are stacked, but they can be placed side by side with position = \"dodge2\" (Figure 4 (b)). (Using “dodge2” rather than “dodge” adds a little space between the bars.)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#themes",
    "href": "lectures/lecture-09-notes.html#themes",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.16 Themes",
    "text": "1.16 Themes\nAt this point, we may like to address the general look of the graph. There are various themes that are built into ggplot2. These include: theme_bw(), theme_classic(), theme_dark(), and theme_minimal(). A full list is available in the ggplot2 cheat sheet. We can use these themes by adding them as a layer (Figure 5). We could also install more themes from other packages, including ggthemes (Arnold 2021), and hrbrthemes (Rudis 2020). We could even build our own!\n\ntheme_bw &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\n\n\n\n\n\n\n\nFigure 5: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes and the use of patchwork\n\n\n\n\n\nIn Figure 5 we use patchwork to bring together multiple graphs. To do this, after installing and loading the package, we assign the graph to a variable. We then use “+” to signal which should be next to each other, “/” to signal which should be on top, and use brackets to indicate precedence"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#facets",
    "href": "lectures/lecture-09-notes.html#facets",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.17 Facets",
    "text": "1.17 Facets\nWe use facets to show variation, based on one or more variables (Wilkinson 2005, 219). Facets are especially useful when we have already used color to highlight variation in some other variable. For instance, we may be interested to explain vote, by age and gender (Figure 6). We rotate the x-axis with guides(x = guide_axis(angle = 90)) to avoid overlapping. We also change the position of the legend with theme(legend.position = \"bottom\").\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 6: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-10",
    "href": "lectures/lecture-09-notes.html#section-10",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.18 ",
    "text": "1.18 \nWe could change facet_wrap() to wrap vertically instead of horizontally with dir = \"v\". Alternatively, we could specify a few rows, say nrow = 2, or a number of columns, say ncol = 2.\nBy default, both facets will have the same x-axis and y-axis. We could enable both facets to have different scales with scales = \"free\", or just the x-axis with scales = \"free_x\", or just the y-axis with scales = \"free_y\" (Figure 7).\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote), scales = \"free\") +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 7: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-11",
    "href": "lectures/lecture-09-notes.html#section-11",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.19 ",
    "text": "1.19 \nFinally, we can change the labels of the facets using labeller() (Figure 8).\n\nnew_labels &lt;- \n  c(\"0\" = \"No knowledge\", \"1\" = \"Low knowledge\",\n    \"2\" = \"Moderate knowledge\", \"3\" = \"High knowledge\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(\n    vars(political_knowledge),\n    scales = \"free\",\n    labeller = labeller(political_knowledge = new_labels)\n  ) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 8: Distribution of age-group by political knowledge, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\n\nWe now have three ways to combine multiple graphs: sub-figures, facets, and patchwork. They are useful in different circumstances:\n\nsub-figures—which we covered in ?@sec-reproducible-workflows—for when we are considering different variables;\nfacets for when we are considering a categorical variable; and\npatchwork for when we are interested in bringing together entirely different graphs."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#colors",
    "href": "lectures/lecture-09-notes.html#colors",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.20 Colors",
    "text": "1.20 Colors\nWe now turn to the colors used in the graph. There are a variety of different ways to change the colors. The many palettes available from RColorBrewer (Neuwirth 2022) can be specified using scale_fill_brewer(). In the case of viridis (Garnier et al. 2021) we can specify the palettes using scale_fill_viridis_d(). Additionally, viridis is particularly focused on color-blind palettes (Figure 9). Neither RColorBrewer nor viridis need to be explicitly installed or loaded because ggplot2, which is part of the tidyverse, takes care of that for us."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-12",
    "href": "lectures/lecture-09-notes.html#section-12",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.21 ",
    "text": "1.21 \n# Panel (a)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Blues\")\n\n# Panel (b)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Panel (c)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d()\n\n# Panel (d)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 9: Distribution of age-group and vote preference, in the 1997-2001 British Election Panel Study, illustrating different colors"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#scatterplots",
    "href": "lectures/lecture-09-notes.html#scatterplots",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.22 Scatterplots",
    "text": "1.22 Scatterplots\nWe are often interested in the relationship between two numeric or continuous variables. We can use scatterplots to show this. A scatterplot may not always be the best choice, but it is rarely a bad one (Weissgerber et al. 2015). Some consider it the most versatile and useful graph option (Friendly and Wainer 2021, 121). To illustrate scatterplots, we install and load WDI and then use that to download some economic indicators from the World Bank. In particular, we use WDIsearch() to find the unique key that we need to pass to WDI() to facilitate the download."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-13",
    "href": "lectures/lecture-09-notes.html#section-13",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.23 ",
    "text": "1.23 \n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nFrom OECD (2014, 15) Gross Domestic Product (GDP) “combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country’s economic territory.” The modern concept was developed by the twentieth century economist Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the economic activity of a country. It is useful and informative that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and disaggregated differences can be important (Moyer and Dunn 2020). It highlights short term economic progress over longer term improvements. And “the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable” (Kuznets, Epstein, and Jenks 1941, xxvi). Summary measures of economic performance shows only one side of a country’s economy. While there are many strengths there are also well-known areas where GDP is weak."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-14",
    "href": "lectures/lecture-09-notes.html#section-14",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.24 ",
    "text": "1.24 \n\nWDIsearch(\"gdp growth\")\nWDIsearch(\"inflation\")\nWDIsearch(\"population, total\")\nWDIsearch(\"Unemployment, total\")"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-15",
    "href": "lectures/lecture-09-notes.html#section-15",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.25 ",
    "text": "1.25 \n\nworld_bank_data &lt;-\n  WDI(\n    indicator =\n      c(\"FP.CPI.TOTL.ZG\", \"NY.GDP.MKTP.KD.ZG\", \"SP.POP.TOTL\",\"SL.UEM.TOTL.NE.ZS\"),\n    country = c(\"AU\", \"ET\", \"IN\", \"US\")\n  )"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-16",
    "href": "lectures/lecture-09-notes.html#section-16",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.26 ",
    "text": "1.26 \nWe may like to change the variable names to be more meaningful, and only keep those that we need.\n\nworld_bank_data &lt;-\n  world_bank_data |&gt;\n  rename(\n    inflation = FP.CPI.TOTL.ZG,\n    gdp_growth = NY.GDP.MKTP.KD.ZG,\n    population = SP.POP.TOTL,\n    unem_rate = SL.UEM.TOTL.NE.ZS\n  ) |&gt;\n  select(country, year, inflation, gdp_growth, population, unem_rate)\n\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unem_rate\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia  1960     3.73       NA      10276477        NA\n2 Australia  1961     2.29        2.48   10483000        NA\n3 Australia  1962    -0.319       1.29   10742000        NA\n4 Australia  1963     0.641       6.22   10950000        NA\n5 Australia  1964     2.87        6.98   11167000        NA\n6 Australia  1965     3.41        5.98   11388000        NA"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-17",
    "href": "lectures/lecture-09-notes.html#section-17",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.27 ",
    "text": "1.27 \nTo get started we can use geom_point() to make a scatterplot showing GDP growth and inflation, by country (Figure 10 (a)).\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default settings\n\n\n\n\n\n\n\n\n\n\n\n(b) With the addition of a theme and labels\n\n\n\n\n\n\n\nFigure 10: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nAs with bar charts, we can change the theme, and update the labels (Figure 10 (b))."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-18",
    "href": "lectures/lecture-09-notes.html#section-18",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.28 ",
    "text": "1.28 \nFor scatterplots we use “color” instead of “fill”, as we did for bar charts, because they use dots rather than bars. This also then slightly affects how we change the palette (Figure 11). That said, with particular types of dots, for instance shape = 21, it is possible to have both fill and color aesthetics.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Blues\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d()\n\n# Panel (d)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\n\n\n\nFigure 11: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-19",
    "href": "lectures/lecture-09-notes.html#section-19",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.29 ",
    "text": "1.29 \nThe points of a scatterplot sometimes overlap. We can address this situation in a variety of ways (Figure 12):\n\nAdding a degree of transparency to our dots with “alpha” (Figure 12 (a)). The value for “alpha” can vary between 0, which is fully transparent, and 1, which is completely opaque.\nAdding a small amount of noise, which slightly moves the points, using geom_jitter() (Figure 12 (b)). By default, the movement is uniform in both directions, but we can specify which direction movement occurs with “width” or “height”. The decision between these two options turns on the degree to which accuracy matters, and the number of points: it is often useful to use geom_jitter() when you want to highlight the relative density of points and not necessarily the exact value of individual points. When using geom_jitter() it is a good idea to set a seed, as introduced in ?@sec-fire-hose, for reproducibility."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-20",
    "href": "lectures/lecture-09-notes.html#section-20",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.30 ",
    "text": "1.30 \nset.seed(853)\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country )) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter(width = 1, height = 1) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Changing the alpha setting\n\n\n\n\n\n\n\n\n\n\n\n(b) Using jitter\n\n\n\n\n\n\n\nFigure 12: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-21",
    "href": "lectures/lecture-09-notes.html#section-21",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.31 ",
    "text": "1.31 \nWe often use scatterplots to illustrate a relationship between two continuous variables. It can be useful to add a “summary” line using geom_smooth() (Figure 13). We can specify the relationship using “method”, change the color with “color”, and add or remove standard errors with “se”. A commonly used “method” is lm, which computes and plots a simple linear regression line similar to using the lm() function. Using geom_smooth() adds a layer to the graph, and so it inherits aesthetics from ggplot(). For instance, that is why we have one line for each country in Figure 13 (a) and Figure 13 (b). We could overwrite that by specifying a particular color (Figure 13 (c)). There are situation where other types of fitted lines such as splines might be preferred.\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default line of best fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Specifying a linear relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Specifying only one color\n\n\n\n\n\n\n\nFigure 13: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#line-plots",
    "href": "lectures/lecture-09-notes.html#line-plots",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.32 Line plots",
    "text": "1.32 Line plots\nWe can use a line plot when we have variables that should be joined together, for instance, an economic time series. We will continue with the dataset from the World Bank and focus on GDP growth in the United States using geom_line() (Figure 14 (a)). The source of the data can be added to the graph using “caption” within labs().\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Using a line plot\n\n\n\n\n\n\n\n\n\n\n\n(b) Using a stairstep line plot\n\n\n\n\n\n\n\nFigure 14: United States GDP growth (1961-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-22",
    "href": "lectures/lecture-09-notes.html#section-22",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.33 ",
    "text": "1.33 \nWe can use geom_step(), a slight variant of geom_line(), to focus attention on the change from year to year (Figure 14 (b)).\nThe Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the United Kingdom between 1861 and 1957 (Phillips 1958). We have a variety of ways to investigate this relationship in our data, including:\n\nAdding a second line to our graph. For instance, we could add inflation (Figure 15 (a)). This requires us to use pivot_longer(), which is discussed in ?@sec-r-essentials, to ensure that the data are in a tidy format.\nUsing geom_path() to link values in the order they appear in the dataset. In Figure 15 (b) we show a Phillips curve for the United States between 1960 and 2020. Figure 15 (b) does not appear to show any clear relationship between unemployment and inflation.\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  select(-population, -gdp_growth) |&gt;\n  pivot_longer(\n    cols = c(\"inflation\", \"unem_rate\"),\n    names_to = \"series\",\n    values_to = \"value\"\n  ) |&gt;\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Year\", y = \"Value\", color = \"Economic indicator\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = unem_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(\n    x = \"Unemployment rate\", y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparing the two time series over time\n\n\n\n\n\n\n\n\n\n\n\n(b) Plotting the two time series against each other\n\n\n\n\n\n\n\nFigure 15: Unemployment and inflation for the United States (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#histograms",
    "href": "lectures/lecture-09-notes.html#histograms",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.34 Histograms",
    "text": "1.34 Histograms\nA histogram is useful to show the shape of the distribution of a continuous variable. The full range of the data values is split into intervals called “bins” and the histogram counts how many observations fall into which bin. In Figure 16 we examine the distribution of GDP in Ethiopia.\n\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\nFigure 16: Distribution of GDP growth in Ethiopia (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-23",
    "href": "lectures/lecture-09-notes.html#section-23",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.35 ",
    "text": "1.35 \nThe key component that determines the shape of a histogram is the number of bins. This can be specified in one of two ways (Figure 17):\n\nspecifying the number of “bins” to include; or\nspecifying their “binwidth”.\n\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (d)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) Five bins\n\n\n\n\n\n\n\n\n\n\n\n(b) 20 bins\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Binwidth of two\n\n\n\n\n\n\n\n\n\n\n\n(d) Binwidth of five\n\n\n\n\n\n\n\nFigure 17: Distribution of GDP growth in Ethiopia (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-24",
    "href": "lectures/lecture-09-notes.html#section-24",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.36 ",
    "text": "1.36 \nHistograms can be thought of as locally averaging data, and the number of bins affects how much of this occurs. When there are only two bins then there is considerable smoothing, but we lose a lot of accuracy. Too few bins results in more bias, while too many bins results in more variance (Wasserman 2005, 303). Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal (Cleveland [1985] 1994, 135). This is one of the reasons that Denby and Mallows (2009) consider histograms to be especially valuable as exploratory tools."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-25",
    "href": "lectures/lecture-09-notes.html#section-25",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.37 ",
    "text": "1.37 \nFinally, while we can use “fill” to distinguish between different types of observations, it can get quite messy. It is usually better to:\n\ntrace the outline of the distribution with geom_freqpoly() (Figure 18 (a))\nbuild stack of dots with geom_dotplot() (Figure 18 (b)); or\nadd transparency, especially if the differences are more stark (Figure 18 (c)).\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = \"histodot\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country %in% c(\"India\", \"United States\")) |&gt;\n  ggplot(mapping = aes(x = gdp_growth, fill = country)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tracing the outline\n\n\n\n\n\n\n\n\n\n\n\n(b) Using dots\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding transparency\n\n\n\n\n\n\n\nFigure 18: Distribution of GDP growth across various countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-26",
    "href": "lectures/lecture-09-notes.html#section-26",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.38 ",
    "text": "1.38 \nAn interesting alternative to a histogram is the empirical cumulative distribution function (ECDF). The choice between this and a histogram is tends to be audience-specific. It may not appropriate for less-sophisticated audiences, but if the audience is quantitatively comfortable, then it can be a great choice because it does less smoothing than a histogram. We can build an ECDF with stat_ecdf(). For instance, Figure 19 shows an ECDF equivalent to Figure 16.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  stat_ecdf(geom = \"point\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Proportion\", color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 19: Distribution of GDP growth in four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#boxplots",
    "href": "lectures/lecture-09-notes.html#boxplots",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.39 Boxplots",
    "text": "1.39 Boxplots\nA boxplot typically shows five aspects: 1) the median, 2) the 25th, and 3) 75th percentiles. The fourth and fifth elements differ depending on specifics. One option is the minimum and maximum values. Another option is to determine the difference between the 75th and 25th percentiles, which is the interquartile range (IQR). The fourth and fifth elements are then the extreme observations within \\(1.5\\times\\mbox{IQR}\\) from the 25th and 75th percentiles. That latter approach is used, by default, in geom_boxplot from ggplot2. Spear (1952, 166) introduced the notion of a chart that focused on the range and various summary statistics including the median and the range, while Tukey (1977) focused on which summary statistics and popularized it (Wickham and Stryjewski 2011).\nOne reason for using graphs is that they help us understand and embrace how complex our data are, rather than trying to hide and smooth it away (Armstrong 2022). One appropriate use case for boxplots is to compare the summary statistics of many variables at once, such as in Bethlehem et al. (2022). But boxplots alone are rarely the best choice because they hide the distribution of data, rather than show it. The same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. The first contains draws from two beta distributions: one that is right skewed and another that is left skewed. The second contains draws from a beta distribution with no skew, noting that \\(\\mbox{Beta}(1, 1)\\) is equivalent to \\(\\mbox{Uniform}(0, 1)\\)."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-27",
    "href": "lectures/lecture-09-notes.html#section-27",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.40 ",
    "text": "1.40 \n\nset.seed(853)\n\nnumber_of_draws &lt;- 10000\n\nboth_left_and_right_skew &lt;-\n  c(\n    rbeta(number_of_draws / 2, 5, 2),\n    rbeta(number_of_draws / 2, 2, 5)\n  )\n\nno_skew &lt;-\n  rbeta(number_of_draws, 1, 1)\n\nbeta_distributions &lt;-\n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(\n      rep(\"Left and right skew\", number_of_draws),\n      rep(\"No skew\", number_of_draws)\n    )\n  )"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-28",
    "href": "lectures/lecture-09-notes.html#section-28",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.41 ",
    "text": "1.41 \nWe can first compare the boxplots of the two series (Figure 20 (a)). But if we plot the actual data then we can see how different they are (Figure 20 (b)).\nbeta_distributions |&gt;\n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\n\nbeta_distributions |&gt;\n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Illustrated with a boxplot\n\n\n\n\n\n\n\n\n\n\n\n(b) Actual data\n\n\n\n\n\n\n\nFigure 20: Data drawn from beta distributions with different parameters"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-29",
    "href": "lectures/lecture-09-notes.html#section-29",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "1.42 ",
    "text": "1.42 \nOne way forward, if a boxplot is to be used, is to include the actual data as a layer on top of the boxplot. For instance, in Figure 21 we show the distribution of inflation across the four countries. The reason that this works well is that it shows the actual observations, as well as the summary statistics.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Country\",\n    y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\nFigure 21: Distribution of inflation data for four countries (1960-2020)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-30",
    "href": "lectures/lecture-09-notes.html#section-30",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.1 ",
    "text": "2.1 \nTables are an important part of telling a compelling story. Tables can communicate less information than a graph, but they do so at a high fidelity. They are especially useful to highlight a few specific values (Andersen and Armstrong 2021). In this book, we primarily use tables in three ways:\n\nTo show an extract of the dataset.\nTo communicate summary statistics.\nTo display regression results."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#showing-part-of-a-dataset",
    "href": "lectures/lecture-09-notes.html#showing-part-of-a-dataset",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.2 Showing part of a dataset",
    "text": "2.2 Showing part of a dataset\nWe illustrate showing part of a dataset using kable() from knitr. We use the World Bank dataset that we downloaded earlier and focus on inflation, GDP growth, and population as unemployment data are not available for every year for every country.\n\nworld_bank_data &lt;- \n  world_bank_data |&gt; \n  select(-unem_rate)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-31",
    "href": "lectures/lecture-09-notes.html#section-31",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.3 ",
    "text": "2.3 \nTo begin, after installing and loading knitr, we can display the first ten rows with the default kable() settings.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable()\n\n\n\n\ncountry\nyear\ninflation\ngdp_growth\npopulation\n\n\n\n\nAustralia\n1960\n3.7288136\nNA\n10276477\n\n\nAustralia\n1961\n2.2875817\n2.482656\n10483000\n\n\nAustralia\n1962\n-0.3194888\n1.294611\n10742000\n\n\nAustralia\n1963\n0.6410256\n6.216107\n10950000\n\n\nAustralia\n1964\n2.8662420\n6.980061\n11167000\n\n\nAustralia\n1965\n3.4055728\n5.980438\n11388000\n\n\nAustralia\n1966\n3.2934132\n2.379040\n11651000\n\n\nAustralia\n1967\n3.4782609\n6.304945\n11799000\n\n\nAustralia\n1968\n2.5210084\n5.094034\n12009000\n\n\nAustralia\n1969\n3.2786885\n7.045584\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-32",
    "href": "lectures/lecture-09-notes.html#section-32",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.4 ",
    "text": "2.4 \nTo be able to cross-reference a table in the text, we need to add a table caption and label to the R chunk as shown in ?@sec-quartocrossreferences of ?@sec-reproducible-workflows. We can also make the column names more informative with “col.names” and specify the number of digits to be displayed (Table 3).\n\n```{r}\n#| label: tbl-gdpfirst\n#| message: false\n#| tbl-cap: \"A dataset of economic indicators for four countries\"\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1\n  )\n```\n\n\n\nTable 3: A dataset of economic indicators for four countries\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-33",
    "href": "lectures/lecture-09-notes.html#section-33",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.5 ",
    "text": "2.5 \n\n2.5.1 Improving the formatting\nWhen producing PDFs, the “booktabs” option makes a host of small changes to the default display and results in tables that look better (Table 4). (This should not have an effect for HTML output.) By default a small space will be added every five lines. We can additionally specify “linesep” to stop that.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 4: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-34",
    "href": "lectures/lecture-09-notes.html#section-34",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.6 ",
    "text": "2.6 \nWe can specify the alignment of the columns using a character vector of “l” (left), “c” (center), and “r” (right) (Table 5). Additionally, we can change the formatting. For instance, we could specify groupings for numbers that are at least 1,000 using format.args = list(big.mark = \",\").\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  mutate(year = as.factor(year)) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\",\n    align = c(\"l\", \"l\", \"c\", \"c\", \"r\", \"r\"),\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 5: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10,276,477\n\n\nAustralia\n1961\n2.3\n2.5\n10,483,000\n\n\nAustralia\n1962\n-0.3\n1.3\n10,742,000\n\n\nAustralia\n1963\n0.6\n6.2\n10,950,000\n\n\nAustralia\n1964\n2.9\n7.0\n11,167,000\n\n\nAustralia\n1965\n3.4\n6.0\n11,388,000\n\n\nAustralia\n1966\n3.3\n2.4\n11,651,000\n\n\nAustralia\n1967\n3.5\n6.3\n11,799,000\n\n\nAustralia\n1968\n2.5\n5.1\n12,009,000\n\n\nAustralia\n1969\n3.3\n7.0\n12,263,000"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#communicating-summary-statistics",
    "href": "lectures/lecture-09-notes.html#communicating-summary-statistics",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.7 Communicating summary statistics",
    "text": "2.7 Communicating summary statistics\nAfter installing and loading modelsummary we can use datasummary_skim() to create tables of summary statistics from our dataset.\nWe can use this to get a table such as Table 6. That might be useful for exploratory data analysis, which we cover in ?@sec-exploratory-data-analysis. (Here we remove population to save space and do not include a histogram of each variable.)\n\nworld_bank_data |&gt;\n  select(-population) |&gt; \n  datasummary_skim(histogram = FALSE)\n\n\n\nTable 6: Summary of economic indicator variables for four countries\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n              \n        \n        \n        \n                \n                  year\n                  62\n                  0\n                  1990.5\n                  17.9\n                  1960.0\n                  1990.5\n                  2021.0\n                \n                \n                  inflation\n                  243\n                  2\n                  6.1\n                  6.5\n                  -9.8\n                  4.3\n                  44.4\n                \n                \n                  gdp_growth\n                  224\n                  10\n                  4.2\n                  3.7\n                  -11.1\n                  3.9\n                  13.9\n                \n                \n                  country\n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Australia\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Ethiopia\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  India\n                  62\n                  25.0\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  United States\n                  62\n                  25.0"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-35",
    "href": "lectures/lecture-09-notes.html#section-35",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.8 ",
    "text": "2.8 \nBy default, datasummary_skim() summarizes the numeric variables, but we can ask for the categorical variables (Table 7). Additionally we can add cross-references in the same way as kable(), that is, include a “tbl-cap” entry and then cross-reference the name of the R chunk.\n\nworld_bank_data |&gt;\n  datasummary_skim(type = \"categorical\")\n\n\n\nTable 7: Summary of categorical economic indicator variables for four countries\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                country\n                N\n                %\n              \n        \n        \n        \n                \n                  Australia    \n                  62\n                  25.0\n                \n                \n                  Ethiopia     \n                  62\n                  25.0\n                \n                \n                  India        \n                  62\n                  25.0\n                \n                \n                  United States\n                  62\n                  25.0"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-36",
    "href": "lectures/lecture-09-notes.html#section-36",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.9 ",
    "text": "2.9 \nWe can create a table that shows the correlation between variables using datasummary_correlation() (Table 8).\n\nworld_bank_data |&gt;\n  datasummary_correlation()\n\n\n\nTable 8: Correlation between the economic indicator variables for four countries (Australia, Ethiopia, India, and the United States)\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                year\n                inflation\n                gdp_growth\n                population\n              \n        \n        \n        \n                \n                  year      \n                  1  \n                  .  \n                  .  \n                  .\n                \n                \n                  inflation \n                  .03\n                  1  \n                  .  \n                  .\n                \n                \n                  gdp_growth\n                  .11\n                  .01\n                  1  \n                  .\n                \n                \n                  population\n                  .25\n                  .06\n                  .16\n                  1"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-37",
    "href": "lectures/lecture-09-notes.html#section-37",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.10 ",
    "text": "2.10 \nWe typically need a table of descriptive statistics that we could add to our paper (Table 9). This contrasts with Table 7 which would likely not be included in the main section of a paper, and is more to help us understand the data. We can add a note about the source of the data using notes.\n\ndatasummary_balance(\n  formula = ~country,\n  data = world_bank_data |&gt; \n    filter(country %in% c(\"Australia\", \"Ethiopia\")),\n  dinm = FALSE,\n  notes = \"Data source: World Bank.\"\n)\n\n\n\nTable 9: Descriptive statistics for the inflation and GDP dataset\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nAustralia (N=62)\nEthiopia (N=62)\n\n        \n              \n                 \n                Mean\n                Std. Dev.\n                Mean\n                Std. Dev.\n              \n        \n        Data source: World Bank.\n        \n                \n                  year      \n                  1990.5    \n                  18.0     \n                  1990.5    \n                  18.0      \n                \n                \n                  inflation \n                  4.7       \n                  3.8      \n                  9.1       \n                  10.6      \n                \n                \n                  gdp_growth\n                  3.4       \n                  1.8      \n                  5.9       \n                  6.4       \n                \n                \n                  population\n                  17351313.1\n                  4407899.0\n                  57185292.0\n                  29328845.8"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#display-regression-results",
    "href": "lectures/lecture-09-notes.html#display-regression-results",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.11 Display regression results",
    "text": "2.11 Display regression results\nWe can report regression results using modelsummary() from modelsummary. For instance, we could display the estimates from a few different models (Table 10).\n\nfirst_model &lt;- lm(\n  formula = gdp_growth ~ inflation,\n  data = world_bank_data\n)\n\nsecond_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country,\n  data = world_bank_data\n)\n\nthird_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country + population,\n  data = world_bank_data\n)\n\nmodelsummary(list(first_model, second_model, third_model))\n\n\n\nTable 10: Explaining GDP as a function of inflation\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  4.147   \n                  3.676   \n                  3.611   \n                \n                \n                                      \n                  (0.343) \n                  (0.484) \n                  (0.482) \n                \n                \n                  inflation           \n                  0.006   \n                  -0.068  \n                  -0.065  \n                \n                \n                                      \n                  (0.039) \n                  (0.040) \n                  (0.039) \n                \n                \n                  countryEthiopia     \n                          \n                  2.896   \n                  2.716   \n                \n                \n                                      \n                          \n                  (0.740) \n                  (0.740) \n                \n                \n                  countryIndia        \n                          \n                  1.916   \n                  -0.730  \n                \n                \n                                      \n                          \n                  (0.642) \n                  (1.465) \n                \n                \n                  countryUnited States\n                          \n                  -0.436  \n                  -1.145  \n                \n                \n                                      \n                          \n                  (0.633) \n                  (0.722) \n                \n                \n                  population          \n                          \n                          \n                  0.000   \n                \n                \n                                      \n                          \n                          \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  223     \n                  223     \n                  223     \n                \n                \n                  R2                  \n                  0.000   \n                  0.111   \n                  0.127   \n                \n                \n                  R2 Adj.             \n                  -0.004  \n                  0.095   \n                  0.107   \n                \n                \n                  AIC                 \n                  1217.7  \n                  1197.5  \n                  1195.4  \n                \n                \n                  BIC                 \n                  1227.9  \n                  1217.9  \n                  1219.3  \n                \n                \n                  Log.Lik.            \n                  -605.861\n                  -592.752\n                  -590.704\n                \n                \n                  F                   \n                  0.024   \n                  6.806   \n                          \n                \n                \n                  RMSE                \n                  3.66    \n                  3.45    \n                  3.42    \n                \n        \n      \n    \n\n\n\n\n\n\nThe number of significant digits can be adjusted with “fmt” (Table 11). To help establish credibility you should generally not add as many significant digits as possible (Howes 2022). Instead, you should think carefully about the data-generating process and adjust based on that."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-38",
    "href": "lectures/lecture-09-notes.html#section-38",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "2.12 ",
    "text": "2.12 \n\nmodelsummary(\n  list(first_model, second_model, third_model),\n  fmt = 1\n)\n\n\n\nTable 11: Three models of GDP as a function of inflation\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  4.1     \n                  3.7     \n                  3.6     \n                \n                \n                                      \n                  (0.3)   \n                  (0.5)   \n                  (0.5)   \n                \n                \n                  inflation           \n                  0.0     \n                  -0.1    \n                  -0.1    \n                \n                \n                                      \n                  (0.0)   \n                  (0.0)   \n                  (0.0)   \n                \n                \n                  countryEthiopia     \n                          \n                  2.9     \n                  2.7     \n                \n                \n                                      \n                          \n                  (0.7)   \n                  (0.7)   \n                \n                \n                  countryIndia        \n                          \n                  1.9     \n                  -0.7    \n                \n                \n                                      \n                          \n                  (0.6)   \n                  (1.5)   \n                \n                \n                  countryUnited States\n                          \n                  -0.4    \n                  -1.1    \n                \n                \n                                      \n                          \n                  (0.6)   \n                  (0.7)   \n                \n                \n                  population          \n                          \n                          \n                  0.0     \n                \n                \n                                      \n                          \n                          \n                  (0.0)   \n                \n                \n                  Num.Obs.            \n                  223     \n                  223     \n                  223     \n                \n                \n                  R2                  \n                  0.000   \n                  0.111   \n                  0.127   \n                \n                \n                  R2 Adj.             \n                  -0.004  \n                  0.095   \n                  0.107   \n                \n                \n                  AIC                 \n                  1217.7  \n                  1197.5  \n                  1195.4  \n                \n                \n                  BIC                 \n                  1227.9  \n                  1217.9  \n                  1219.3  \n                \n                \n                  Log.Lik.            \n                  -605.861\n                  -592.752\n                  -590.704\n                \n                \n                  F                   \n                  0.024   \n                  6.806   \n                          \n                \n                \n                  RMSE                \n                  3.66    \n                  3.45    \n                  3.42"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-39",
    "href": "lectures/lecture-09-notes.html#section-39",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.1 ",
    "text": "3.1 \nIn many ways maps can be thought of as another type of graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or background image. It is possible that they are the oldest and best understood type of chart (Karsten 1923, 1). We can generate a map in a straight-forward manner. That said, it is not to be taken lightly; things quickly get complicated!\nThe first step is to get some data. There is some geographic data built into ggplot2 that we can access with map_data(). There are additional variables in the world.cities dataset from maps.\n\nfrance &lt;- map_data(map = \"france\")\n\nhead(france)\n\n      long      lat group order region subregion\n1 2.557093 51.09752     1     1   Nord      &lt;NA&gt;\n2 2.579995 51.00298     1     2   Nord      &lt;NA&gt;\n3 2.609101 50.98545     1     3   Nord      &lt;NA&gt;\n4 2.630782 50.95073     1     4   Nord      &lt;NA&gt;\n5 2.625894 50.94116     1     5   Nord      &lt;NA&gt;\n6 2.597699 50.91967     1     6   Nord      &lt;NA&gt;\n\nfrench_cities &lt;-\n  world.cities |&gt;\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n\n             name country.etc    pop   lat long capital\n1       Abbeville      France  26656 50.12 1.83       0\n2         Acheres      France  23219 48.97 2.06       0\n3            Agde      France  23477 43.33 3.46       0\n4            Agen      France  34742 44.20 0.62       0\n5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n6 Aix-en-Provence      France 148622 43.53 5.44       0"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-40",
    "href": "lectures/lecture-09-notes.html#section-40",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.2 ",
    "text": "3.2 \nUsing that information you can create a map of France that shows the larger cities (Figure 22). Use geom_polygon() from ggplot2 to draw shapes by connecting points within groups. And coord_map() adjusts for the fact that we are making a 2D map to represent a world that is 3D.\n\nggplot() +\n  geom_polygon(\n    data = france,\n    aes(x = long, y = lat, group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map() +\n  geom_point(\n    aes(x = french_cities$long, y = french_cities$lat),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  theme_minimal() +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n\n\n\n\n\nFigure 22: Map of France showing the largest cities"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-41",
    "href": "lectures/lecture-09-notes.html#section-41",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.3 ",
    "text": "3.3 \nAs is often the case with R, there are many ways to get started creating static maps. We have seen how they can be built using only ggplot2, but ggmap brings additional functionality.\nThere are two essential components to a map:\n\na border or background image (sometimes called a tile); and\nsomething of interest within that border, or on top of that tile.\n\nIn ggmap, we use an open-source option for our tile, Stamen Maps. And we use plot points based on latitude and longitude."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#australian-polling-places",
    "href": "lectures/lecture-09-notes.html#australian-polling-places",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.4 Australian polling places",
    "text": "3.4 Australian polling places\nIn Australia, people have to go to “booths” in order to vote. Because the booths have coordinates (latitude and longitude), we can plot them. One reason we may like to do that is to notice spatial voting patterns.\nTo get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap. The main argument to this function is to specify a bounding box. A bounding box is the coordinates of the edges that you are interested in. This requires two latitudes and two longitudes."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-42",
    "href": "lectures/lecture-09-notes.html#section-42",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.5 ",
    "text": "3.5 \nIt can be useful to use Google Maps, or other mapping platform, to find the coordinate values that you need. In this case we have provided it with coordinates such that it will be centered around Australia’s capital Canberra.\n\nbbox &lt;- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-43",
    "href": "lectures/lecture-09-notes.html#section-43",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.6 ",
    "text": "3.6 \nIt is free, but we need to register in order to get a map. To do this go to https://client.stadiamaps.com/signup/ and create an account. Then create a new property, then “Add API Key”. Copy the key and run (replacing PUT-KEY-HERE with the key) register_stadiamaps(key = \"PUT-KEY-HERE\", write = TRUE). Then once you have defined the bounding box, the function get_stadiamap() will get the tiles in that area (?@fig-heyitscanberra). The number of tiles that it needs depends on the zoom, and the type of tiles that it gets depends on the type of map. We have used “toner-lite”, which is black and white, but there are others including: “terrain”, “toner”, and “toner-lines”. We pass the tiles to ggmap() which will plot it. An internet connection is needed for this to work as get_stadiamap() downloads the tiles.\n\ncanberra_stamen_map &lt;- get_stadiamap(bbox, zoom = 11, maptype = \"stamen_toner_lite\")\n\nggmap(canberra_stamen_map)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-44",
    "href": "lectures/lecture-09-notes.html#section-44",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.7 ",
    "text": "3.7 \nOnce we have a map then we can use ggmap() to plot it. Now we want to get some data that we plot on top of our tiles. We will plot the location of the polling place based on its “division”. This is available from the Australian Electoral Commission (AEC).\n\nbooths &lt;-\n  read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-45",
    "href": "lectures/lecture-09-notes.html#section-45",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.8 ",
    "text": "3.8 \nThis dataset is for the whole of Australia, but as we are only plotting the area around Canberra, we will filter the data to only booths with a geography close to Canberra.\n\nbooths_reduced &lt;-\n    booths |&gt;\n    filter(State == \"ACT\") |&gt;\n    select(PollingPlaceID, DivisionNm, Latitude, Longitude) |&gt;\n    filter(!is.na(Longitude)) |&gt; # Remove rows without geography\n    filter(Longitude &lt; 165) # Remove Norfolk Island\n\nNow we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest.\n\nggmap(canberra_stamen_map, extent = \"normal\", maprange = FALSE) +\n    geom_point(\n        data = booths_reduced,\n        aes(x = Longitude, y = Latitude, colour = DivisionNm),\n        alpha = 0.7\n    ) +\n    scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n    coord_map(\n        projection = \"mercator\",\n        xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n        ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n    ) +\n    labs(\n        x = \"Longitude\",\n        y = \"Latitude\"\n    ) +\n    theme_minimal() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()```\n\n##\n\nWe may like to save the map so that we do not have to create it every time, and we can do that in the same way as any other graph, using `ggsave()`.\n\n\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")\n\nFinally, the reason that we used Stamen Maps and OpenStreetMap is because they are open source, but we could have also used Google Maps. This requires you to first register a credit card with Google, and specify a key, but with low usage the service should be free. Using Google Maps—by using get_googlemap() within ggmap—brings some advantages over get_stadiamap(). For instance it will attempt to find a place name rather than needing to specify a bounding box."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#united-states-military-bases",
    "href": "lectures/lecture-09-notes.html#united-states-military-bases",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.9 United States military bases",
    "text": "3.9 United States military bases\nTo see another example of a static map we will plot some United States military bases after installing and loading troopdata. We can access data about United States overseas military bases back to the start of the Cold War using get_basedata().\n\nbases &lt;- get_basedata()\n\nhead(bases)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-46",
    "href": "lectures/lecture-09-notes.html#section-46",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.10 ",
    "text": "3.10 \nWe will look at the locations of United States military bases in Germany, Japan, and Australia. The troopdata dataset already has the latitude and longitude of each base, and we will use that as our item of interest. The first step is to define a bounding box for each country.\n\n# Use: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany &lt;- c(left = 5.867, bottom = 45.967, right = 15.033, top = 55.133)\n\nbbox_japan &lt;- c(left = 127, bottom = 30, right = 146, top = 45)\n\nbbox_australia &lt;- c(left = 112.467, bottom = -45, right = 155, top = -9.133)"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-47",
    "href": "lectures/lecture-09-notes.html#section-47",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.11 ",
    "text": "3.11 \nThen we need to get the tiles using get_stadiamap() from ggmap.\n\ngerman_stamen_map &lt;- get_stadiamap(bbox_germany, zoom = 6, maptype = \"stamen_toner_lite\")\n\njapan_stamen_map &lt;- get_stadiamap(bbox_japan, zoom = 6, maptype = \"stamen_toner_lite\")\n\naus_stamen_map &lt;- get_stadiamap(bbox_australia, zoom = 5, maptype = \"stamen_toner_lite\")"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-48",
    "href": "lectures/lecture-09-notes.html#section-48",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.12 ",
    "text": "3.12 \nAnd finally, we can bring it all together with maps showing United States military bases in Germany (?@fig-mapbasesin-1), Japan (?@fig-mapbasesin-2), and Australia (?@fig-mapbasesin-3)."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#geocoding",
    "href": "lectures/lecture-09-notes.html#geocoding",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.13 Geocoding",
    "text": "3.13 Geocoding\nSo far we have assumed that we already have geocoded data. This means that we have latitude and longitude coordinates for each place. But sometimes we only have place names, such as “Sydney, Australia”, “Toronto, Canada”, “Accra, Ghana”, and “Guayaquil, Ecuador”. Before we can plot them, we need to get the latitude and longitude coordinates for each case. The process of going from names to coordinates is called geocoding."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-49",
    "href": "lectures/lecture-09-notes.html#section-49",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.14 ",
    "text": "3.14 \n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nWhile you almost surely know where you live, it can be surprisingly difficult to specifically define the boundaries of many places. And this is made especially difficult when different levels of government have different definitions. Bronner (2021) illustrates this in the case of Atlanta, Georgia, where there are (at least) three official different definitions:\n\nthe metropolitan statistical area;\nthe urbanized area; and\nthe census place.\n\nWhich definition is used can have a substantial effect on the analysis, or even the data that are available, even though they are all “Atlanta”."
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-50",
    "href": "lectures/lecture-09-notes.html#section-50",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.15 ",
    "text": "3.15 \nThere are a range of options to geocode data in R, but tidygeocoder is especially useful. We first need a dataframe of locations.\n\nplace_names &lt;-\n    tibble(\n        city = c(\"Sydney\", \"Toronto\", \"Accra\", \"Guayaquil\"),\n        country = c(\"Australia\", \"Canada\", \"Ghana\", \"Ecuador\")\n    )\n\nplace_names\n\n\nplace_names &lt;-\n    geo(\n        city = place_names$city,\n        country = place_names$country,\n        method = \"osm\"\n    )\n\nplace_names"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#section-51",
    "href": "lectures/lecture-09-notes.html#section-51",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.16 ",
    "text": "3.16 \nAnd we can now plot and label these cities (?@fig-mynicemap).\n\nworld &lt;- map_data(map = \"world\")\n\nggplot() +\n    geom_polygon(\n        data = world,\n        aes(x = long, y = lat, group = group),\n        fill = \"white\",\n        colour = \"grey\"\n    ) +\n    geom_point(\n        aes(x = place_names$long, y = place_names$lat),\n        color = \"black\"\n    ) +\n    geom_text(\n        aes(x = place_names$long, y = place_names$lat, label = place_names$city),\n        nudge_y = -5\n    ) +\n    theme_minimal() +\n    labs(\n        x = \"Longitude\",\n        y = \"Latitude\"\n    )"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#concluding-remarks",
    "href": "lectures/lecture-09-notes.html#concluding-remarks",
    "title": "Creating Graphs, Tables, & Maps",
    "section": "3.17 Concluding remarks",
    "text": "3.17 Concluding remarks\nIn this chapter we considered many ways of communicating data. We spent substantial time on graphs, because of their ability to convey a large amount of information in an efficient way. We then turned to tables because of how they can specifically convey information. Finally, we discussed maps, which allow us to display geographic information. The most important task is to show the observations to the full extent possible."
  },
  {
    "objectID": "lectures/lecture-11-notes.html",
    "href": "lectures/lecture-11-notes.html",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#reading-assignment",
    "href": "lectures/lecture-11-notes.html#reading-assignment",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#lecture-slides",
    "href": "lectures/lecture-11-notes.html#lecture-slides",
    "title": "Measurement, Censuses, and Sampling",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#coming-soon",
    "href": "lectures/lecture-11-notes.html#coming-soon",
    "title": "Measurement, Censuses, and Sampling",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#references",
    "href": "lectures/lecture-11-notes.html#references",
    "title": "Measurement, Censuses, and Sampling",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-13-notes.html",
    "href": "lectures/lecture-13-notes.html",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required: [](https://tellingstorieswithdata.com/07-gather.html\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#reading-assignment",
    "href": "lectures/lecture-13-notes.html#reading-assignment",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "Required: [](https://tellingstorieswithdata.com/07-gather.html\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#lecture-slides",
    "href": "lectures/lecture-13-notes.html#lecture-slides",
    "title": "APIs, Scraping, and Parsing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#coming-soon",
    "href": "lectures/lecture-13-notes.html#coming-soon",
    "title": "APIs, Scraping, and Parsing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#references",
    "href": "lectures/lecture-13-notes.html#references",
    "title": "APIs, Scraping, and Parsing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-15-notes.html",
    "href": "lectures/lecture-15-notes.html",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#reading-assignment",
    "href": "lectures/lecture-15-notes.html#reading-assignment",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#lecture-slides",
    "href": "lectures/lecture-15-notes.html#lecture-slides",
    "title": "Experiments and Surveys",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#coming-soon",
    "href": "lectures/lecture-15-notes.html#coming-soon",
    "title": "Experiments and Surveys",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#references",
    "href": "lectures/lecture-15-notes.html#references",
    "title": "Experiments and Surveys",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-17-notes.html",
    "href": "lectures/lecture-17-notes.html",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:  RA Ch 9\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#reading-assignment",
    "href": "lectures/lecture-17-notes.html#reading-assignment",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "Required:  RA Ch 9\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#lecture-slides",
    "href": "lectures/lecture-17-notes.html#lecture-slides",
    "title": "Cleaning, Preparing, and Testing",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#coming-soon",
    "href": "lectures/lecture-17-notes.html#coming-soon",
    "title": "Cleaning, Preparing, and Testing",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#references",
    "href": "lectures/lecture-17-notes.html#references",
    "title": "Cleaning, Preparing, and Testing",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-19-notes.html",
    "href": "lectures/lecture-19-notes.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:  RA Ch 11\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#reading-assignment",
    "href": "lectures/lecture-19-notes.html#reading-assignment",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Required:  RA Ch 11\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#lecture-slides",
    "href": "lectures/lecture-19-notes.html#lecture-slides",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#coming-soon",
    "href": "lectures/lecture-19-notes.html#coming-soon",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#references",
    "href": "lectures/lecture-19-notes.html#references",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-21-notes.html",
    "href": "lectures/lecture-21-notes.html",
    "title": "Linear Models",
    "section": "",
    "text": "Required:  RA Ch 12\nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#reading-assignment",
    "href": "lectures/lecture-21-notes.html#reading-assignment",
    "title": "Linear Models",
    "section": "",
    "text": "Required:  RA Ch 12\nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#lecture-slides",
    "href": "lectures/lecture-21-notes.html#lecture-slides",
    "title": "Linear Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#coming-soon",
    "href": "lectures/lecture-21-notes.html#coming-soon",
    "title": "Linear Models",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#references",
    "href": "lectures/lecture-21-notes.html#references",
    "title": "Linear Models",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-23-notes.html",
    "href": "lectures/lecture-23-notes.html",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#reading-assignment",
    "href": "lectures/lecture-23-notes.html#reading-assignment",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#lecture-slides",
    "href": "lectures/lecture-23-notes.html#lecture-slides",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#coming-soon",
    "href": "lectures/lecture-23-notes.html#coming-soon",
    "title": "Generalized Linear Models (GLMs)",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#references",
    "href": "lectures/lecture-23-notes.html#references",
    "title": "Generalized Linear Models (GLMs)",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-25-notes.html",
    "href": "lectures/lecture-25-notes.html",
    "title": "Project Work",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#reading-assignment",
    "href": "lectures/lecture-25-notes.html#reading-assignment",
    "title": "Project Work",
    "section": "",
    "text": "Required: \nRecommended: ?meta:recommended_reading"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#lecture-slides",
    "href": "lectures/lecture-25-notes.html#lecture-slides",
    "title": "Project Work",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#coming-soon",
    "href": "lectures/lecture-25-notes.html#coming-soon",
    "title": "Project Work",
    "section": "1 Coming soon…",
    "text": "1 Coming soon…"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#references",
    "href": "lectures/lecture-25-notes.html#references",
    "title": "Project Work",
    "section": "2 References",
    "text": "2 References"
  },
  {
    "objectID": "lectures/lecture-01-content.html",
    "href": "lectures/lecture-01-content.html",
    "title": "🔥 Quantitative Research Methods",
    "section": "",
    "text": "This is an introduction the knowledge and skills you need to tell credible stories with quantitative data…\n\nTODO: Add intro slides and overview\nThey read the chapter for Thursday, but Thursday will be intro to R in lab session"
  },
  {
    "objectID": "lectures/lecture-01-content.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-content.html#welcome-to-soci-3040",
    "title": "🔥 Quantitative Research Methods",
    "section": "",
    "text": "This is an introduction the knowledge and skills you need to tell credible stories with quantitative data…\n\nTODO: Add intro slides and overview\nThey read the chapter for Thursday, but Thursday will be intro to R in lab session"
  },
  {
    "objectID": "lectures/lecture-01-content.html#why-data-storytelling-matters",
    "href": "lectures/lecture-01-content.html#why-data-storytelling-matters",
    "title": "🔥 Quantitative Research Methods",
    "section": "Why Data Storytelling Matters",
    "text": "Why Data Storytelling Matters\n\n“A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.” [@alexander2023telling]\n\n\nCore foundation of quantitative research methods\nBridge between analysis and understanding\nEssential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-content.html#common-concerns",
    "href": "lectures/lecture-01-content.html#common-concerns",
    "title": "🔥 Quantitative Research Methods",
    "section": "Common Concerns",
    "text": "Common Concerns\nFive Key Questions for Data Stories\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What’s missing?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data?\n\n\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components",
    "href": "lectures/lecture-01-content.html#core-workflow-components",
    "title": "🔥 Quantitative Research Methods",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\n\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure 1: Rohan Alexander’s [-@alexander2023telling] workflow for telling stories with data.\n\n\n\n\n\n\nPlan and sketch endpoint\nSimulate and consider data\nAcquire and prepare data\nExplore and understand data\nShare findings"
  },
  {
    "objectID": "lectures/lecture-01-content.html#plan-and-sketch",
    "href": "lectures/lecture-01-content.html#plan-and-sketch",
    "title": "🔥 Quantitative Research Methods",
    "section": " Plan and Sketch",
    "text": "Plan and Sketch\n\n\n\ndeliberate, reasoned decisions\npurposeful adjustments\neven 10 minutes of planning is valuable\n\n\n\n\n\nThink of Alice’s conversation with the Cheshire Cat 😸. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.\n\n\n\n\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice’s conversation with the Cheshire Cat in Alice’s Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn’t require extensive time—often just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-content.html#simulate-data",
    "href": "lectures/lecture-01-content.html#simulate-data",
    "title": "🔥 Quantitative Research Methods",
    "section": " Simulate Data",
    "text": "Simulate Data\n\nForces detailed thinking\nClarifies expected data structure and distributions.\nHelps with cleaning and preparation\nIdentifies potential issues beforehand.\nProvides clear testing framework\nEnsures data meets expectations.\n“Almost free” with modern computing\nProvides “an intimate feeling for the situation” [@hamming1996]\n\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides “an intimate feeling for the situation” and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-content.html#acquire-and-prepare",
    "href": "lectures/lecture-01-content.html#acquire-and-prepare",
    "title": "🔥 Quantitative Research Methods",
    "section": " Acquire and Prepare",
    "text": "Acquire and Prepare\n\nOften overlooked but crucial stage\nMany difficult decisions required: data sources, formats, permissions.\nCan significantly affect statistical results [@huntington2021influence]\nCommon challenges: quantity (too little or too much data) and quality\n\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed—either by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-content.html#explore-and-understand",
    "href": "lectures/lecture-01-content.html#explore-and-understand",
    "title": "🔥 Quantitative Research Methods",
    "section": " Explore and Understand",
    "text": "Explore and Understand\n\nBegin with descriptive statistics\nMove to statistical models\nRemember: Models are tools, not truth\nModels reflect:\n\nEarlier decisions\nData acquisition choices\nCleaning procedures\n\n\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It’s important to remember that statistical models are tools—not absolute truths—and they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-content.html#share-findings",
    "href": "lectures/lecture-01-content.html#share-findings",
    "title": "🔥 Quantitative Research Methods",
    "section": " Share Findings",
    "text": "Share Findings\n\nHigh-fidelity communication essential\nDocument all decisions\nBuild credibility through transparency\n\nInclude:\n\nWhat was done\nWhy it was done\nWhat was found\nWeaknesses of the approach\n\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it’s essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process—from data acquisition to analysis—builds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-content.html#the-foundation",
    "href": "lectures/lecture-01-content.html#the-foundation",
    "title": "🔥 Quantitative Research Methods",
    "section": "The Foundation",
    "text": "The Foundation\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-content.html#communication-most-important",
    "href": "lectures/lecture-01-content.html#communication-most-important",
    "title": "🔥 Quantitative Research Methods",
    "section": " Communication (Most Important)",
    "text": "Communication (Most Important)\n\n“Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly.” [@alexander2023telling]\n\n\n“One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.” [@alexander2023telling]\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience’s perspective"
  },
  {
    "objectID": "lectures/lecture-01-content.html#reproducibility",
    "href": "lectures/lecture-01-content.html#reproducibility",
    "title": "🔥 Quantitative Research Methods",
    "section": " Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\nRequirements:\n\nOpen access to code\nData availability or simulation\nAutomated testing\nClear documentation\nAim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-content.html#ethics",
    "href": "lectures/lecture-01-content.html#ethics",
    "title": "🔥 Quantitative Research Methods",
    "section": " Ethics",
    "text": "Ethics\n\n“This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?” [@alexander2023telling]\n\n\nConsider the full context of the dataset [@datafeminism2020]\nAcknowledge the social, cultural, and political forces [@crawford]\nUse data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-content.html#questions",
    "href": "lectures/lecture-01-content.html#questions",
    "title": "🔥 Quantitative Research Methods",
    "section": " Questions",
    "text": "Questions\n\nQuestions evolve through understanding\nChallenge of operationalizing variables\nCuriosity is essential, drives deeper exploration\nValue of “hybrid” knowledge that combines multiple disciplines\nComfort with asking “dumb” questions\n\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a “hybrid”—cultivating knowledge across various disciplines—and being comfortable with asking seemingly simple or “dumb” questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-content.html#measurement",
    "href": "lectures/lecture-01-content.html#measurement",
    "title": "🔥 Quantitative Research Methods",
    "section": " Measurement",
    "text": "Measurement\n\n“The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.” [@alexander2023telling]\n\n\n\nMeasuring even simple things is challenging\nExample: Measuring height\n\nShoes on or off?\nTime of day affects height.\nDifferent tools yield different results.\n\nMore complex measurements are even harder. How do we measure happiness or pain?\n\nMeasurement requires decisions and is not value-free\nContext and purpose guide all measurement choices\n\n\n\n\n\nPicasso’s dog and the challenges of reduction.\n\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person’s height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso’s minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we’re measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-content.html#data-collection-cleaning",
    "href": "lectures/lecture-01-content.html#data-collection-cleaning",
    "title": "🔥 Quantitative Research Methods",
    "section": " &  Data Collection & Cleaning",
    "text": "&  Data Collection & Cleaning\n\n“Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.” [@alexander2023telling]\n\n\nCollection determines possibilities\n\nWhat and how we measure matters.\n\nCleaning requires many decisions\n\nExample: Survey responses on gender\nOptions: “man”, “woman”, “prefer not to say”, “other”\nHandling “prefer not to say” and open-text responses.\n\nDocument every step\n\nEnsures transparency and reproducibility.\n\nConsider implications of choices\n\nEthical considerations and representation.\n\n\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like “man,” “woman,” “prefer not to say,” and “other” (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude “prefer not to say” responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents’ specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it’s vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-content.html#eda-modeling-scaling",
    "href": "lectures/lecture-01-content.html#eda-modeling-scaling",
    "title": "🔥 Quantitative Research Methods",
    "section": "+ EDA, Modeling, & Scaling",
    "text": "+ EDA, Modeling, & Scaling\n\nExploratory Data Analysis (EDA)\n\nIterative process\nNever truly complete\nShapes understanding\n\n\n\nModeling\n\nTool for understanding\nNot a recipe to follow\nJust one representation of reality\nStatistical significance \\(\\neq\\) scientific significance\nStatistical models help us explore the shape of the data; are like echolocation\n\n\n\nScaling\n\nUsing programming languages like R and Python\n\nHandle large datasets efficiently\nAutomate repetitive tasks\nShare work widely and quickly\n\nOutputs can reach many people easily\nAPIs can make analyses accessible in real-time\n\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there’s always more to explore. Although it’s challenging to delineate where EDA ends and formal statistical modeling begins—since our beliefs and understanding evolve continuously—EDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it’s crucial that all steps taken during EDA are recorded and shared.\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it’s a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference—the process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it’s essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly—whether it’s one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data",
    "title": "🔥 Quantitative Research Methods",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\n“There is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.” [@hamming1996, 177]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-1",
    "title": "🔥 Quantitative Research Methods",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. [@alexander2023telling]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-2",
    "title": "🔥 Quantitative Research Methods",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction 👨‍🍳\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data—plausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it’s crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment—much like a chef monitoring a reduction to achieve the desired consistency without overcooking—to determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors—from selecting what to measure to deciding on the methods and accuracy required. Advances in instruments—from telescopes in astronomy to real-time internet data collection—have expanded our ability to gather data, much like new culinary techniques enhance a chef’s ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our “reduction” process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-content.html#what-is-data-science",
    "href": "lectures/lecture-01-content.html#what-is-data-science",
    "title": "🔥 Quantitative Research Methods",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n“Data science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict.” [@alexander2023telling]\n\nKey Principles\n\nData are generated, and must be gathered, cleaned, and prepared\nThese decisions matter\nThe process will be difficult\nDevelop resilience and intrinsic motivation"
  },
  {
    "objectID": "lectures/lecture-01-content.html#the-power-of-multiple-perspectives",
    "href": "lectures/lecture-01-content.html#the-power-of-multiple-perspectives",
    "title": "🔥 Quantitative Research Methods",
    "section": "The Power of Multiple Perspectives",
    "text": "The Power of Multiple Perspectives\n\n“The strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past.” [@alexander2023telling]\n\n\nData science is multi-disciplinary\nCombines statistics, software engineering, subject-matter expertise, and more.\nDiversity enhances understanding\nDifferent perspectives lead to better questions and solutions.\nCollaboration is key\nRespect and integrate insights from various fields."
  },
  {
    "objectID": "lectures/lecture-01-content.html#embracing-the-challenge",
    "href": "lectures/lecture-01-content.html#embracing-the-challenge",
    "title": "🔥 Quantitative Research Methods",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\nOur world is messy, and so are our data. Telling stories with data is difficult but rewarding.\n\nDevelop resilience and intrinsic motivation\nAccept that failure is part of the process.\nConsider possibilities and probabilities\nLearn to make trade-offs.\nNo perfect analysis exists\nAim for transparency and continuous improvement.\n\n\n“Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.” [@alexander2023telling]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#key-takeaways",
    "href": "lectures/lecture-01-content.html#key-takeaways",
    "title": "🔥 Quantitative Research Methods",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-04-notes.html",
    "href": "lectures/lecture-04-notes.html",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "",
    "text": "Required:  RA Ch 2\nRecommended:  KH Ch 1\n\n\n\n\n    View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#reading-assignment",
    "href": "lectures/lecture-04-notes.html#reading-assignment",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "",
    "text": "Required:  RA Ch 2\nRecommended:  KH Ch 1"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#lecture-slides",
    "href": "lectures/lecture-04-notes.html#lecture-slides",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#import-libraries",
    "href": "lectures/lecture-04-notes.html#import-libraries",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "1.1 import libraries",
    "text": "1.1 import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#plan-1",
    "href": "lectures/lecture-04-notes.html#plan-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.1  plan",
    "text": "3.1  plan\n\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure 1 (a) (next slide). We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure 1 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#plan-2",
    "href": "lectures/lecture-04-notes.html#plan-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.2  plan",
    "text": "3.2  plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\n\n\n\nFigure 1: Sketches of a dataset and graph about the neonatal mortality rate (NMR)"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-1",
    "href": "lectures/lecture-04-notes.html#simulate-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.1  simulate",
    "text": "4.1  simulate\n\n\nTo simulate some data that aligns with our plan, we will need three columns: country, year, and NMR. We can do this by repeating the name of each country 50 times with rep(), and enabling the passing of 50 years. Then we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-2",
    "href": "lectures/lecture-04-notes.html#simulate-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.2  simulate",
    "text": "4.2  simulate\n\n\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", 50), rep(\"Australia\", 50),\n                rep(\"Canada\", 50), rep(\"Kenya\", 50)\n            ),\n        year =\n            rep(c(1971:2020), 4),\n        nmr =\n            runif(n = 200, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-3",
    "href": "lectures/lecture-04-notes.html#simulate-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.3  simulate",
    "text": "4.3  simulate\n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-4",
    "href": "lectures/lecture-04-notes.html#simulate-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.4  simulate",
    "text": "4.4  simulate\n\n\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n                rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)\n            ),\n        year =\n            rep(c(1:number_of_years + 1970), 4),\n        nmr =\n            runif(n = number_of_years * 4, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-5",
    "href": "lectures/lecture-04-notes.html#simulate-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.5  simulate",
    "text": "4.5  simulate\n\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat “country” is exclusively one of these four: “Argentina”, “Australia”, “Canada”, or “Kenya”.\nConversely, “country” contains all those four countries.\nThat “year” is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat “nmr” is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-6",
    "href": "lectures/lecture-04-notes.html#simulate-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.6  simulate",
    "text": "4.6  simulate\n\nsimulated_nmr_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#simulate-7",
    "href": "lectures/lecture-04-notes.html#simulate-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.7  simulate",
    "text": "4.7  simulate\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section",
    "href": "lectures/lecture-04-notes.html#section",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.1 ",
    "text": "5.1 \nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\nigme_data_path &lt;- here(\"data\", \"igme.csv\")\nigme_data_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/igme.csv\"\n\n\n\nraw_igme_data &lt;-\n    read_csv(\n        file =\n            \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n        show_col_types = FALSE\n    )\n\nwrite_csv(x = raw_igme_data, file = igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-1",
    "href": "lectures/lecture-04-notes.html#section-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.2 ",
    "text": "5.2 \n\nraw_igme_data &lt;-\n    read_csv(\n        file = igme_data_path,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-2",
    "href": "lectures/lecture-04-notes.html#section-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.3 ",
    "text": "5.3 \nWith established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail()\n\nhead(raw_igme_data)\n\n# A tibble: 6 × 29\n  `Geographic area` Indicator              Sex   `Wealth Quintile` `Series Name`\n  &lt;chr&gt;             &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;        \n1 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n2 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n3 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n4 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n5 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n6 Afghanistan       Neonatal mortality ra… Total Total             Afghanistan …\n# ℹ 24 more variables: `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;, COUNTRY_NOTES &lt;chr&gt;, CONNECTION &lt;lgl&gt;,\n#   DEATH_CATEGORY &lt;lgl&gt;, CATEGORY &lt;chr&gt;, `Observation Status` &lt;chr&gt;,\n#   `Unit of measure` &lt;chr&gt;, `Series Category` &lt;chr&gt;, `Series Type` &lt;chr&gt;,\n#   STD_ERR &lt;dbl&gt;, REF_DATE &lt;dbl&gt;, `Age Group of Women` &lt;chr&gt;,\n#   `Time Since First Birth` &lt;chr&gt;, DEFINITION &lt;chr&gt;, INTERVAL &lt;dbl&gt;,\n#   `Series Method` &lt;chr&gt;, LOWER_BOUND &lt;dbl&gt;, UPPER_BOUND &lt;dbl&gt;, …"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-3",
    "href": "lectures/lecture-04-notes.html#section-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.4 ",
    "text": "5.4 \nand what the names of the columns are with names()\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\""
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-4",
    "href": "lectures/lecture-04-notes.html#section-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.5 ",
    "text": "5.5 \n\nWe would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where “Sex” is “Total”, “Series Name” is “UN IGME estimate”, “Geographic area” is one of “Argentina”, “Australia”, “Canada”, and “Kenya”, and the “Indicator” is “Neonatal mortality rate”. After this we are interested in just a few columns: “geographic_area”, “time_period”, and “obs_value”."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-5",
    "href": "lectures/lecture-04-notes.html#section-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.6 ",
    "text": "5.6 \n\ncleaned_igme_data &lt;-\n    clean_names(raw_igme_data) |&gt;\n    filter(\n        sex == \"Total\",\n        series_name == \"UN IGME estimate\",\n        geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n        indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n    select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-6",
    "href": "lectures/lecture-04-notes.html#section-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.7 ",
    "text": "5.7 \n\nWe need to fix two other aspects: the class of “time_period” is character when we need it to be a year, and the name of “obs_value” should be “nmr” to be more informative."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-7",
    "href": "lectures/lecture-04-notes.html#section-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.8 ",
    "text": "5.8 \n\ncleaned_igme_data &lt;-\n    cleaned_igme_data |&gt;\n    mutate(\n        time_period = str_remove(time_period, \"-06\"),\n        time_period = as.integer(time_period)\n    ) |&gt;\n    filter(time_period &gt;= 1971) |&gt;\n    rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-8",
    "href": "lectures/lecture-04-notes.html#section-8",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.9 ",
    "text": "5.9 \n\nFinally, we can check that our dataset passes the tests that we developed based on the simulated dataset."
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-9",
    "href": "lectures/lecture-04-notes.html#section-9",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.10 ",
    "text": "5.10 \n\ncleaned_igme_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-10",
    "href": "lectures/lecture-04-notes.html#section-10",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.11 ",
    "text": "5.11 \nAll that remains is to save the nicely cleaned dataset.\n\ncleaned_igme_data_path &lt;- here(\"data\", \"cleaned_igme_data.csv\")\nwrite_csv(x = cleaned_igme_data, file = cleaned_igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-11",
    "href": "lectures/lecture-04-notes.html#section-11",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.1 ",
    "text": "6.1 \nWe would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\ncleaned_igme_data &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_igme_data.csv\"),\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#section-12",
    "href": "lectures/lecture-04-notes.html#section-12",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.2 ",
    "text": "6.2 \n\nWe can now make a graph of how NMR has changed over time and the differences between countries (Figure 2).\n\n\ncleaned_igme_data |&gt;\n    ggplot(aes(x = year, y = nmr, color = country)) +\n    geom_point() +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#share-1",
    "href": "lectures/lecture-04-notes.html#share-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "7.1  share",
    "text": "7.1  share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries—Argentina, Australia, Canada, and Kenya—over the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023).\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure 2). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  },
  {
    "objectID": "lectures/lecture-03-notes.html",
    "href": "lectures/lecture-03-notes.html",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "",
    "text": "Lecture Slides\nView slides in full screen\nClass Notes"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#the-firehose",
    "href": "lectures/lecture-03-notes.html#the-firehose",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "1.1 the firehose",
    "text": "1.1 the firehose\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s (2023) Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#the-firehose-1",
    "href": "lectures/lecture-03-notes.html#the-firehose-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "1.2 the firehose",
    "text": "1.2 the firehose\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by Barrett (2021)\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan Alexander (2023)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#preliminaries-1",
    "href": "lectures/lecture-03-notes.html#preliminaries-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "2.1 preliminaries",
    "text": "2.1 preliminaries\n\nRStudio / CodeSpaces / Whatever…"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#preliminaries-2",
    "href": "lectures/lecture-03-notes.html#preliminaries-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "2.2 preliminaries",
    "text": "2.2 preliminaries\n\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#import-libraries",
    "href": "lectures/lecture-03-notes.html#import-libraries",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "2.3 import libraries",
    "text": "2.3 import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#plan-1",
    "href": "lectures/lecture-03-notes.html#plan-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.1  plan",
    "text": "3.1  plan\n\n\n3.1.1 Australian Elections\n\n\n\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#plan-2",
    "href": "lectures/lecture-03-notes.html#plan-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "3.2  plan",
    "text": "3.2  plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure 1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a “division” in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-1",
    "href": "lectures/lecture-03-notes.html#simulate-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.1  simulate",
    "text": "4.1  simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-2",
    "href": "lectures/lecture-03-notes.html#simulate-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.2  simulate",
    "text": "4.2  simulate\n\nWe’ll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-3",
    "href": "lectures/lecture-03-notes.html#simulate-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.3  simulate",
    "text": "4.3  simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we’re creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named “Division”. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it’s used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options — “Liberal”, “Labor”, “National”, “Green”, and “Other” — into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the “Division” column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we’re picking “with replacement” (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn’t match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-4",
    "href": "lectures/lecture-03-notes.html#simulate-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "4.4  simulate",
    "text": "4.4  simulate\n🤘 We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Liberal \n 2        2 Labor   \n 3        3 Liberal \n 4        4 National\n 5        5 Labor   \n 6        6 Liberal \n 7        7 Labor   \n 8        8 Liberal \n 9        9 National\n10       10 National\n# ℹ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-1",
    "href": "lectures/lecture-03-notes.html#acquire-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.1  acquire",
    "text": "5.1  acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-2",
    "href": "lectures/lecture-03-notes.html#acquire-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.2  acquire",
    "text": "5.2  acquire\n\nWe’ll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n\n\n\n✌️ R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief “Ode to the here package,” “here here,” which you can read… here."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-3",
    "href": "lectures/lecture-03-notes.html#acquire-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.3  acquire",
    "text": "5.3  acquire\n🤘 We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 × 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA…\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather… KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge… ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE…\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ℹ 141 more rows\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-4",
    "href": "lectures/lecture-03-notes.html#acquire-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.4  acquire",
    "text": "5.4  acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA…\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri… KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-5",
    "href": "lectures/lecture-03-notes.html#acquire-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.5  acquire",
    "text": "5.5  acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma… STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO…\n# ℹ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-6",
    "href": "lectures/lecture-03-notes.html#acquire-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.6  acquire",
    "text": "5.6  acquire\n\n“We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.” (Alexander 2023)\n\n\nLet’s clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-7",
    "href": "lectures/lecture-03-notes.html#acquire-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.7  acquire",
    "text": "5.7  acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let’s look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ℹ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-8",
    "href": "lectures/lecture-03-notes.html#acquire-8",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.8  acquire",
    "text": "5.8  acquire\n\n\n\n\n\n\n✌️ R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-9",
    "href": "lectures/lecture-03-notes.html#acquire-9",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.9  acquire",
    "text": "5.9  acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-10",
    "href": "lectures/lecture-03-notes.html#acquire-10",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.10  acquire",
    "text": "5.10  acquire\n\n\n\n\n\n\n\n✌️ R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet’s rename."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-11",
    "href": "lectures/lecture-03-notes.html#acquire-11",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.11  acquire",
    "text": "5.11  acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-12",
    "href": "lectures/lecture-03-notes.html#acquire-12",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.12  acquire",
    "text": "5.12  acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let’s simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-13",
    "href": "lectures/lecture-03-notes.html#acquire-13",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.13  acquire",
    "text": "5.13  acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-14",
    "href": "lectures/lecture-03-notes.html#acquire-14",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.14  acquire",
    "text": "5.14  acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! 😎"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#aus_elections_clean_path",
    "href": "lectures/lecture-03-notes.html#aus_elections_clean_path",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "5.15  acquire",
    "text": "5.15  acquire\n\nLet’s save the cleaned data so that we can start with it data in the next stage. We’ll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-1",
    "href": "lectures/lecture-03-notes.html#explore-understand-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.1  explore / understand",
    "text": "6.1  explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-2",
    "href": "lectures/lecture-03-notes.html#explore-understand-2",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.2  explore / understand",
    "text": "6.2  explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nI’m using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won’t work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-3",
    "href": "lectures/lecture-03-notes.html#explore-understand-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.3  explore / understand",
    "text": "6.3  explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n😎"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-4",
    "href": "lectures/lecture-03-notes.html#explore-understand-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.4  explore / understand",
    "text": "6.4  explore / understand\n\n\n\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-5",
    "href": "lectures/lecture-03-notes.html#explore-understand-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.5  explore / understand",
    "text": "6.5  explore / understand\n\n\n\n\n\n\nRemember, we’re trying to make something like this.\n\n\n\n\n\n\n\n\n\n✌️ R Tip\n\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer’s understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer’s eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-6",
    "href": "lectures/lecture-03-notes.html#explore-understand-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.6  explore / understand",
    "text": "6.6  explore / understand\n\nLet’s visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it’s cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\n\n\n\n\n\n\nFigure 2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-7",
    "href": "lectures/lecture-03-notes.html#explore-understand-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.7  explore / understand",
    "text": "6.7  explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels\n\n\n\n\n\n\n\nFigure 3: Number of seats won, by political party, at the 2022 Australian Federal Election. 😎"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#section",
    "href": "lectures/lecture-03-notes.html#section",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "6.8 ",
    "text": "6.8 \ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure 4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#share-1",
    "href": "lectures/lecture-03-notes.html#share-1",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "7.1  share",
    "text": "7.1  share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure 3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#plan-4",
    "href": "lectures/lecture-03-notes.html#plan-4",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "9.1  plan",
    "text": "9.1  plan\n\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure 5 (a) (next slide).\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure 5 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#plan-5",
    "href": "lectures/lecture-03-notes.html#plan-5",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "9.2  plan",
    "text": "9.2  plan\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a table\n\n\n\n\n\n\n\nFigure 5: Sketches of a dataset and table of the average number of beds occupied each month for shelters in Toronto."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-6",
    "href": "lectures/lecture-03-notes.html#simulate-6",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "10.1  simulate",
    "text": "10.1  simulate\n\n\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process. When we turn to analysis, it will provide us with a guide. Conducting analysis without first using simulation can be thought of as shooting arrows without a target—while you are certainly doing something, it is not clear whether you are doing it well."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#simulate-7",
    "href": "lectures/lecture-03-notes.html#simulate-7",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "10.2  simulate",
    "text": "10.2  simulate\n\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            ) # Draw 1,095 times from the Poisson distribution\n    )\n\nsimulated_occupancy_data\n\n# A tibble: 1,095 × 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ℹ 1,085 more rows\n\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it later in the course."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-16",
    "href": "lectures/lecture-03-notes.html#acquire-16",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.1  acquire",
    "text": "11.1  acquire\n\n\ntoronto_shelters &lt;-\n    # Each package is associated with a unique id  found in the \"For\n    # Developers\" tab of the relevant page from Open Data Toronto\n    # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    # Within that package, we are interested in the 2021 dataset\n    filter(name ==\n        \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    # Having reduced the dataset to one row we can get the resource\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-17",
    "href": "lectures/lecture-03-notes.html#acquire-17",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.2  acquire",
    "text": "11.2  acquire\n\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 × 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME    \n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                \n1     1 21-01-01                    24 COSTI Immigrant Serv…\n2     2 21-01-01                    24 COSTI Immigrant Serv…\n3     3 21-01-01                    24 COSTI Immigrant Serv…\n4     4 21-01-01                    24 COSTI Immigrant Serv…\n5     5 21-01-01                    24 COSTI Immigrant Serv…\n6     6 21-01-01                    24 COSTI Immigrant Serv…\n# ℹ 28 more variables: SHELTER_ID &lt;dbl&gt;,\n#   SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;,\n#   LOCATION_POSTAL_CODE &lt;chr&gt;, …"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-18",
    "href": "lectures/lecture-03-notes.html#acquire-18",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.3  acquire",
    "text": "11.3  acquire\n\nWe’ll change the names to make them easier to type using clean_names(), and select() the relevant columns.\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#acquire-19",
    "href": "lectures/lecture-03-notes.html#acquire-19",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "11.4  acquire",
    "text": "11.4  acquire\n\nAll that remains for this step is to save the cleaned dataset.\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\nWHERE ARE THESE NAs COMING FROM?"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-8",
    "href": "lectures/lecture-03-notes.html#explore-understand-8",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.1  explore / understand",
    "text": "12.1  explore / understand\n\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\ntoronto_shelters_clean\n\n# A tibble: 50,944 × 2\n   occupancy_date occupied_beds\n   &lt;date&gt;                 &lt;dbl&gt;\n 1 2021-01-01                NA\n 2 2021-01-01                NA\n 3 2021-01-01                NA\n 4 2021-01-01                NA\n 5 2021-01-01                NA\n 6 2021-01-01                 6\n 7 2021-01-01                NA\n 8 2021-01-01                NA\n 9 2021-01-01                NA\n10 2021-01-01                NA\n# ℹ 50,934 more rows"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-9",
    "href": "lectures/lecture-03-notes.html#explore-understand-9",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.2  explore / understand",
    "text": "12.2  explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\nTable 1: Shelter usage in Toronto in 2021\n\n\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426\n\n\n\n\n\n\n\n\n\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments—“label” and “abbr”—to get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in sec-farm-data and sec-exploratory-data-analysis. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create tbl-homelessoccupancyd."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#explore-understand-10",
    "href": "lectures/lecture-03-notes.html#explore-understand-10",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "12.3  explore / understand",
    "text": "12.3  explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable(\n        col.names = c(\"Month\", \"Average daily number of&lt;br&gt;occupied beds (per shelter)\"),\n        digits = 1\n    )\n\n\n\n\nMonth\nAverage daily number ofoccupied beds (per shelter)\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5\n\n\n\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (tbl-homelessoccupancy). In particular we make the column names easier to read, and only show an appropriate number of decimal places."
  },
  {
    "objectID": "lectures/lecture-03-notes.html#share-3",
    "href": "lectures/lecture-03-notes.html#share-3",
    "title": "Drinking from the Firehose – Data Analysis Workflow",
    "section": "13.1  share",
    "text": "13.1  share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R (R Core Team 2023) as well as the tidyverse (Wickham 2017), janitor (Firke 2023), opendatatoronto (Gelfand 2022), lubridate (Grolemund and Wickham 2011), and knitr (Xie 2023). We then made a table of the average number of occupied beds each night for each month (tbl-homelessoccupancy).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (tbl-homelessoccupancy). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\n\n\n\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of Hao (2019), that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": " Quantitative Research Methods",
    "section": "",
    "text": "Course Instructor John McLevey (he/him) Professor, Department of Sociology Memorial University\n\n  john.mclevey@uwaterloo.ca Note: I do not check or respond to email in the evenings or on weekends.\n\n\nWhere is class? CP-2003 (Chemistry-Physics, Computer Lab) When is class? Tuesdays & Thursdays, 1:30 - 2:50 pm Office Hours: A4054, Tuesdays & Thursdays, 3:00 - 4:00 pm\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander’s (2023) Telling Stories with Data, you will learn how to:\nYou will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear models, generalized linear models, and multilevel models. Reproducibility and research ethics are considered throughout the workflow, and the entire course.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#alexander2023telling-telling-stories-with-data",
    "href": "syllabus/syllabus.html#alexander2023telling-telling-stories-with-data",
    "title": " Quantitative Research Methods",
    "section": " Alexander (2023) Telling Stories with Data",
    "text": "Alexander (2023) Telling Stories with Data\n\n“This is not another statistics book. It is much better than that. It is a book about doing quantitative research, about scientific justification, about quality control, about communication and epistemic humility. It’s a valuable supplement to any methods curriculum, and useful for self-learners as well.” – Richard McElreath\n\n\n\n\n\n“This clean and fun book covers a wide range of topics on statistical communication, programming, and modeling in a way that should be a useful supplement to any statistics course or self-learning program. I absolutely love this book!” – Andrew Gelman\n\n\n“Every data analyst has to tell stories with data, and yet traditional textbooks focus on statistical methods alone. Telling Stories with Data teaches the entire data science workflow, including data acquisition, communication, and reproducibility. I highly recommend this unique book!” – Kosuke Imai\n\n\n“Telling (true) Stories with Data requires more than fancy statistical models and big data. With a series of fascinating case studies, Rohan Alexander teaches us how to ask good questions, acquire data, estimate models, and communicate our results. This holistic approach is explained with crisp and engaging prose. The pages are filled with detailed R examples, which emphasize the importance of transparency and reproducibility. I absolutely love this book and recommend it to all my students.” – Vincent Arel-Bundock\n\n\n“An excellent book. Communication and reproducibility are of increasing concern in statistics, and this book covers these topics and more in a practical, appealing, and truly unique way.” – Daniela Witten",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#healy2019data-data-visualization",
    "href": "syllabus/syllabus.html#healy2019data-data-visualization",
    "title": " Quantitative Research Methods",
    "section": " Healy (2019) Data Visualization",
    "text": "Healy (2019) Data Visualization\n\n“Finally! A data visualization guide that is simultaneously practical and elegant. … Data Visualization is brimming with insights into how quantitative analysts can use visualization as a tool for understanding and communication. A must-read for anyone who works with data.” – Elizabeth Bruch\n\n\n\n\n\n“Healy’s fun and readable book is unusual in covering the ‘why do’ as well as the ‘how to’ of data visualization, demonstrating how dataviz is a key step in all stages of social science – from theory construction to measurement to modeling and interpretation of analyses―and giving readers the tools to integrate visualization into their own work.” – Andrew Gelman\n\n\n“Data Visualization is a brilliant book that not only teaches the reader how to visualize data but also carefully considers why data visualization is essential for good social science. The book is broadly relevant, beautifully rendered, and engagingly written. It is easily accessible for students at any level and will be an incredible teaching resource for courses on research methods, statistics, and data visualization. It is packed full of clear-headed and sage insights.” – Becky Pettit\n\n\n“Kieran Healy has written a wonderful book that fills an important niche in an increasingly crowded landscape of materials about software in R. Data Visualization is clear, beautifully formatted, and full of careful insights.” – Brandon Stewart",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#flowcharts",
    "href": "syllabus/conventions.html#flowcharts",
    "title": " Conventions",
    "section": "Flowcharts",
    "text": "Flowcharts",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#mathematical-symbols",
    "href": "syllabus/conventions.html#mathematical-symbols",
    "title": " Conventions",
    "section": "Mathematical Symbols",
    "text": "Mathematical Symbols",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#references",
    "href": "syllabus/conventions.html#references",
    "title": " Conventions",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "lectures/lecture-05-content.html",
    "href": "lectures/lecture-05-content.html",
    "title": "Get Started",
    "section": "",
    "text": "Key concepts and skills\n\nReproducibility typically begins as something that someone imposes on you. It can be onerous and annoying. This typically lasts until you need to revisit a project after a small break. At that point you typically realize that reproducibility is not just a requirement for data science because it is the only way that we can make genuine progress, but because it helps us help ourselves.\nReproducibility implies sharing data, code, and environment. This is enhanced by using Quarto, R Projects, and Git and GitHub: Quarto builds documents that integrate normal text and R code; R Projects enable a file structure that is not dependent on a user’s personal directory set-up; and Git and GitHub make it easier to share code and data.\nThis is not an unimpeachable workflow, but one that is good enough and provides many of the benefits. We will improve various aspects of it through various tools, but improving code structure and comments goes a long way.\nThere are always errors that occur, and it is important to recognize that debugging is a skill that improves with practice. But one key aspect of being able to get help is to be able to make a reproducible example others can use.\n\n\n\n\n\n\n\nBase R [@citeR]\nAER [@citeaer]\nfuture [@future]\ngitcreds [@gitcreds]\nknitr [@citeknitr]\nlintr [@lintr]\nrenv [@renv]\nreprex [@reprex]\nstyler [@styler]\ntidyverse [@tidyverse]\ntinytex [@tinytex]\nusethis [@usethis]\n\n\n\nlibrary(AER)\nlibrary(future)\nlibrary(gitcreds)\nlibrary(knitr)\nlibrary(lintr)\nlibrary(renv)\nlibrary(reprex)\nlibrary(styler)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(usethis)\n\n\n\n\n\n\n\nThe number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\\(\\dots\\) So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty.\nFrançois Chollet, 20 February 2020.\n\nIf science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is a critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, there is a need for reproducible data science workflows.\n@Alexander2019 defines reproducible research as that which can be exactly redone, given all the materials used. This underscores the importance of providing the code, data, and environment. The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables. Ironically, there are different definitions of reproducibility between disciplines. @barba2018terminologies surveys a variety of disciplines and concludes that the predominant language usage implies the following definitions:\n\nReproducible research is when “[a]uthors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.”\nA replication is a study “that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.”\n\nRegardless of what it is specifically called, @Gelman2016 identifies how large an issue the lack of it is in various social sciences. Work that is not reproducible does not contribute to our stock of knowledge about the world. This is wasteful and potentially even unethical. Since @Gelman2016, a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. That is also the case in the life sciences [@heil2021reproducibility], cancer research [@Begley2012; @Mullard2021], and computer science [@pineau2021improving].\nSome of the examples that @Gelman2016 talks about are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created “nudge” units that implement public policy [@sunstein2017economics] even though there is evidence that some of the claims lack credibility [@nonudge; @gelmannudge]. Governments are increasingly using algorithms that they do not make open [@chouldechova18a]. And @herndon2014does document how research in economics that was used by governments to justify austerity policies following the 2007–2008 financial crisis turned out to not be reproducible.\nAt a minimum, and with few exceptions, we must release our code, datasets, and environment. Without these, it is difficult to know what a finding speaks to [@miyakawa2020no]. More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked [@merali2010computational; @hillelwayne; @natefixesmistake]. Increasingly, following @buckheit1995wavelab, we consider a paper to be an advertisement, and for the associated code, data, and environment to be the actual work. Steve Jobs, a co-founder of Apple, talked about how people who are the best at their craft ensure that even the aspects of their work that no one else will ever see are as well finished and high quality as the aspects that are public facing [@stevejobs]. The same is true in data science, where often one of the distinguishing aspects of high-quality work is that the README and code comments are as polished as, say, the abstract of the associated paper.\n\n\n\nWorkflows exist within a cultural and social context, which imposes an additional ethical reason for the need for them to be reproducible. For instance, @wang2018deep train a neural network to distinguish between the faces of gay and heterosexual men. (@murphy2017 provides a summary of the paper, the associated issues, and comments from its authors.) To do this, @wang2018deep [p. 248] needed a dataset of photos of people that were “adult, Caucasian, fully visible, and of a gender that matched the one reported on the user’s profile”. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. The instructions provided to the Mechanical Turk workers for this task specify that Barack Obama, the 44th US President, who had a white mother and a black father, should be classified as “Black”; and that Latino is an ethnicity, rather than a race [@Mattson2017]. The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.\nThis is just one specific concern about one part of the @wang2018deep workflow. Broader concerns are raised by others including @Gelman_2018. The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of @wang2018deep is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others.\n\n\n\nSome of the steps that we can take to make our work more reproducible include:\n\nEnsure the entire workflow is documented. This may involve addressing questions such as:\n\nHow was the original, unedited dataset obtained and is access likely to be persistent and available to others?\nWhat specific steps are being taken to transform the original, unedited data into the data that were analyzed, and how can this be made available to others?\nWhat analysis has been done, and how clearly can this be shared?\nHow has the final paper or report been built and to what extent can others follow that process themselves?\n\nNot worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to do the last, until you can do the first:\n\nCan you run your entire workflow again?\nCan another person run your entire workflow again?\nCan “future-you” run your entire workflow again?\nCan “future-another-person” run your entire workflow again?\n\nIncluding a detailed discussion about the limitations of the dataset and the approach in the final paper or report.\n\n\n\n\nThe workflow that we advocate in this book is:\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\nBut it can be alternatively considered as: “Think an awful lot, mostly read and write, sometimes code”.\n\n\n\nThere are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes\n\nQuarto,\nR Projects\nGit and GitHub\n\n\n\n\n\n\nQuarto integrates code and natural language in a way that is called “literate programming” [@Knuth1984]. It is the successor to R Markdown, which was a variant of Markdown specifically designed to allow R code chunks to be included. Quarto uses a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a “What You See Is What You Get” (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level headings will look the same. But it means that we must designate or “mark up” how we would like certain aspects to appear. And it is only when we render the document that we get to see what it looks like. A visual editor option can also be used, and this hides the need for the user to do this mark-up themselves.\n\n\n\n\nOne advantage of literate programming is that we get a “live” document in which code executes and then forms part of the document. Another advantage of Quarto is that similar code can compile into a variety of documents, including HTML and PDFs. Quarto also has default options for including a title, author, and date. One disadvantage is that it can take a while for a document to compile because the code needs to run.\nWe need to download Quarto from here. (Skip this step if you are using Posit Cloud because it is already installed.) We can then create a new Quarto document within RStudio: “File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “Quarto Document\\(\\dots\\)”.\nAfter opening a new Quarto document and selecting “Source” view, you will see the default top matter, contained within a pair of three dashes, as well as some examples of text showing a few of the markdown essential commands and R chunks, each of which are discussed further in the following sections.\n\n\n\n\n\nTop matter consists of defining aspects such as the title, author, and date. It is contained within three dashes at the top of a Quarto document. For instance, the following would specify a title, a date that automatically updated to the date the document was rendered, and an author.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: html\n---\n\n\n\n\nAn abstract is a short summary of the paper, and we could add that to the top matter.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: html\n---\n\n\n\nBy default, Quarto will create an HTML document, but we can change the output format to produce a PDF. This uses LaTeX in the background and requires the installation of supporting packages. To do this install tinytex. But as it is used in the background we should not need to load it.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: pdf\n---\n\n\n\nWe can include references by specifying a BibTeX file in the top matter and then calling it within the text, as needed.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: pdf\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---\n\n\n\nWe would need to make a separate file called “bibliography.bib” and save it next to the Quarto file. In the BibTeX file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with citation() and this can be added to the “bibliography.bib” file. The citation for a package can be found by including the package name, for instance citation(\"tidyverse\"), and again adding the output to the “.bib” file. It can be helpful to use Google Scholar or doi2bib to get citations for books or articles.\n\n\n\nWe need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance “citeR”.\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@book{tellingstories,\n    title = {Telling Stories with Data},\n    author = {Rohan Alexander},\n    year = {2023},\n    publisher = {Chapman and Hall/CRC},\n    url = {https://tellingstorieswithdata.com}\n  }\n\n\n\nTo cite R in the Quarto document we then include @citeR, which would put brackets around the year: @citeR, or [@citeR], which would put brackets around the whole thing: [@citeR].\n\n\n\nThe reference list at the end of the paper is automatically built based on calling the BibTeX file and including references in the paper. At the end of the Quarto document, include a heading “# References” and the actual citations will be included after that. When the Quarto file is rendered, Quarto sees these in the content, goes to the BibTeX file to get the reference details that it needs, builds the reference list, and then adds it at the end of the rendered document.\n\n\n\n\n\nQuarto uses a variation of Markdown as its underlying syntax. Essential Markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in RStudio: “Help” \\(\\rightarrow\\) “Markdown Quick Reference”. It is your choice as to whether you want to use the visual or source editor. But either way, it is good to understand these essentials because it will not always be possible to use a visual editor (for instance if you are quickly looking at a Quarto document in GitHub). As you get more experience it can be useful to use a text editor such as Sublime Text, or an alternative Integrated Development Environment such as VS Code.\n\nEmphasis: *italic*, **bold**\nHeaders (these go on their own line with a blank line before and after):\n\n         # First level header\n         \n         ## Second level header\n         \n         ### Third level header\n\nUnordered list, with sub-lists:\n\n    * Item 1\n    * Item 2\n        + Item 2a\n        + Item 2b\n\nOrdered list, with sub-lists:\n\n    1. Item 1\n    2. Item 2\n    3. Item 3\n        + Item 3a\n        + Item 3b\n\nURLs can be added: [this book](https://www.tellingstorieswithdata.com) results in this book.\nA paragraph is created by leaving a blank line.\n\nA paragraph about an idea, nicely spaced from the following paragraph.\n\nA paragraph about another idea, again spaced from the earlier paragraph.\n\n\n\n\nOnce we have added some aspects, then we may want to see the actual document. To build the document click “Render”.\n\n\n\n\n\nWe can include code for R and many other languages in code chunks within a Quarto document. When we render the document the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell Quarto that this is an R chunk. Anything inside this chunk will be considered R code and run as such. We use data from @citeaer who provide the R package AER to accompany their book Applied Econometrics with R. We could load the tidyverse and install and load AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\n\n\n\n\nThe output of that code is Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\n\nThere are various evaluation options that are available in chunks. We include these, each on a new line, by opening the line with the chunk-specific comment delimiter “#|” and then the option. Helpful options include:\n\necho: This controls whether the code itself is included in the document. For instance, #| echo: false would mean the code will be run and its output will show, but the code itself would not be included in the document.\ninclude: This controls whether the output of the code is included in the document. For instance, #| include: false would run the code, but would not result in any output, and the code itself would not be included in the document.\neval: This controls whether the code should be included in the document. For instance, #| eval: false would mean that the code is not run, and hence there would not be any output to include, but the code itself would be included in the document.\nwarning: This controls whether warnings should be included in the document. For instance, #| warning: false would mean that warnings are not included.\nmessage: This controls whether messages should be included in the document. For instance, #| message: false would mean that messages are not included in the document.\n\nFor instance, we could include the output, but not the code, and suppress any warnings.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nLeave a blank line on either side of an R chunk, otherwise it may not run properly. And use lower case for logical values, i.e. “false” not “FALSE”.\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then...\nThe Quarto document itself must load any datasets that are needed. It is not enough that they are in the environment. This is because the Quarto document evaluates the code in the document when it is rendered, not necessarily the environment.\n\n\nWe can include equations by using LaTeX, which is based on the programming language TeX. We invoke math mode in LaTeX by using two dollar signs as opening and closing tags. Then whatever is inside is evaluated as LaTeX mark-up. For instance we can produce the compound interest formula with:\n$$\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n$$\n\\[\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n\\]\n\n\n\n\nLaTeX is a comprehensive mark-up language but we will mostly just use it to specify the model of interest. We include some examples here that contain the critical aspects we will draw on starting in ?@sec-its-just-a-linear-model.\n$$\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n$$\n\\[\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n\\]\n\n\n\nUnderscores are used to get subscripts: y_i for \\(y_i\\). And we can get a subscript of more than one item by surrounding it with curly braces: y_{i,c} for \\(y_{i,c}\\). In this case we wanted math mode within the line, and so we surround these with only one dollar sign as opening and closing tags.\nGreek letters are typically preceded by a backslash. Common Greek letters include: \\alpha for \\(\\alpha\\), \\beta for \\(\\beta\\), \\delta for \\(\\delta\\), \\epsilon for \\(\\epsilon\\), \\gamma for \\(\\gamma\\), \\lambda for \\(\\lambda\\), \\mu for \\(\\mu\\), \\phi for \\(\\phi\\), \\pi for \\(\\pi\\), \\Pi for \\(\\Pi\\), \\rho for \\(\\rho\\), \\sigma for \\(\\sigma\\), \\Sigma for \\(\\Sigma\\), \\tau for \\(\\tau\\), and \\theta for \\(\\theta\\).\n\n\n\nLaTeX math mode assumes letters are variables and so makes them italic, but sometimes we want a word to appear in normal font because it is not a variable, such as “Normal”. In that case we surround it with \\mbox{}, for instance \\mbox{Normal} for \\(\\mbox{Normal}\\).\n\n\n\nWe line up equations across multiple lines using \\begin{aligned} and \\end{aligned}. Then the item that is to be lined up is noted by an ampersand. The following is a model that we will estimate in ?@sec-multilevel-regression-with-post-stratification.\n$$\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]\n\n\n\nFinally, certain functions are built into LaTeX. For instance, we can appropriately typeset “log” with \\log.\n\n\n\n\n\nIt can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, consider the following code.\n\n\n\n\n\n```{r}\n#| label: fig-uniquename\n#| fig-cap: Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\n#| warning: false\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\")\n```\n\n\n\n\n\n\n\nFigure 2: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\n\nThen (@fig-uniquename) would produce: (Figure 2) as the name of the R chunk is fig-uniquename. We need to add “fig” to the start of the chunk name so that Quarto knows that this is a figure. We then include a “fig-cap:” in the R chunk that specifies a caption.\n\n\n\nWe can add #| layout-ncol: 2 in an R chunk within a Quarto document to have two graphs appear side by side (Figure 3). Here Figure 3 (a) uses the minimal theme, and Figure 3 (b) uses the classic theme. These both cross-reference the same label #| label: fig-doctorgraphsidebyside in the R chunk, with an additional option added in the R chunk of #| fig-subcap: [\"Number of illnesses\",\"Number of visits to the doctor\"] which provides the sub-captions. The addition of a letter in-text is accomplished by adding “-1” and “-2” to the end of the label when it is used in-text: (@fig-doctorgraphsidebyside), @fig-doctorgraphsidebyside-1, and @fig-doctorgraphsidebyside-2 for (Figure 3), Figure 3 (a), and Figure 3 (b), respectively.\n```{r}\n#| eval: true\n#| warning: false\n#| label: fig-doctorgraphsidebyside\n#| fig-cap: \"Two variants of graphs\"\n#| fig-subcap: [\"Illnesses\",\"Visits to the doctor\"]\n#| layout-ncol: 2\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\") +\n    theme_minimal()\n\nDoctorVisits |&gt;\n    ggplot(aes(x = visits)) +\n    geom_histogram(stat = \"count\") +\n    theme_classic()\n```\n\n\n\n\n\n\n\n\n\n\n\n(a) Illnesses\n\n\n\n\n\n\n\n\n\n\n\n(b) Visits to the doctor\n\n\n\n\n\n\n\nFigure 3: Two variants of graphs\n\n\n\n\n\n\nWe can take a similar approach to cross-reference tables. For instance, (@tbl-docvisittable) will produce: (Table 1). In this case we specify “tbl” at the start of the label so that Quarto knows that it is a table. And we specify a caption for the table with “tbl-cap:”.\n\n```{r}\n#| label: tbl-docvisittable\n#| tbl-cap: \"Distribution of the number of doctor visits\"\n\nDoctorVisits |&gt;\n    count(visits) |&gt;\n    kable()\n```\n\n\n\nTable 1: Distribution of the number of doctor visits\n\n\n\n\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can also cross-reference equations. To that we need to add a tag such as {#eq-macroidentity} which we then reference.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-gdpidentity}\nFor instance, we then use @eq-gdpidentity to produce Equation 1\n\\[\nY = C + I + G + (X - M)\n\\tag{1}\\]\nLabels should be relatively simple when using cross-references. In general, try to keep the names simple but unique, avoid punctuation, and stick to letters and hyphens. Try not to use underscores, because they can cause an error.\n\n\n\nProjects are widely used in software development and exist to keep all the files (data, analysis, report, etc) associated with a particular project together and related to each other. (This use of “project” in a software development sense, is distinct to a “project”, in the project management sense.) An R Project can be created in RStudio. Click “File” \\(\\rightarrow\\) “New Project”, then select “Empty project”, name the R Project and decide where to save it. For instance, a R Project focused on maternal mortality may be called “maternalmortality”. The use of R Projects enables “reliable, polite behavior across different computers or users and over time” [@whattheyforgot]. This is because they remove the context of that folder from its broader existence; files exist in relation to the base of the R Project, not the base of the computer.\n\n\n\nOnce a project has been created, a new file with the extension “.RProj” will appear in that folder. An example of a folder with an R Project, a Quarto document, and an appropriate file structure is available here. That can be downloaded: “Code” \\(\\rightarrow\\) “Download ZIP”.\nThe main advantage of using an R Project is that we can reference files within it in a self-contained way. That means when others want to reproduce our work, they will not need to change all the file references and structure as everything is referenced in relation to the “.Rproj” file. For instance, instead of reading a CSV from, say, \"~/Documents/projects/book/data/\" you can read it from book/data/. It may be that someone else does not have a projects folder, and so the former would not work for them, while the latter would.\n\n\n\nThe use of projects is required to meet the minimal level of reproducibility expected of credible work. The use of functions such as setwd(), and computer-specific file paths, bind work to a specific computer in a way that is not appropriate.\nThere are a variety of ways to set up a folder. A variant of @wilsongoodenough that is often useful when you are getting started is shown in the example file structure linked above.\n\n\n\nexample_project/\n├── .gitignore\n├── LICENSE.md\n├── README.md\n├── example_project.Rproj\n├── inputs\n│   ├── data\n│   │   ├── unedited_data.csv\n│   │   └── ...\n│   ├── literature\n│   │   ├── alexander-tellingstorieswithdata.pdf\n│   │   ├── gelman-xboxpaper.pdf\n│   │   └── ...\n├── outputs\n│   ├── README.md\n│   ├── data\n│   │   ├── analysis_data.csv\n│   │   └── ...\n│   ├── paper\n│   │   ├── paper.pdf\n│   │   ├── paper.qmd\n│   │   ├── references.bib\n│   │   └── ...\n│   └── ...\n├── scripts\n│   ├── 00-simulate_data.R\n│   ├── 01-download_data.R\n│   ├── 02-data_cleaning.R\n│   ├── 03-test_data.R\n│   └── ...\n└── ...\nHere we have an inputs folder that contains original, unedited data that should not be written over [@wilsongoodenough] and literature related to the project. An outputs folder contains data that we create using R, as well as the paper that we are writing. And a scripts folder is what modifies the unedited data and saves it into outputs. We will do most of our work in “scripts”, and the Quarto file for the paper in outputs. Useful other aspects include a README.md which will specify overview details about the project, and a LICENSE. An example of what to put in the README is here. Another helpful variant of this project skeleton is provided by @GoodResearchCode."
  },
  {
    "objectID": "lectures/lecture-05-content.html#key-concepts-and-skills",
    "href": "lectures/lecture-05-content.html#key-concepts-and-skills",
    "title": "Get Started",
    "section": "",
    "text": "Key concepts and skills\n\nReproducibility typically begins as something that someone imposes on you. It can be onerous and annoying. This typically lasts until you need to revisit a project after a small break. At that point you typically realize that reproducibility is not just a requirement for data science because it is the only way that we can make genuine progress, but because it helps us help ourselves.\nReproducibility implies sharing data, code, and environment. This is enhanced by using Quarto, R Projects, and Git and GitHub: Quarto builds documents that integrate normal text and R code; R Projects enable a file structure that is not dependent on a user’s personal directory set-up; and Git and GitHub make it easier to share code and data.\nThis is not an unimpeachable workflow, but one that is good enough and provides many of the benefits. We will improve various aspects of it through various tools, but improving code structure and comments goes a long way.\nThere are always errors that occur, and it is important to recognize that debugging is a skill that improves with practice. But one key aspect of being able to get help is to be able to make a reproducible example others can use."
  },
  {
    "objectID": "lectures/lecture-05-content.html#software-and-packages",
    "href": "lectures/lecture-05-content.html#software-and-packages",
    "title": "Get Started",
    "section": "",
    "text": "Base R [@citeR]\nAER [@citeaer]\nfuture [@future]\ngitcreds [@gitcreds]\nknitr [@citeknitr]\nlintr [@lintr]\nrenv [@renv]\nreprex [@reprex]\nstyler [@styler]\ntidyverse [@tidyverse]\ntinytex [@tinytex]\nusethis [@usethis]\n\n\n\nlibrary(AER)\nlibrary(future)\nlibrary(gitcreds)\nlibrary(knitr)\nlibrary(lintr)\nlibrary(renv)\nlibrary(reprex)\nlibrary(styler)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(usethis)"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section",
    "href": "lectures/lecture-05-content.html#section",
    "title": "Get Started",
    "section": "",
    "text": "The number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\\(\\dots\\) So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty.\nFrançois Chollet, 20 February 2020.\n\nIf science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is a critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, there is a need for reproducible data science workflows.\n@Alexander2019 defines reproducible research as that which can be exactly redone, given all the materials used. This underscores the importance of providing the code, data, and environment. The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables. Ironically, there are different definitions of reproducibility between disciplines. @barba2018terminologies surveys a variety of disciplines and concludes that the predominant language usage implies the following definitions:\n\nReproducible research is when “[a]uthors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.”\nA replication is a study “that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.”\n\nRegardless of what it is specifically called, @Gelman2016 identifies how large an issue the lack of it is in various social sciences. Work that is not reproducible does not contribute to our stock of knowledge about the world. This is wasteful and potentially even unethical. Since @Gelman2016, a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. That is also the case in the life sciences [@heil2021reproducibility], cancer research [@Begley2012; @Mullard2021], and computer science [@pineau2021improving].\nSome of the examples that @Gelman2016 talks about are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created “nudge” units that implement public policy [@sunstein2017economics] even though there is evidence that some of the claims lack credibility [@nonudge; @gelmannudge]. Governments are increasingly using algorithms that they do not make open [@chouldechova18a]. And @herndon2014does document how research in economics that was used by governments to justify austerity policies following the 2007–2008 financial crisis turned out to not be reproducible.\nAt a minimum, and with few exceptions, we must release our code, datasets, and environment. Without these, it is difficult to know what a finding speaks to [@miyakawa2020no]. More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked [@merali2010computational; @hillelwayne; @natefixesmistake]. Increasingly, following @buckheit1995wavelab, we consider a paper to be an advertisement, and for the associated code, data, and environment to be the actual work. Steve Jobs, a co-founder of Apple, talked about how people who are the best at their craft ensure that even the aspects of their work that no one else will ever see are as well finished and high quality as the aspects that are public facing [@stevejobs]. The same is true in data science, where often one of the distinguishing aspects of high-quality work is that the README and code comments are as polished as, say, the abstract of the associated paper."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-1",
    "href": "lectures/lecture-05-content.html#section-1",
    "title": "Get Started",
    "section": "",
    "text": "Workflows exist within a cultural and social context, which imposes an additional ethical reason for the need for them to be reproducible. For instance, @wang2018deep train a neural network to distinguish between the faces of gay and heterosexual men. (@murphy2017 provides a summary of the paper, the associated issues, and comments from its authors.) To do this, @wang2018deep [p. 248] needed a dataset of photos of people that were “adult, Caucasian, fully visible, and of a gender that matched the one reported on the user’s profile”. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. The instructions provided to the Mechanical Turk workers for this task specify that Barack Obama, the 44th US President, who had a white mother and a black father, should be classified as “Black”; and that Latino is an ethnicity, rather than a race [@Mattson2017]. The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.\nThis is just one specific concern about one part of the @wang2018deep workflow. Broader concerns are raised by others including @Gelman_2018. The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of @wang2018deep is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-2",
    "href": "lectures/lecture-05-content.html#section-2",
    "title": "Get Started",
    "section": "",
    "text": "Some of the steps that we can take to make our work more reproducible include:\n\nEnsure the entire workflow is documented. This may involve addressing questions such as:\n\nHow was the original, unedited dataset obtained and is access likely to be persistent and available to others?\nWhat specific steps are being taken to transform the original, unedited data into the data that were analyzed, and how can this be made available to others?\nWhat analysis has been done, and how clearly can this be shared?\nHow has the final paper or report been built and to what extent can others follow that process themselves?\n\nNot worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to do the last, until you can do the first:\n\nCan you run your entire workflow again?\nCan another person run your entire workflow again?\nCan “future-you” run your entire workflow again?\nCan “future-another-person” run your entire workflow again?\n\nIncluding a detailed discussion about the limitations of the dataset and the approach in the final paper or report."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-3",
    "href": "lectures/lecture-05-content.html#section-3",
    "title": "Get Started",
    "section": "",
    "text": "The workflow that we advocate in this book is:\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander’s [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\nBut it can be alternatively considered as: “Think an awful lot, mostly read and write, sometimes code”."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-4",
    "href": "lectures/lecture-05-content.html#section-4",
    "title": "Get Started",
    "section": "",
    "text": "There are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes\n\nQuarto,\nR Projects\nGit and GitHub"
  },
  {
    "objectID": "lectures/lecture-05-content.html#quarto",
    "href": "lectures/lecture-05-content.html#quarto",
    "title": "Get Started",
    "section": "",
    "text": "Quarto integrates code and natural language in a way that is called “literate programming” [@Knuth1984]. It is the successor to R Markdown, which was a variant of Markdown specifically designed to allow R code chunks to be included. Quarto uses a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a “What You See Is What You Get” (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level headings will look the same. But it means that we must designate or “mark up” how we would like certain aspects to appear. And it is only when we render the document that we get to see what it looks like. A visual editor option can also be used, and this hides the need for the user to do this mark-up themselves."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-5",
    "href": "lectures/lecture-05-content.html#section-5",
    "title": "Get Started",
    "section": "",
    "text": "One advantage of literate programming is that we get a “live” document in which code executes and then forms part of the document. Another advantage of Quarto is that similar code can compile into a variety of documents, including HTML and PDFs. Quarto also has default options for including a title, author, and date. One disadvantage is that it can take a while for a document to compile because the code needs to run.\nWe need to download Quarto from here. (Skip this step if you are using Posit Cloud because it is already installed.) We can then create a new Quarto document within RStudio: “File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “Quarto Document\\(\\dots\\)”.\nAfter opening a new Quarto document and selecting “Source” view, you will see the default top matter, contained within a pair of three dashes, as well as some examples of text showing a few of the markdown essential commands and R chunks, each of which are discussed further in the following sections."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-6",
    "href": "lectures/lecture-05-content.html#section-6",
    "title": "Get Started",
    "section": "",
    "text": "Top matter consists of defining aspects such as the title, author, and date. It is contained within three dashes at the top of a Quarto document. For instance, the following would specify a title, a date that automatically updated to the date the document was rendered, and an author.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: html\n---"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-7",
    "href": "lectures/lecture-05-content.html#section-7",
    "title": "Get Started",
    "section": "",
    "text": "An abstract is a short summary of the paper, and we could add that to the top matter.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: html\n---"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-8",
    "href": "lectures/lecture-05-content.html#section-8",
    "title": "Get Started",
    "section": "",
    "text": "By default, Quarto will create an HTML document, but we can change the output format to produce a PDF. This uses LaTeX in the background and requires the installation of supporting packages. To do this install tinytex. But as it is used in the background we should not need to load it.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: pdf\n---"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-9",
    "href": "lectures/lecture-05-content.html#section-9",
    "title": "Get Started",
    "section": "",
    "text": "We can include references by specifying a BibTeX file in the top matter and then calling it within the text, as needed.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: pdf\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-10",
    "href": "lectures/lecture-05-content.html#section-10",
    "title": "Get Started",
    "section": "",
    "text": "We would need to make a separate file called “bibliography.bib” and save it next to the Quarto file. In the BibTeX file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with citation() and this can be added to the “bibliography.bib” file. The citation for a package can be found by including the package name, for instance citation(\"tidyverse\"), and again adding the output to the “.bib” file. It can be helpful to use Google Scholar or doi2bib to get citations for books or articles."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-11",
    "href": "lectures/lecture-05-content.html#section-11",
    "title": "Get Started",
    "section": "",
    "text": "We need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance “citeR”.\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@book{tellingstories,\n    title = {Telling Stories with Data},\n    author = {Rohan Alexander},\n    year = {2023},\n    publisher = {Chapman and Hall/CRC},\n    url = {https://tellingstorieswithdata.com}\n  }"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-12",
    "href": "lectures/lecture-05-content.html#section-12",
    "title": "Get Started",
    "section": "",
    "text": "To cite R in the Quarto document we then include @citeR, which would put brackets around the year: @citeR, or [@citeR], which would put brackets around the whole thing: [@citeR]."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-13",
    "href": "lectures/lecture-05-content.html#section-13",
    "title": "Get Started",
    "section": "",
    "text": "The reference list at the end of the paper is automatically built based on calling the BibTeX file and including references in the paper. At the end of the Quarto document, include a heading “# References” and the actual citations will be included after that. When the Quarto file is rendered, Quarto sees these in the content, goes to the BibTeX file to get the reference details that it needs, builds the reference list, and then adds it at the end of the rendered document."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-14",
    "href": "lectures/lecture-05-content.html#section-14",
    "title": "Get Started",
    "section": "",
    "text": "Quarto uses a variation of Markdown as its underlying syntax. Essential Markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in RStudio: “Help” \\(\\rightarrow\\) “Markdown Quick Reference”. It is your choice as to whether you want to use the visual or source editor. But either way, it is good to understand these essentials because it will not always be possible to use a visual editor (for instance if you are quickly looking at a Quarto document in GitHub). As you get more experience it can be useful to use a text editor such as Sublime Text, or an alternative Integrated Development Environment such as VS Code.\n\nEmphasis: *italic*, **bold**\nHeaders (these go on their own line with a blank line before and after):\n\n         # First level header\n         \n         ## Second level header\n         \n         ### Third level header\n\nUnordered list, with sub-lists:\n\n    * Item 1\n    * Item 2\n        + Item 2a\n        + Item 2b\n\nOrdered list, with sub-lists:\n\n    1. Item 1\n    2. Item 2\n    3. Item 3\n        + Item 3a\n        + Item 3b\n\nURLs can be added: [this book](https://www.tellingstorieswithdata.com) results in this book.\nA paragraph is created by leaving a blank line.\n\nA paragraph about an idea, nicely spaced from the following paragraph.\n\nA paragraph about another idea, again spaced from the earlier paragraph."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-15",
    "href": "lectures/lecture-05-content.html#section-15",
    "title": "Get Started",
    "section": "",
    "text": "Once we have added some aspects, then we may want to see the actual document. To build the document click “Render”."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-16",
    "href": "lectures/lecture-05-content.html#section-16",
    "title": "Get Started",
    "section": "",
    "text": "We can include code for R and many other languages in code chunks within a Quarto document. When we render the document the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell Quarto that this is an R chunk. Anything inside this chunk will be considered R code and run as such. We use data from @citeaer who provide the R package AER to accompany their book Applied Econometrics with R. We could load the tidyverse and install and load AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-17",
    "href": "lectures/lecture-05-content.html#section-17",
    "title": "Get Started",
    "section": "",
    "text": "The output of that code is Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\n\nThere are various evaluation options that are available in chunks. We include these, each on a new line, by opening the line with the chunk-specific comment delimiter “#|” and then the option. Helpful options include:\n\necho: This controls whether the code itself is included in the document. For instance, #| echo: false would mean the code will be run and its output will show, but the code itself would not be included in the document.\ninclude: This controls whether the output of the code is included in the document. For instance, #| include: false would run the code, but would not result in any output, and the code itself would not be included in the document.\neval: This controls whether the code should be included in the document. For instance, #| eval: false would mean that the code is not run, and hence there would not be any output to include, but the code itself would be included in the document.\nwarning: This controls whether warnings should be included in the document. For instance, #| warning: false would mean that warnings are not included.\nmessage: This controls whether messages should be included in the document. For instance, #| message: false would mean that messages are not included in the document.\n\nFor instance, we could include the output, but not the code, and suppress any warnings.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nLeave a blank line on either side of an R chunk, otherwise it may not run properly. And use lower case for logical values, i.e. “false” not “FALSE”.\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| eval: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then...\nThe Quarto document itself must load any datasets that are needed. It is not enough that they are in the environment. This is because the Quarto document evaluates the code in the document when it is rendered, not necessarily the environment.\n\n\nWe can include equations by using LaTeX, which is based on the programming language TeX. We invoke math mode in LaTeX by using two dollar signs as opening and closing tags. Then whatever is inside is evaluated as LaTeX mark-up. For instance we can produce the compound interest formula with:\n$$\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n$$\n\\[\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-18",
    "href": "lectures/lecture-05-content.html#section-18",
    "title": "Get Started",
    "section": "",
    "text": "LaTeX is a comprehensive mark-up language but we will mostly just use it to specify the model of interest. We include some examples here that contain the critical aspects we will draw on starting in ?@sec-its-just-a-linear-model.\n$$\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n$$\n\\[\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-19",
    "href": "lectures/lecture-05-content.html#section-19",
    "title": "Get Started",
    "section": "",
    "text": "Underscores are used to get subscripts: y_i for \\(y_i\\). And we can get a subscript of more than one item by surrounding it with curly braces: y_{i,c} for \\(y_{i,c}\\). In this case we wanted math mode within the line, and so we surround these with only one dollar sign as opening and closing tags.\nGreek letters are typically preceded by a backslash. Common Greek letters include: \\alpha for \\(\\alpha\\), \\beta for \\(\\beta\\), \\delta for \\(\\delta\\), \\epsilon for \\(\\epsilon\\), \\gamma for \\(\\gamma\\), \\lambda for \\(\\lambda\\), \\mu for \\(\\mu\\), \\phi for \\(\\phi\\), \\pi for \\(\\pi\\), \\Pi for \\(\\Pi\\), \\rho for \\(\\rho\\), \\sigma for \\(\\sigma\\), \\Sigma for \\(\\Sigma\\), \\tau for \\(\\tau\\), and \\theta for \\(\\theta\\)."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-20",
    "href": "lectures/lecture-05-content.html#section-20",
    "title": "Get Started",
    "section": "",
    "text": "LaTeX math mode assumes letters are variables and so makes them italic, but sometimes we want a word to appear in normal font because it is not a variable, such as “Normal”. In that case we surround it with \\mbox{}, for instance \\mbox{Normal} for \\(\\mbox{Normal}\\)."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-21",
    "href": "lectures/lecture-05-content.html#section-21",
    "title": "Get Started",
    "section": "",
    "text": "We line up equations across multiple lines using \\begin{aligned} and \\end{aligned}. Then the item that is to be lined up is noted by an ampersand. The following is a model that we will estimate in ?@sec-multilevel-regression-with-post-stratification.\n$$\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-22",
    "href": "lectures/lecture-05-content.html#section-22",
    "title": "Get Started",
    "section": "",
    "text": "Finally, certain functions are built into LaTeX. For instance, we can appropriately typeset “log” with \\log."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-23",
    "href": "lectures/lecture-05-content.html#section-23",
    "title": "Get Started",
    "section": "",
    "text": "It can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, consider the following code."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-24",
    "href": "lectures/lecture-05-content.html#section-24",
    "title": "Get Started",
    "section": "",
    "text": "```{r}\n#| label: fig-uniquename\n#| fig-cap: Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\n#| warning: false\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\")\n```\n\n\n\n\n\n\n\nFigure 2: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\n\nThen (@fig-uniquename) would produce: (Figure 2) as the name of the R chunk is fig-uniquename. We need to add “fig” to the start of the chunk name so that Quarto knows that this is a figure. We then include a “fig-cap:” in the R chunk that specifies a caption."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-25",
    "href": "lectures/lecture-05-content.html#section-25",
    "title": "Get Started",
    "section": "",
    "text": "We can add #| layout-ncol: 2 in an R chunk within a Quarto document to have two graphs appear side by side (Figure 3). Here Figure 3 (a) uses the minimal theme, and Figure 3 (b) uses the classic theme. These both cross-reference the same label #| label: fig-doctorgraphsidebyside in the R chunk, with an additional option added in the R chunk of #| fig-subcap: [\"Number of illnesses\",\"Number of visits to the doctor\"] which provides the sub-captions. The addition of a letter in-text is accomplished by adding “-1” and “-2” to the end of the label when it is used in-text: (@fig-doctorgraphsidebyside), @fig-doctorgraphsidebyside-1, and @fig-doctorgraphsidebyside-2 for (Figure 3), Figure 3 (a), and Figure 3 (b), respectively.\n```{r}\n#| eval: true\n#| warning: false\n#| label: fig-doctorgraphsidebyside\n#| fig-cap: \"Two variants of graphs\"\n#| fig-subcap: [\"Illnesses\",\"Visits to the doctor\"]\n#| layout-ncol: 2\n\nDoctorVisits |&gt;\n    ggplot(aes(x = illness)) +\n    geom_histogram(stat = \"count\") +\n    theme_minimal()\n\nDoctorVisits |&gt;\n    ggplot(aes(x = visits)) +\n    geom_histogram(stat = \"count\") +\n    theme_classic()\n```\n\n\n\n\n\n\n\n\n\n\n\n(a) Illnesses\n\n\n\n\n\n\n\n\n\n\n\n(b) Visits to the doctor\n\n\n\n\n\n\n\nFigure 3: Two variants of graphs"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-26",
    "href": "lectures/lecture-05-content.html#section-26",
    "title": "Get Started",
    "section": "",
    "text": "We can take a similar approach to cross-reference tables. For instance, (@tbl-docvisittable) will produce: (Table 1). In this case we specify “tbl” at the start of the label so that Quarto knows that it is a table. And we specify a caption for the table with “tbl-cap:”.\n\n```{r}\n#| label: tbl-docvisittable\n#| tbl-cap: \"Distribution of the number of doctor visits\"\n\nDoctorVisits |&gt;\n    count(visits) |&gt;\n    kable()\n```\n\n\n\nTable 1: Distribution of the number of doctor visits\n\n\n\n\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-27",
    "href": "lectures/lecture-05-content.html#section-27",
    "title": "Get Started",
    "section": "",
    "text": "Finally, we can also cross-reference equations. To that we need to add a tag such as {#eq-macroidentity} which we then reference.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-gdpidentity}\nFor instance, we then use @eq-gdpidentity to produce Equation 1\n\\[\nY = C + I + G + (X - M)\n\\tag{1}\\]\nLabels should be relatively simple when using cross-references. In general, try to keep the names simple but unique, avoid punctuation, and stick to letters and hyphens. Try not to use underscores, because they can cause an error."
  },
  {
    "objectID": "lectures/lecture-05-content.html#r-projects-and-file-structure",
    "href": "lectures/lecture-05-content.html#r-projects-and-file-structure",
    "title": "Get Started",
    "section": "",
    "text": "Projects are widely used in software development and exist to keep all the files (data, analysis, report, etc) associated with a particular project together and related to each other. (This use of “project” in a software development sense, is distinct to a “project”, in the project management sense.) An R Project can be created in RStudio. Click “File” \\(\\rightarrow\\) “New Project”, then select “Empty project”, name the R Project and decide where to save it. For instance, a R Project focused on maternal mortality may be called “maternalmortality”. The use of R Projects enables “reliable, polite behavior across different computers or users and over time” [@whattheyforgot]. This is because they remove the context of that folder from its broader existence; files exist in relation to the base of the R Project, not the base of the computer."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-28",
    "href": "lectures/lecture-05-content.html#section-28",
    "title": "Get Started",
    "section": "",
    "text": "Once a project has been created, a new file with the extension “.RProj” will appear in that folder. An example of a folder with an R Project, a Quarto document, and an appropriate file structure is available here. That can be downloaded: “Code” \\(\\rightarrow\\) “Download ZIP”.\nThe main advantage of using an R Project is that we can reference files within it in a self-contained way. That means when others want to reproduce our work, they will not need to change all the file references and structure as everything is referenced in relation to the “.Rproj” file. For instance, instead of reading a CSV from, say, \"~/Documents/projects/book/data/\" you can read it from book/data/. It may be that someone else does not have a projects folder, and so the former would not work for them, while the latter would."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-29",
    "href": "lectures/lecture-05-content.html#section-29",
    "title": "Get Started",
    "section": "",
    "text": "The use of projects is required to meet the minimal level of reproducibility expected of credible work. The use of functions such as setwd(), and computer-specific file paths, bind work to a specific computer in a way that is not appropriate.\nThere are a variety of ways to set up a folder. A variant of @wilsongoodenough that is often useful when you are getting started is shown in the example file structure linked above."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-30",
    "href": "lectures/lecture-05-content.html#section-30",
    "title": "Get Started",
    "section": "",
    "text": "example_project/\n├── .gitignore\n├── LICENSE.md\n├── README.md\n├── example_project.Rproj\n├── inputs\n│   ├── data\n│   │   ├── unedited_data.csv\n│   │   └── ...\n│   ├── literature\n│   │   ├── alexander-tellingstorieswithdata.pdf\n│   │   ├── gelman-xboxpaper.pdf\n│   │   └── ...\n├── outputs\n│   ├── README.md\n│   ├── data\n│   │   ├── analysis_data.csv\n│   │   └── ...\n│   ├── paper\n│   │   ├── paper.pdf\n│   │   ├── paper.qmd\n│   │   ├── references.bib\n│   │   └── ...\n│   └── ...\n├── scripts\n│   ├── 00-simulate_data.R\n│   ├── 01-download_data.R\n│   ├── 02-data_cleaning.R\n│   ├── 03-test_data.R\n│   └── ...\n└── ...\nHere we have an inputs folder that contains original, unedited data that should not be written over [@wilsongoodenough] and literature related to the project. An outputs folder contains data that we create using R, as well as the paper that we are writing. And a scripts folder is what modifies the unedited data and saves it into outputs. We will do most of our work in “scripts”, and the Quarto file for the paper in outputs. Useful other aspects include a README.md which will specify overview details about the project, and a LICENSE. An example of what to put in the README is here. Another helpful variant of this project skeleton is provided by @GoodResearchCode."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-32",
    "href": "lectures/lecture-05-content.html#section-32",
    "title": "Get Started",
    "section": "",
    "text": "In this book we implement version control through a combination of Git and GitHub. There are a variety of reasons for this including:\n\nenhancing the reproducibility of work by making it easier to share code and data;\nmaking it easier to share work;\nimproving workflow by encouraging systematic approaches; and\nmaking it easier to work in teams."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-33",
    "href": "lectures/lecture-05-content.html#section-33",
    "title": "Get Started",
    "section": "",
    "text": "Git is a version control system with a fascinating history [@githistory]. The way one often starts doing version control is to have various copies of the one file: “first_go.R”, “first_go-fixed.R”, “first_go-fixed-with-mons-edits.R”. But this soon becomes cumbersome. One often soon turns to dates, for instance: “2022-01-01-analysis.R”, “2022-01-02-analysis.R”, “2022-01-03-analysis.R”, etc. While this keeps a record, it can be difficult to search when we need to go back, because it is hard to remember the date some change was made. In any case, it quickly gets unwieldy for a project that is being regularly worked on."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-34",
    "href": "lectures/lecture-05-content.html#section-34",
    "title": "Get Started",
    "section": "",
    "text": "Instead of this, we use Git so that we can have one version of the file. Git keeps a record of the changes to that file, and a snapshot of that file at a given point in time. We determine when Git takes that snapshot. We additionally include a message saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, and the history can be more easily searched."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-35",
    "href": "lectures/lecture-05-content.html#section-35",
    "title": "Get Started",
    "section": "",
    "text": "One complication is that Git was designed for teams of software developers. As such, while it works, it can be a little ungainly for non-developers. Nonetheless Git has been usefully adapted for data science, even when the only collaborator one may have is one’s future self [@Bryan2018].\nGitHub, GitLab, and various other companies offer easier-to-use services that build on Git. While there are tradeoffs, we introduce GitHub here because it is the predominant platform [@eghbal2020working, p. 21]. Git and GitHub are built into Posit Cloud, which provides a nice option if you have issues with local installation. One of the initial challenging aspects of Git is the terminology. Folders are called “repos”. Creating a snapshot is called a “commit”. One gets used to it eventually, but feeling confused initially is normal. @happygit is especially useful for setting up and using Git and GitHub."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-36",
    "href": "lectures/lecture-05-content.html#section-36",
    "title": "Get Started",
    "section": "",
    "text": "Git\nWe first need to check whether Git is installed. Open RStudio, go to the Terminal, type the following, and then enter/return.\n\ngit --version\n\nIf you get a version number, then you are done (Figure 4 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Using Terminal to check whether Git is installed in RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Adding a username and email address to Git in RStudio\n\n\n\n\n\n\n\nFigure 4: An overview of the steps involved in setting up Git"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-37",
    "href": "lectures/lecture-05-content.html#section-37",
    "title": "Get Started",
    "section": "",
    "text": "Git is pre-installed in Posit Cloud, it should be pre-installed on Mac, and it may be pre-installed on Windows. If you do not get a version number in response, then you need to install it. To do that you should follow the instructions specific to your operating system in @happygit [Chapter 5].\nAfter Git is installed we need to tell it a username and email. We need to do this because Git adds this information whenever we take a snapshot, or to use Git’s language, whenever we make a commit."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-38",
    "href": "lectures/lecture-05-content.html#section-38",
    "title": "Get Started",
    "section": "",
    "text": "Again, within the Terminal, type the following, replacing the details with yours, and then press “enter/return” after each line.\n\ngit config --global user.name \"Rohan Alexander\"\ngit config --global user.email \"rohan.alexander@utoronto.ca\"\ngit config --global --list\n\nWhen this set-up has been done properly, the values that you entered for “user.name” and “user.email” will be returned after the last line (Figure 4 (b)).\nThese details—username and email address—will be public. There are various ways to hide the email address if necessary, and GitHub provides instructions about this. @happygit [Chapter 7] provides more detailed instructions about this step, and a trouble-shooting guide."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-39",
    "href": "lectures/lecture-05-content.html#section-39",
    "title": "Get Started",
    "section": "",
    "text": "GitHub\nNow that Git is set up, we need to set up GitHub. We created a GitHub account in ?@sec-fire-hose, which we use again here. After being signed in at github.com we first need to make a new folder, which is called a “repo” in Git. Look for a “+” in the top right, and then select “New Repository” (Figure 5 (a)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Start process of creating a new repository\n\n\n\n\n\n\n\n\n\n\n\n(b) Copy the URL of the new repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Adding the project to Posit Cloud\n\n\n\n\n\n\n\n\n\n\n\n(d) Creating a PAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Adding files to be committed\n\n\n\n\n\n\n\n\n\n\n\n(f) Making a commit\n\n\n\n\n\n\n\nFigure 5: An overview of the steps involved in setting up GitHub"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-40",
    "href": "lectures/lecture-05-content.html#section-40",
    "title": "Get Started",
    "section": "",
    "text": "At this point we can add a sensible name for the repo. Leave it as “public” for now, because it can always be deleted later. And check the box to “Initialize this repository with a README”. Change “Add .gitignore” to R. After that, click “Create repository”.\nThis will take us to a screen that is fairly empty, but the details that we need—a URL—are in the green “Clone or Download” button, which we can copy by clicking the clipboard (Figure 5 (b))."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-41",
    "href": "lectures/lecture-05-content.html#section-41",
    "title": "Get Started",
    "section": "",
    "text": "Now returning to RStudio, in Posit Cloud, we create a new project using “New Project from Git Repository”. It will ask for the URL that we just copied (Figure 5 (c)). If you are using a local computer, then this step is accomplished through the menu: “File” \\(\\rightarrow\\) “New Project…” \\(\\rightarrow\\) “Version Control” \\(\\rightarrow\\) “Git”, then paste in the URL, give the folder a meaningful name, check “Open in new session”, then click “Create Project”."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-42",
    "href": "lectures/lecture-05-content.html#section-42",
    "title": "Get Started",
    "section": "",
    "text": "At this point, a new folder has been created that we can use. We will want to be able to push it back to GitHub, and for that we will need to use a Personal Access Token (PAT) to link our RStudio Workspace with our GitHub account. We use usethis and gitcreds to enable this. These are, respectively, a package that automates repetitive tasks, and a package that authenticates with GitHub. To create a PAT, while signed into GitHub in the browser, and after installing and loading usethis run create_github_token() in your R session. GitHub will open in the browser with various options filled out (Figure 5 (d)). It can be useful to give the PAT an informative name by replacing “Note”, for instance “PAT for RStudio”, then click “Generate token”."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-43",
    "href": "lectures/lecture-05-content.html#section-43",
    "title": "Get Started",
    "section": "",
    "text": "We only have one chance to copy this token, and if we make a mistake then we will need to generate a new one. Do not include the PAT in any R script or Quarto document. Instead, after installing and loading gitcreds, run gitcreds_set(), which will then prompt you to add your PAT in the console."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-44",
    "href": "lectures/lecture-05-content.html#section-44",
    "title": "Get Started",
    "section": "",
    "text": "To use GitHub for a project that we are actively working on we follow this procedure:\n\nThe first thing to do is almost always to get any changes with “pull”. To do this, open the Git pane in RStudio, and click the blue down arrow. This gets any changes to the folder, as it is on GitHub, into our own version of the folder.\nWe can then make our changes to our copy of the folder. For instance, we could update the README, and then save it as normal.\nOnce this is done, we need to add, commit, and push. In the Git pane in RStudio, select the files to be added. This adds them to the staging area. Then click “Commit” (Figure 5 (e)). A new window will open. Add an informative message about the change that was made, and then click “Commit” in that new window (Figure 5 (f)). Finally, click “Push” to send the changes to GitHub."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-45",
    "href": "lectures/lecture-05-content.html#section-45",
    "title": "Get Started",
    "section": "",
    "text": "There are a few common pain-points when it comes to Git and GitHub. We recommend committing and pushing regularly, especially when you are new to version control. This increases the number of snapshots that you could come back to if needed. All commits should have an informative commit message. If you are new to version control, then the expectation of a good commit message is that it contains a short summary of the change, followed by a blank line, and then an explanation of the change including what the change is, and why it is being made. For instance, if your commit adds graphs to a paper, then a commit message could be:\nAdd graphs\n\nGraphs of unemployment and inflation added into Data section."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-46",
    "href": "lectures/lecture-05-content.html#section-46",
    "title": "Get Started",
    "section": "",
    "text": "There is some evidence of a relationship between overall quality and commit behavior [@sprint2019mining]. As you get more experience ideally the commit messages will act as a kind of journal of the project. But the main thing is to commit regularly.\nGit and GitHub were designed for software developers, rather than data scientists. GitHub limits the size of the files it will consider to 100MB, and even 50MB can prompt a warning. Data science projects regularly involve datasets that are larger than this. In ?@sec-store-and-share we discuss the use of data deposits, which can be especially useful when a project is completed, but when we are actively working on a project it can be useful to ignore large data files, at least as far as Git and GitHub are concerned. We do this using a “.gitignore” file, in which we list all of the files that we do not want to track using Git. The example folder contains a “.gitignore” file. And it can be helpful to run git_vaccinate() from usethis, which will add a variety of files to a global “.gitignore” file in case you forget to do it on a project basis. Mac users will find it useful that this will cause “.DS_Store” files to be ignored."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-47",
    "href": "lectures/lecture-05-content.html#section-47",
    "title": "Get Started",
    "section": "",
    "text": "We used the Git pane in RStudio which removed the need to use the Terminal, but it did not remove the need to go to GitHub and set up a new project. Having set up Git and GitHub, we can further improve this aspect of our workflow with usethis.\nFirst check that Git is set up with git_sitrep() from usethis. This should print information about the username and email. We can use use_git_config() to update these details if needed.\n\nuse_git_config(\n    user.name = \"Rohan Alexander\",\n    user.email = \"rohan.alexander@utoronto.ca\"\n)"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-48",
    "href": "lectures/lecture-05-content.html#section-48",
    "title": "Get Started",
    "section": "",
    "text": "Rather than starting a new project in GitHub, and then adding it locally, we can now use use_git() to initiate it and commit the files. Having committed, we can use use_github() to push to GitHub, which will create the folder on GitHub as well.\nIt is normal to be intimidated by Git and GitHub. Many data scientists only know a little about how to use it, and that is okay. Try to push regularly so that you have a recent snapshot in case you need it."
  },
  {
    "objectID": "lectures/lecture-05-content.html#sec-dealingwitherrors",
    "href": "lectures/lecture-05-content.html#sec-dealingwitherrors",
    "title": "Get Started",
    "section": "Dealing with errors",
    "text": "Dealing with errors\n\nWhen you are programming, eventually your code will break, when I say eventually, I mean like probably 10 or 20 times a day.\n@sharlatalks"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-49",
    "href": "lectures/lecture-05-content.html#section-49",
    "title": "Get Started",
    "section": "",
    "text": "Everyone who uses R, or any programming language for that matter, has trouble find them at some point. This is normal. Programming is hard. At some point code will not run or will throw an error. This happens to everyone. It is common to get frustrated, but to move forward we develop strategies to work through the issues:\n\nIf you are getting an error message, then sometimes it will be useful. Try to read it carefully to see if there is anything of use in it.\nTry to search for the error message. It can be useful to include “tidyverse” or “in R” in the search to help make the results more appropriate. Sometimes Stack Overflow results can be useful.\nLook at the help file for the function by putting “?” before the function, for instance, ?pivot_wider(). A common issue is to use a slightly incorrect argument name or format, such as accidentally including a string instead of an object name.\nLook at where the error is happening and remove or comment out code until the error is resolved, and then slowly add code back again.\nCheck the class of the object with class(), for instance, class(data_set$data_column). Ensure that it is what is expected.\nRestart R: “Session” \\(\\rightarrow\\) “Restart R and Clear Output”. Then load everything again.\nRestart your computer.\nSearch for what you are trying to do, rather than the error, being sure to include “tidyverse” or “in R” in the search to help make the results more appropriate. For instance, “save PDF of graph in R using ggplot”. Sometimes there are relevant blog posts or Stack Overflow answers that will help.\nMake a small, self-contained, reproducible example “reprex” to see if the issue can be isolated and to enable others to help.\nIf you are working in a Quarto doc then include label in the chunk options to make it easier to find where the mistake may be happening."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-50",
    "href": "lectures/lecture-05-content.html#section-50",
    "title": "Get Started",
    "section": "",
    "text": "More generally, while this is not always possible, it is almost always helpful to take a break and come back the next day."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-51",
    "href": "lectures/lecture-05-content.html#section-51",
    "title": "Get Started",
    "section": "",
    "text": "Reproducible examples\nAsking for help is a skill like any other. We get better at it with practice. It is important to try not to say “this doesn’t work”, “I tried everything”, “your code does not work”, or “here is the error message, what do I do?”. In general, it is not possible to help based on these comments, because there are too many possible issues. You need to make it easy for others to help you. This involves a few steps.\n\nProvide a small, self-contained example of your data, and code, and detail what is going wrong.\nDocument what you have tried so far, including which Stack Overflow and Posit Forum posts you looked at, and why they are not what you are after.\nBe clear about the outcome that you would like."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-52",
    "href": "lectures/lecture-05-content.html#section-52",
    "title": "Get Started",
    "section": "",
    "text": "Begin by creating a minimal REPRoducible EXample—a “reprex”. This is code that contains what is needed to reproduce the error, but only what is needed. This means that the code is likely a smaller, simpler version that nonetheless reproduces the error.\nSometimes this process enables one to solve the problem. If it does not, then it gives someone else a fighting chance of being able to help. There is almost no chance that you have got a problem that someone has not addressed before. It is more likely that the main difficulty is trying to communicate what you want to do and what is happening, in a way that allows others to recognize both. Developing tenacity is important.\nTo develop reproducible examples, reprex is especially useful. After installing it we:\n\nLoad the reprex package: library(reprex).\nHighlight and copy the code that is giving issues.\nRun reprex() in the console.\n\nIf the code is self-contained, then it will preview in the viewer. If it is not, then it will error, and you should rewrite the code so that it is self-contained."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-53",
    "href": "lectures/lecture-05-content.html#section-53",
    "title": "Get Started",
    "section": "",
    "text": "If you need data to reproduce the error, then you should use data that is built into R. There are a large number of datasets that are built into R and can be seen using library(help = \"datasets\"). But if possible, you should use a common option such as mtcars or faithful. Combining a reprex with a GitHub Gist that was introduced in ?@sec-fire-hose increases the chances that someone is able to help you."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-54",
    "href": "lectures/lecture-05-content.html#section-54",
    "title": "Get Started",
    "section": "",
    "text": "Mentality\n\n(Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you\n(L)et’s break down the gates, there’s enough room for everyone\nSharla Gelfand, 10 March 2020.\n\nIf you write code, then you are a programmer, regardless of how you do it, what you are using it for, or who you are. But there are a few traits that one tends to notice great programmers have in common."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-55",
    "href": "lectures/lecture-05-content.html#section-55",
    "title": "Get Started",
    "section": "",
    "text": "Focused: Often having an aim to “learn R” or similar tends to be problematic, because there is no real end point to that. It tends to be more efficient to have smaller, more specific goals, such as “make a histogram about the 2022 Australian Election with ggplot2”. This is something that can be focused on and achieved in a few hours. The issue with goals that are more nebulous, such as “I want to learn R”, is that it is easier to get lost on tangents and more difficult to get help. This can be demoralizing and lead to people quitting too early.\nCurious: It is almost always useful to “have a go”; that is, if you are not sure, then just try it. In general, the worst that happens is that you waste your time. You can rarely break something irreparably. For instance, if you want to know what happens if you pass a vector instead of a dataframe to ggplot() then try it.\nPragmatic: At the same time, it can be useful to stick within reasonable bounds, and make one small change each time. For instance, say you want to run some regressions, and are curious about the possibility of using rstanarm instead of lm(). A pragmatic way to proceed is to use one aspect from rstanarm initially and then make another change next time.\nTenacious: Again, this is a balancing act. Unexpected problems and issues arise with every project. On the one hand, persevering despite these is a good tendency. But on the other hand, sometimes one does need to be prepared to give up on something if it does not seem like a break through is possible. Mentors can be useful as they tend to be a better judge of what is reasonable.\nPlanned: It is almost always useful to excessively plan what you are going to do. For instance, you may want to make a histogram of some data. You should plan the steps that are needed and even sketch out how each step might be implemented. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is the back-up plan if the data do not exist there?\nDone is better than perfect: We all have various perfectionist tendencies, but it can be useful to initially try to turn them off to a certain extent. Initially just worry about writing code that works. You can always come back and improve aspects of it. But it is important to actually ship. Ugly code that gets the job done is better than beautiful code that is never finished."
  },
  {
    "objectID": "lectures/lecture-05-content.html#code-comments-and-style",
    "href": "lectures/lecture-05-content.html#code-comments-and-style",
    "title": "Get Started",
    "section": "Code comments and style",
    "text": "Code comments and style\nCode must be commented. Comments should focus on why certain code was written and to a lesser extent, why a common alternative was not selected. Indeed, it can be a good idea to write the comments before you write the code, explaining what you want to do and why, and then returning to write the code [@refactoringbook, p. 59].\nThere is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you are just working on your own. Most projects will evolve over time, and one purpose of code comments is to enable future-you to retrace what was done and why certain decisions were made [@bowers2016improve]."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-56",
    "href": "lectures/lecture-05-content.html#section-56",
    "title": "Get Started",
    "section": "",
    "text": "Comments in R scripts can be added by including the # symbol. (The behavior of # is different for lines inside an R chunk in a Quarto document where it acts as a comment, compared with lines outside an R chunk where it sets heading levels.) We do not have to put a comment at the start of the line, it can be midway through. In general, you do not need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if we read in some value then we may like to comment where it is coming from.\nYou should try to comment why you are doing something [@tidyversestyleguide]. What are you trying to achieve? You must comment to explain weird things. Like if you are removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you will not remember."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-57",
    "href": "lectures/lecture-05-content.html#section-57",
    "title": "Get Started",
    "section": "",
    "text": "You should break your code into sections. For instance, setting up the workspace, reading in datasets, manipulating and cleaning the datasets, analyzing the datasets, and finally producing tables and figures. Each of these should be separated with comments explaining what is going on, and sometimes into separate files, depending on the length."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-58",
    "href": "lectures/lecture-05-content.html#section-58",
    "title": "Get Started",
    "section": "",
    "text": "Additionally, at the top of each file it is important to note basic information, such as the purpose of the file, and prerequisites or dependencies, the date, the author and contact information, and finally any red flags or todos.\nYour R scripts should have a preamble and a clear demarcation of sections.\n#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Date: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep install.packages lines; comment out if need be\n# Load packages\nlibrary(tidyverse)\n\n# Read in the unedited data. \nraw_data &lt;- read_csv(\"inputs/data/unedited_data.csv\")\n\n\n#### Next section ####\n..."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-59",
    "href": "lectures/lecture-05-content.html#section-59",
    "title": "Get Started",
    "section": "",
    "text": "Finally, try not to rely on a user commenting and uncommenting code, or any other manual step, such as directory specification, for code to work. This will preclude the use of automated code checking and testing.\nThis all takes time. As a rough rule of thumb, you should expect to spend at least as much time commenting and improving your code as you spent writing it. Some examples of nicely commented code include @AhadyDolatsara2021 and @burton2021reconsidering."
  },
  {
    "objectID": "lectures/lecture-05-content.html#tests",
    "href": "lectures/lecture-05-content.html#tests",
    "title": "Get Started",
    "section": "Tests",
    "text": "Tests\nTests should be written throughout the code, and you need to write them as we go, not all at the end. This will slow you down. But it will help you to think, and to fix mistakes, which will make your code better and lead to better overall productivity. Code without tests should be viewed with suspicion. There is room for improvement when it comes to testing practices in R packages [@Vidoni2021], let alone R code more generally.\nThe need for other people, and ideally, automated processes, to run tests on code is one reason that we emphasize reproducibility. That is also why we emphasize smaller aspects such as not hardcoding file-paths, using projects, and not having spaces in file names."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-60",
    "href": "lectures/lecture-05-content.html#section-60",
    "title": "Get Started",
    "section": "",
    "text": "It is difficult to define a complete and general suite of tests, but broadly we want to test:\n\nboundary conditions,\nclasses,\nmissing data,\nthe number of observations and variables,\nduplicates, and\nregression results."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-61",
    "href": "lectures/lecture-05-content.html#section-61",
    "title": "Get Started",
    "section": "",
    "text": "We do all this initially on our simulated data and then move to the real data. The mirrors the evolution of testing during the Apollo Program. Initially testing occured based on expectations of requirements, and these tests were later updated to take into account actual launch measurements [@testingforsuccess, p. 21]. It is possible to write an infinite number of tests but a smaller number of high-quality tests is better than many thoughtless tests."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-62",
    "href": "lectures/lecture-05-content.html#section-62",
    "title": "Get Started",
    "section": "",
    "text": "One type of test is an “assertion”. Assertions are written throughout the code to check whether something is true and stop the code from running if not [@researchsoftware, p. 272]. For instance, you might assert that a variable should be numeric. If it was tested against this assertion and found to be a character, then the test would fail and the script would stop running. Assertion tests in data science will typically be used in data cleaning and preparation scripts. We have more to say about these in ?@sec-clean-and-prepare. Unit tests check some complete aspect of code [@researchsoftware, p. 274]. We will consider them more in ?@sec-its-just-a-linear-model when we consider modeling."
  },
  {
    "objectID": "lectures/lecture-05-content.html#efficiency",
    "href": "lectures/lecture-05-content.html#efficiency",
    "title": "Get Started",
    "section": "Efficiency",
    "text": "Efficiency\nGenerally in this book we are, and will continue to be, concerned with just getting something done. Not necessarily getting it done in the best or most efficient way, because to a large extent, being worried about that is a waste of time. For the most part one is better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But that eventually becomes unfeasible. At a certain point, and this differs depending on context, efficiency becomes important. Eventually ugly or slow code, and dogmatic insistence on a particular way of doing things, have an effect. And it is at that point that one needs to be open to new approaches to ensure efficiency. There is rarely a most common area for obvious performance gains. Instead, it is important to develop the ability to measure, evaluate, and think."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-63",
    "href": "lectures/lecture-05-content.html#section-63",
    "title": "Get Started",
    "section": "",
    "text": "One of the best ways to improve the efficiency of our code is preparing it in such a way that we can bring in a second pair of eyes. To make the most of their time, it is important that our code easy to read. So we start with “code linting” and “styling”. This does not speed up our code, per se, but instead makes it more efficient when another person comes to it, or we revisit it. This enables formal code review and refactoring, which is where we rewrite code to make it better, while not changing what it does (it does the same thing, but in a different way). We then turn to measurement of run time, and introduce parallel processing, where we allow our computer to run code for multiple processes at the same time"
  },
  {
    "objectID": "lectures/lecture-05-content.html#sharing-a-code-environment",
    "href": "lectures/lecture-05-content.html#sharing-a-code-environment",
    "title": "Get Started",
    "section": "Sharing a code environment",
    "text": "Sharing a code environment\nWe have discussed at length the need to share code, and we have put forward an approach to this using GitHub. And in ?@sec-store-and-share, we will discuss sharing data. But, there is another requirement to enable other people to run our code. In ?@sec-fire-hose we discussed how R itself, as well as R packages update from time to time, as new functionality is developed, errors fixed, and other general improvements made. ?@sec-r-essentials describes how one advantage of the tidyverse is that it can update faster than base R, because it is more specific. But this could mean that even if we were to share all the code and data that we use, it is possible that the software versions that have become available would cause errors."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-64",
    "href": "lectures/lecture-05-content.html#section-64",
    "title": "Get Started",
    "section": "",
    "text": "The solution to this is to detail the environment that was used. There are a large number of ways to do this, and they can add complexity. We just focus on documenting the version of R and R packages that were used, and making it easier for others to install that exact version. Essentially we are just isolating the set-up that we used because that will help with reproducibility [@perkel2023]. In R we can use renv to do this.\nOnce renv is installed and loaded, we use init() to get the infrastructure set-up that we will need. We are going to create a file that will record the packages and versions used. We then use snapshot() to actually document what we are using. This creates a “lockfile” that records the information."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-65",
    "href": "lectures/lecture-05-content.html#section-65",
    "title": "Get Started",
    "section": "",
    "text": "If we want to see which packages we are using in the R Project, then we can use dependencies(). Doing this for the example folder indicates that the following packages are used: rmarkdown, bookdown, knitr, rmarkdown, bookdown, knitr, palmerpenguins, tidyverse, renv, haven, readr, and tidyverse."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-66",
    "href": "lectures/lecture-05-content.html#section-66",
    "title": "Get Started",
    "section": "",
    "text": "We could open the lockfile file—“renv.lock”—to see the exact versions if we wanted. The lockfile also documents all the other packages that were installed and where they were downloaded from. Someone coming to this project from outside could then use restore() which would install the exact version of the packages that we used."
  },
  {
    "objectID": "lectures/lecture-05-content.html#code-linting-and-styling",
    "href": "lectures/lecture-05-content.html#code-linting-and-styling",
    "title": "Get Started",
    "section": "Code linting and styling",
    "text": "Code linting and styling\nBeing fast is valuable but it is mostly about being able to iterate fast, not necessarily having code that runs fast. @oldlanguages [p. 26] describes how even in 1954 a programmer cost at least as much as a computer, and these days additional computational power is usually much cheaper than a programmer. Performant code is important, but it is also important to use other people’s time efficiently. Code is rarely only written once. Instead we typically have to come back to it, even if to just fix mistakes, and this means that code must be able to be read by humans [@beautifulcode, p. 478]. If this is not done then there will be an efficiency cost.\nLinting and styling is the process of checking code, mostly for stylistic issues, and re-arranging code to make it easier to read. (There is another aspect of linting, which is dealing with programming errors, such as forgetting a closing bracket, but here we focus on stylistic issues.) Often the best efficiency gain comes from making it easier for others to read our code, even if this is just ourselves returning to the code after a break. Jane Street, a US proprietary trading firm, places a very strong focus on ensuring their code is readable, as a core part of risk mitigation [@minsky2011ocaml]. While we may not all have billions of dollars under the potentially mercurial management of code, we all would likely prefer that our code does not produce errors."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-67",
    "href": "lectures/lecture-05-content.html#section-67",
    "title": "Get Started",
    "section": "",
    "text": "We use lint() from lintr to lint our code. For instance, consider the following R code (saved as “linting_example.R”).\n\nSIMULATED_DATA &lt;-\n    tibble(\n        division = c(1:150, 151),\n        party = sample(\n            x = c(\"Liberal\"),\n            size = 151,\n            replace = T\n        )\n    )"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-68",
    "href": "lectures/lecture-05-content.html#section-68",
    "title": "Get Started",
    "section": "",
    "text": "lint(filename = \"linting_example.R\")"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-69",
    "href": "lectures/lecture-05-content.html#section-69",
    "title": "Get Started",
    "section": "",
    "text": "The result is that the file “linting_example.R” is opened and the issues that lint() found are printed in “Markers” (Figure 6). It is then up to you to deal with the issues.\n\n\n\n\n\n\nFigure 6: Linting results from example R code"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-70",
    "href": "lectures/lecture-05-content.html#section-70",
    "title": "Get Started",
    "section": "",
    "text": "Making the recommended changes results in code that is more readable, and consistent with best practice, as defined by @tidyversestyleguide.\n\nsimulated_data &lt;-\n    tibble(\n        division = c(1:150, 151),\n        party = sample(\n            x = c(\"Liberal\"),\n            size = 151,\n            replace = TRUE\n        )\n    )"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-71",
    "href": "lectures/lecture-05-content.html#section-71",
    "title": "Get Started",
    "section": "",
    "text": "At first it may seem that some aspects that the linter is identifying, like trailing whitespace and only using double quotes are small and inconsequential. But they distract from being able to fix bigger issues. Further, if we are not able to get small things right, then how could anyone trust that we could get the big things right? Therefore, it is important to have dealt with all the small aspects that a linter identifies."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-72",
    "href": "lectures/lecture-05-content.html#section-72",
    "title": "Get Started",
    "section": "",
    "text": "In addition to lintr we also use styler. This will automatically adjust style issues, in contrast to the linter, which gave a list of issues to look at. To run this we use style_file().\n\nstyle_file(path = \"linting_example.R\")\n\nThis will automatically make changes, such as spacing and indentation. As such this should be done regularly, rather than only once at the end of a project, so as to be able to review the changes and make sure no errors have been introduced."
  },
  {
    "objectID": "lectures/lecture-05-content.html#code-review",
    "href": "lectures/lecture-05-content.html#code-review",
    "title": "Get Started",
    "section": "Code review",
    "text": "Code review\nHaving dealt with all of these aspects of style, we can turn to code review. This is the process of having another person go through and critique the code. Many professional writers have editors, and code review is the closest that we come to that in data science. Code review is a critical part of writing code, and @researchsoftware [p. 465] describe it as “the most effective way to find bugs”. It is especially helpful, although quite daunting, when learning to code because getting feedback is a great way to improve."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-73",
    "href": "lectures/lecture-05-content.html#section-73",
    "title": "Get Started",
    "section": "",
    "text": "Go out of your way to be polite and collegial when reviewing another person’s code. Small aspects to do with style, things like spacing and separation, should have been taken care of by a linter and styler, but if not, then make a general recommendation about that. Most of your time as a code reviewer in data science should be spent on aspects such as:\n\nIs there an informative README and how could it be improved?\nAre the file names and variable names consistent, informative, and meaningful?\nDo the comments allow you to understand why something is being done?\nAre the tests both appropriate and sufficient? Are there edge cases or corner solutions that are not considered? Similarly, are there unnecessary tests that could be removed?\nAre there magic numbers that could be changed to variables and explained?\nIs there duplicated code that could be changed?\nAre there any outstanding warnings that should be addressed?\nAre there any especially large functions or pipes that could be separated into smaller ones?\nIs the structure of the project appropriate?\nCan we change any of the code to data [@researchsoftware, p. 462]?"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-74",
    "href": "lectures/lecture-05-content.html#section-74",
    "title": "Get Started",
    "section": "",
    "text": "For instance, consider some code that looked for the names of prime ministers and presidents. When we first wrote this code we likely added the relevant names directly into the code. But as part of code review, we might instead recommend that this be changed. We might recommend creating a small dataset of relevant names, and then re-writing the code to have it look up that dataset.\nCode review ensures that the code can be understood by at least one other person. This is a critical part of building knowledge about the world. At Google, code review is not primarily about finding defects, although that may happen, but is instead about ensuring readability and maintainability as well as education [@codereview]. This is also the case at Jane Street where they use code review to catch bugs, share institutional knowledge, assist with training, and oblige staff to write code that can be read [@ocamlyaronpodcast]."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-75",
    "href": "lectures/lecture-05-content.html#section-75",
    "title": "Get Started",
    "section": "",
    "text": "Finally, code review does not have to, and should not, be an onerous days-consuming process of reading all the code. The best code review is a quick review of just one file, focused on suggesting changes to just a handful of lines. Indeed, it may be better to have a review done by a small team of people rather than one individual. Do not review too much code at any one time. At most a few hundred lines, which should take around an hour, because any more than that has been found to be associated with reduced efficacy [@cohen2006best, p. 79]."
  },
  {
    "objectID": "lectures/lecture-05-content.html#code-refactoring",
    "href": "lectures/lecture-05-content.html#code-refactoring",
    "title": "Get Started",
    "section": "Code refactoring",
    "text": "Code refactoring\nTo refactor code means to rewrite it so that the new code achieves the same outcome as the old code, but the new code does it better. For instance, @refactornature discuss how the code underpinning an important UK Covid model was initially written by epidemiologists, and months later clarified and cleaned up by a team from the Royal Society, Microsoft, and GitHub. This was valuable because it provided more confidence in the model, even though both versions produced the same outputs, given the same inputs."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-76",
    "href": "lectures/lecture-05-content.html#section-76",
    "title": "Get Started",
    "section": "",
    "text": "We typically refer to code refactoring in relation to code that someone else wrote. (Although it may be that we actually wrote the code, and it was just that it was some time ago.) When we start to refactor code, we want to make sure that the rewritten code achieves the same outcomes as the original code. This means that we need a suite of appropriate tests written that we can depend on. If these do not exist, then we may need to create them.\nWe rewrite code to make it easier for others to understand, which in turn allows more confidence in our conclusions. But before we can do that, we need to understand what the existing code is doing. One way to get started is to go through the code and add extensive comments. These comments are different to normal comments. They are our active process of trying to understand what is each code chunk trying to do and how could this be improved."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-77",
    "href": "lectures/lecture-05-content.html#section-77",
    "title": "Get Started",
    "section": "",
    "text": "Refactoring code is an opportunity to ensure that it satisfies best practice. @Trisovic2022 details some core recommendations based on examining 9,000 R scripts including:\n\nRemove setwd() and any absolute paths, and ensure that only relative paths, in relation to the “.Rproj” file, are used.\nEnsure there is a clear order of execution. We have recommended using numbers in filenames to achieve this initially, but eventually more sophisticated approaches, such as targets [@targets], could be used instead.\nEnsure that code can run on a different computer."
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-78",
    "href": "lectures/lecture-05-content.html#section-78",
    "title": "Get Started",
    "section": "",
    "text": "For instance, consider the following code:\n\nsetwd(\"/Users/rohanalexander/Documents/telling_stories\")\n\nlibrary(tidyverse)\n\nd &lt;- read_csv(\"cars.csv\")\n\nmtcars &lt;-\n    mtcars |&gt;\n    mutate(K_P_L = mpg / 2.352)\n\nlibrary(datasauRus)\n\ndatasaurus_dozen"
  },
  {
    "objectID": "lectures/lecture-05-content.html#section-79",
    "href": "lectures/lecture-05-content.html#section-79",
    "title": "Get Started",
    "section": "",
    "text": "We could change that, starting by creating an R Project which enables us to remove setwd(), grouping all the library() calls at the top, using “&lt;-” instead of “=”, and being consistent with variable names:\n\nlibrary(tidyverse)\nlibrary(datasauRus)\n\ncars_data &lt;- read_csv(\"cars.csv\")\n\nmpg_to_kpl_conversion_factor &lt;- 2.352\n\nmtcars &lt;-\n    mtcars |&gt;\n    mutate(kpl = mpg / mpg_to_kpl_conversion_factor)"
  },
  {
    "objectID": "lectures/lecture-05-content.html#concluding-remarks",
    "href": "lectures/lecture-05-content.html#concluding-remarks",
    "title": "Get Started",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nIn this chapter we have considered much and it is normal to be overwhelmed. Come back to the Quarto section as needed. Many people are confused by Git and GitHub and just know enough to get by. And while there was a lot of material in efficiency, the most important aspect of performant code is making it easier for another person to read it, even if that person is just yourself returning after a break."
  },
  {
    "objectID": "lectures/lecture-05-content.html#references",
    "href": "lectures/lecture-05-content.html#references",
    "title": "Get Started",
    "section": "References",
    "text": "References"
  }
]