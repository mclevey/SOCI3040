[
  {
    "objectID": "syllabus/conventions.html",
    "href": "syllabus/conventions.html",
    "title": " Conventions",
    "section": "",
    "text": "üöß Under Construction\n\n\n\n I am currently developing these course materials. Please check back in January 2025.  - John",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#callout-boxes",
    "href": "syllabus/conventions.html#callout-boxes",
    "title": " Conventions",
    "section": "Callout Boxes",
    "text": "Callout Boxes",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#flowcharts",
    "href": "syllabus/conventions.html#flowcharts",
    "title": " Conventions",
    "section": "Flowcharts",
    "text": "Flowcharts",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#mathematical-symbols",
    "href": "syllabus/conventions.html#mathematical-symbols",
    "title": " Conventions",
    "section": "Mathematical Symbols",
    "text": "Mathematical Symbols",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/conventions.html#references",
    "href": "syllabus/conventions.html#references",
    "title": " Conventions",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Conventions"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": " Quantitative Research Methods",
    "section": "",
    "text": "Course Instructor John McLevey (he/him) Professor, Department of Sociology Memorial University\n\n¬† john.mclevey@uwaterloo.ca Note: I do not check or respond to email in the evenings or on weekends.\n\n\nWhere is class? CP-2003 (Chemistry-Physics, Computer Lab) When is class? Tuesdays & Thursdays, 1:30 - 2:50 pm Office Hours: A4054, Tuesdays & Thursdays, 3:00 - 4:00 pm\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander‚Äôs (2023) Telling Stories with Data, you will learn how to:\nYou will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear models, generalized linear models, and multilevel models. Reproducibility and research ethics are considered throughout the workflow, and the entire course.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#alexander2023telling-telling-stories-with-data",
    "href": "syllabus/syllabus.html#alexander2023telling-telling-stories-with-data",
    "title": " Quantitative Research Methods",
    "section": " Alexander (2023) Telling Stories with Data",
    "text": "Alexander (2023) Telling Stories with Data\n\n‚ÄúThis is not another statistics book. It is much better than that. It is a book about doing quantitative research, about scientific justification, about quality control, about communication and epistemic humility. It‚Äôs a valuable supplement to any methods curriculum, and useful for self-learners as well.‚Äù ‚Äì Richard McElreath\n\n\n\n\n\n‚ÄúThis clean and fun book covers a wide range of topics on statistical communication, programming, and modeling in a way that should be a useful supplement to any statistics course or self-learning program. I absolutely love this book!‚Äù ‚Äì Andrew Gelman\n\n\n‚ÄúEvery data analyst has to tell stories with data, and yet traditional textbooks focus on statistical methods alone. Telling Stories with Data teaches the entire data science workflow, including data acquisition, communication, and reproducibility. I highly recommend this unique book!‚Äù ‚Äì Kosuke Imai\n\n\n‚ÄúTelling (true) Stories with Data requires more than fancy statistical models and big data. With a series of fascinating case studies, Rohan Alexander teaches us how to ask good questions, acquire data, estimate models, and communicate our results. This holistic approach is explained with crisp and engaging prose. The pages are filled with detailed R examples, which emphasize the importance of transparency and reproducibility. I absolutely love this book and recommend it to all my students.‚Äù ‚Äì Vincent Arel-Bundock\n\n\n‚ÄúAn excellent book. Communication and reproducibility are of increasing concern in statistics, and this book covers these topics and more in a practical, appealing, and truly unique way.‚Äù ‚Äì Daniela Witten",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#healy2019data-data-visualization",
    "href": "syllabus/syllabus.html#healy2019data-data-visualization",
    "title": " Quantitative Research Methods",
    "section": " Healy (2019) Data Visualization",
    "text": "Healy (2019) Data Visualization\n\n‚ÄúFinally! A data visualization guide that is simultaneously practical and elegant. ‚Ä¶ Data Visualization is brimming with insights into how quantitative analysts can use visualization as a tool for understanding and communication. A must-read for anyone who works with data.‚Äù ‚Äì Elizabeth Bruch\n\n\n\n\n\n‚ÄúHealy‚Äôs fun and readable book is unusual in covering the ‚Äòwhy do‚Äô as well as the ‚Äòhow to‚Äô of data visualization, demonstrating how dataviz is a key step in all stages of social science ‚Äì from theory construction to measurement to modeling and interpretation of analyses‚Äïand giving readers the tools to integrate visualization into their own work.‚Äù ‚Äì Andrew Gelman\n\n\n‚ÄúData Visualization is a brilliant book that not only teaches the reader how to visualize data but also carefully considers why data visualization is essential for good social science. The book is broadly relevant, beautifully rendered, and engagingly written. It is easily accessible for students at any level and will be an incredible teaching resource for courses on research methods, statistics, and data visualization. It is packed full of clear-headed and sage insights.‚Äù ‚Äì Becky Pettit\n\n\n‚ÄúKieran Healy has written a wonderful book that fills an important niche in an increasingly crowded landscape of materials about software in R. Data Visualization is clear, beautifully formatted, and full of careful insights.‚Äù ‚Äì Brandon Stewart",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;SOCI 3040"
    ]
  },
  {
    "objectID": "lectures/lecture-02-notes.html",
    "href": "lectures/lecture-02-notes.html",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#the-firehose",
    "href": "lectures/lecture-02-notes.html#the-firehose",
    "title": "Telling Stories with Data",
    "section": "the firehose",
    "text": "the firehose\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander‚Äôs (2023) Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#the-firehose-1",
    "href": "lectures/lecture-02-notes.html#the-firehose-1",
    "title": "Telling Stories with Data",
    "section": "the firehose",
    "text": "the firehose\n\nWhenever you‚Äôre learning a new tool, for a long time, you‚Äôre going to suck\\(\\dots\\) But the good news is that is typical; that‚Äôs something that happens to everyone, and it‚Äôs only temporary.\nHadley Wickham as quoted by Barrett (2021)\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan Alexander (2023)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#preliminaries-1",
    "href": "lectures/lecture-02-notes.html#preliminaries-1",
    "title": "Telling Stories with Data",
    "section": "preliminaries",
    "text": "preliminaries\n\nRStudio / CodeSpaces / Whatever‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#preliminaries-2",
    "href": "lectures/lecture-02-notes.html#preliminaries-2",
    "title": "Telling Stories with Data",
    "section": "preliminaries",
    "text": "preliminaries\n\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#import-libraries",
    "href": "lectures/lecture-02-notes.html#import-libraries",
    "title": "Telling Stories with Data",
    "section": "import libraries",
    "text": "import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan-1",
    "href": "lectures/lecture-02-notes.html#plan-1",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\nAustralian Elections\n\n\n\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan-2",
    "href": "lectures/lecture-02-notes.html#plan-2",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure¬†1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a ‚Äúdivision‚Äù in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-1",
    "href": "lectures/lecture-02-notes.html#simulate-1",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-2",
    "href": "lectures/lecture-02-notes.html#simulate-2",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\nWe‚Äôll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-3",
    "href": "lectures/lecture-02-notes.html#simulate-3",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we‚Äôre creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named ‚ÄúDivision‚Äù. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it‚Äôs used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options ‚Äî ‚ÄúLiberal‚Äù, ‚ÄúLabor‚Äù, ‚ÄúNational‚Äù, ‚ÄúGreen‚Äù, and ‚ÄúOther‚Äù ‚Äî into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the ‚ÄúDivision‚Äù column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we‚Äôre picking ‚Äúwith replacement‚Äù (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn‚Äôt match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-4",
    "href": "lectures/lecture-02-notes.html#simulate-4",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\nü§ò We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 √ó 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 National\n 2        2 Liberal \n 3        3 Green   \n 4        4 Other   \n 5        5 Liberal \n 6        6 Labor   \n 7        7 Green   \n 8        8 Liberal \n 9        9 Other   \n10       10 Green   \n# ‚Ñπ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-1",
    "href": "lectures/lecture-02-notes.html#acquire-1",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-2",
    "href": "lectures/lecture-02-notes.html#acquire-2",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nWe‚Äôll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief ‚ÄúOde to the here package,‚Äù ‚Äúhere here,‚Äù which you can read‚Ä¶ here."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-3",
    "href": "lectures/lecture-02-notes.html#acquire-3",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\nü§ò We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 √ó 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA‚Ä¶\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather‚Ä¶ KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge‚Ä¶ ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE‚Ä¶\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ‚Ñπ 141 more rows\n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-4",
    "href": "lectures/lecture-02-notes.html#acquire-4",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 √ó 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA‚Ä¶\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri‚Ä¶ KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-5",
    "href": "lectures/lecture-02-notes.html#acquire-5",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 √ó 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma‚Ä¶ STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO‚Ä¶\n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-6",
    "href": "lectures/lecture-02-notes.html#acquire-6",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n‚ÄúWe are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.‚Äù (Alexander 2023)\n\n\nLet‚Äôs clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-7",
    "href": "lectures/lecture-02-notes.html#acquire-7",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let‚Äôs look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ‚Ñπ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-8",
    "href": "lectures/lecture-02-notes.html#acquire-8",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n\n\n‚úåÔ∏è R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-9",
    "href": "lectures/lecture-02-notes.html#acquire-9",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-10",
    "href": "lectures/lecture-02-notes.html#acquire-10",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet‚Äôs rename."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-11",
    "href": "lectures/lecture-02-notes.html#acquire-11",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-12",
    "href": "lectures/lecture-02-notes.html#acquire-12",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let‚Äôs simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-13",
    "href": "lectures/lecture-02-notes.html#acquire-13",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-14",
    "href": "lectures/lecture-02-notes.html#acquire-14",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! üòé"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#aus_elections_clean_path",
    "href": "lectures/lecture-02-notes.html#aus_elections_clean_path",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nLet‚Äôs save the cleaned data so that we can start with it data in the next stage. We‚Äôll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-1",
    "href": "lectures/lecture-02-notes.html#explore-understand-1",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-2",
    "href": "lectures/lecture-02-notes.html#explore-understand-2",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\n\n\n\nI‚Äôm using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won‚Äôt work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-3",
    "href": "lectures/lecture-02-notes.html#explore-understand-3",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\nüòé"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-4",
    "href": "lectures/lecture-02-notes.html#explore-understand-4",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 √ó 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-5",
    "href": "lectures/lecture-02-notes.html#explore-understand-5",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nRemember, we‚Äôre trying to make something like this.\n\n\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\n\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer‚Äôs understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer‚Äôs eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-6",
    "href": "lectures/lecture-02-notes.html#explore-understand-6",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\nLet‚Äôs visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it‚Äôs cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\n\n\n\n\n\n\nFigure¬†2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-7",
    "href": "lectures/lecture-02-notes.html#explore-understand-7",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels\n\n\n\n\n\n\n\nFigure¬†3: Number of seats won, by political party, at the 2022 Australian Federal Election. üòé"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section",
    "href": "lectures/lecture-02-notes.html#section",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure¬†4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#share-1",
    "href": "lectures/lecture-02-notes.html#share-1",
    "title": "Telling Stories with Data",
    "section": " share",
    "text": "share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties‚Äî‚ÄúLiberal‚Äù and ‚ÄúLabor‚Äù‚Äîtwo minor parties‚Äî‚ÄúNationals‚Äù and ‚ÄúGreens‚Äù‚Äîand many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure¬†3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan-4",
    "href": "lectures/lecture-02-notes.html#plan-4",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure¬†5 (a) (next slide).\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure¬†5 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan-5",
    "href": "lectures/lecture-02-notes.html#plan-5",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a table\n\n\n\n\n\n\n\nFigure¬†5: Sketches of a dataset and table of the average number of beds occupied each month for shelters in Toronto."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-6",
    "href": "lectures/lecture-02-notes.html#simulate-6",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process. When we turn to analysis, it will provide us with a guide. Conducting analysis without first using simulation can be thought of as shooting arrows without a target‚Äîwhile you are certainly doing something, it is not clear whether you are doing it well."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-7",
    "href": "lectures/lecture-02-notes.html#simulate-7",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            ) # Draw 1,095 times from the Poisson distribution\n    )\n\nsimulated_occupancy_data\n\n# A tibble: 1,095 √ó 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ‚Ñπ 1,085 more rows\n\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it later in the course."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-16",
    "href": "lectures/lecture-02-notes.html#acquire-16",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    # Each package is associated with a unique id  found in the \"For\n    # Developers\" tab of the relevant page from Open Data Toronto\n    # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    # Within that package, we are interested in the 2021 dataset\n    filter(name ==\n        \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    # Having reduced the dataset to one row we can get the resource\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-17",
    "href": "lectures/lecture-02-notes.html#acquire-17",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 √ó 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME    \n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                \n1     1 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n2     2 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n3     3 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n4     4 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n5     5 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n6     6 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n# ‚Ñπ 28 more variables: SHELTER_ID &lt;dbl&gt;,\n#   SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;,\n#   LOCATION_POSTAL_CODE &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-18",
    "href": "lectures/lecture-02-notes.html#acquire-18",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nWe‚Äôll change the names to make them easier to type using clean_names(), and select() the relevant columns.\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 √ó 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#acquire-19",
    "href": "lectures/lecture-02-notes.html#acquire-19",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nAll that remains for this step is to save the cleaned dataset.\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\nWHERE ARE THESE NAs COMING FROM?"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-8",
    "href": "lectures/lecture-02-notes.html#explore-understand-8",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\ntoronto_shelters_clean\n\n# A tibble: 50,944 √ó 2\n   occupancy_date occupied_beds\n   &lt;date&gt;                 &lt;dbl&gt;\n 1 2021-01-01                NA\n 2 2021-01-01                NA\n 3 2021-01-01                NA\n 4 2021-01-01                NA\n 5 2021-01-01                NA\n 6 2021-01-01                 6\n 7 2021-01-01                NA\n 8 2021-01-01                NA\n 9 2021-01-01                NA\n10 2021-01-01                NA\n# ‚Ñπ 50,934 more rows"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-9",
    "href": "lectures/lecture-02-notes.html#explore-understand-9",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\nTable¬†1: Shelter usage in Toronto in 2021\n\n\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426\n\n\n\n\n\n\n\n\n\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments‚Äî‚Äúlabel‚Äù and ‚Äúabbr‚Äù‚Äîto get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in sec-farm-data and sec-exploratory-data-analysis. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create tbl-homelessoccupancyd."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#explore-understand-10",
    "href": "lectures/lecture-02-notes.html#explore-understand-10",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable(\n        col.names = c(\"Month\", \"Average daily number of&lt;br&gt;occupied beds (per shelter)\"),\n        digits = 1\n    )\n\n\n\n\nMonth\nAverage daily number ofoccupied beds (per shelter)\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5\n\n\n\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (tbl-homelessoccupancy). In particular we make the column names easier to read, and only show an appropriate number of decimal places."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#share-3",
    "href": "lectures/lecture-02-notes.html#share-3",
    "title": "Telling Stories with Data",
    "section": " share",
    "text": "share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R (R Core Team 2023) as well as the tidyverse (Wickham 2017), janitor (Firke 2023), opendatatoronto (Gelfand 2022), lubridate (Grolemund and Wickham 2011), and knitr (Xie 2023). We then made a table of the average number of occupied beds each night for each month (tbl-homelessoccupancy).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (tbl-homelessoccupancy). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\n\n\n\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of Hao (2019), that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan-7",
    "href": "lectures/lecture-02-notes.html#plan-7",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure¬†6 (a) (next slide). We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure¬†6 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#plan-8",
    "href": "lectures/lecture-02-notes.html#plan-8",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\n\n\n\nFigure¬†6: Sketches of a dataset and graph about the neonatal mortality rate (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-9",
    "href": "lectures/lecture-02-notes.html#simulate-9",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nTo simulate some data that aligns with our plan, we will need three columns: country, year, and NMR. We can do this by repeating the name of each country 50 times with rep(), and enabling the passing of 50 years. Then we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-10",
    "href": "lectures/lecture-02-notes.html#simulate-10",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", 50), rep(\"Australia\", 50),\n                rep(\"Canada\", 50), rep(\"Kenya\", 50)\n            ),\n        year =\n            rep(c(1971:2020), 4),\n        nmr =\n            runif(n = 200, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-11",
    "href": "lectures/lecture-02-notes.html#simulate-11",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-12",
    "href": "lectures/lecture-02-notes.html#simulate-12",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n                rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)\n            ),\n        year =\n            rep(c(1:number_of_years + 1970), 4),\n        nmr =\n            runif(n = number_of_years * 4, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-13",
    "href": "lectures/lecture-02-notes.html#simulate-13",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat ‚Äúcountry‚Äù is exclusively one of these four: ‚ÄúArgentina‚Äù, ‚ÄúAustralia‚Äù, ‚ÄúCanada‚Äù, or ‚ÄúKenya‚Äù.\nConversely, ‚Äúcountry‚Äù contains all those four countries.\nThat ‚Äúyear‚Äù is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat ‚Äúnmr‚Äù is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-14",
    "href": "lectures/lecture-02-notes.html#simulate-14",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\nsimulated_nmr_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#simulate-15",
    "href": "lectures/lecture-02-notes.html#simulate-15",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-1",
    "href": "lectures/lecture-02-notes.html#section-1",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "The UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\nigme_data_path &lt;- here(\"data\", \"igme.csv\")\nigme_data_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/igme.csv\"\n\n\n\nraw_igme_data &lt;-\n    read_csv(\n        file =\n            \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n        show_col_types = FALSE\n    )\n\nwrite_csv(x = raw_igme_data, file = igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-2",
    "href": "lectures/lecture-02-notes.html#section-2",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "raw_igme_data &lt;-\n    read_csv(\n        file = igme_data_path,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-3",
    "href": "lectures/lecture-02-notes.html#section-3",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "With established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail()\n\nhead(raw_igme_data)\n\n# A tibble: 6 √ó 29\n  `Geographic area` Indicator        Sex   `Wealth Quintile`\n  &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n2 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n3 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n4 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n5 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n6 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n# ‚Ñπ 25 more variables: `Series Name` &lt;chr&gt;,\n#   `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;,\n#   COUNTRY_NOTES &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-4",
    "href": "lectures/lecture-02-notes.html#section-4",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "and what the names of the columns are with names()\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\""
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-5",
    "href": "lectures/lecture-02-notes.html#section-5",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where ‚ÄúSex‚Äù is ‚ÄúTotal‚Äù, ‚ÄúSeries Name‚Äù is ‚ÄúUN IGME estimate‚Äù, ‚ÄúGeographic area‚Äù is one of ‚ÄúArgentina‚Äù, ‚ÄúAustralia‚Äù, ‚ÄúCanada‚Äù, and ‚ÄúKenya‚Äù, and the ‚ÄúIndicator‚Äù is ‚ÄúNeonatal mortality rate‚Äù. After this we are interested in just a few columns: ‚Äúgeographic_area‚Äù, ‚Äútime_period‚Äù, and ‚Äúobs_value‚Äù."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-6",
    "href": "lectures/lecture-02-notes.html#section-6",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    clean_names(raw_igme_data) |&gt;\n    filter(\n        sex == \"Total\",\n        series_name == \"UN IGME estimate\",\n        geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n        indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n    select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 √ó 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-7",
    "href": "lectures/lecture-02-notes.html#section-7",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We need to fix two other aspects: the class of ‚Äútime_period‚Äù is character when we need it to be a year, and the name of ‚Äúobs_value‚Äù should be ‚Äúnmr‚Äù to be more informative."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-8",
    "href": "lectures/lecture-02-notes.html#section-8",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    cleaned_igme_data |&gt;\n    mutate(\n        time_period = str_remove(time_period, \"-06\"),\n        time_period = as.integer(time_period)\n    ) |&gt;\n    filter(time_period &gt;= 1971) |&gt;\n    rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-9",
    "href": "lectures/lecture-02-notes.html#section-9",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "Finally, we can check that our dataset passes the tests that we developed based on the simulated dataset."
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-10",
    "href": "lectures/lecture-02-notes.html#section-10",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_igme_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-11",
    "href": "lectures/lecture-02-notes.html#section-11",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "All that remains is to save the nicely cleaned dataset.\n\ncleaned_igme_data_path &lt;- here(\"data\", \"cleaned_igme_data.csv\")\nwrite_csv(x = cleaned_igme_data, file = cleaned_igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-12",
    "href": "lectures/lecture-02-notes.html#section-12",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\ncleaned_igme_data &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_igme_data.csv\"),\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#section-13",
    "href": "lectures/lecture-02-notes.html#section-13",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We can now make a graph of how NMR has changed over time and the differences between countries (Figure¬†7).\n\n\ncleaned_igme_data |&gt;\n    ggplot(aes(x = year, y = nmr, color = country)) +\n    geom_point() +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure¬†7: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)"
  },
  {
    "objectID": "lectures/lecture-02-notes.html#share-5",
    "href": "lectures/lecture-02-notes.html#share-5",
    "title": "Telling Stories with Data",
    "section": " share",
    "text": "share\nExample taken directly from Alexander (2023), here.\n\n\n\n\n\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries‚ÄîArgentina, Australia, Canada, and Kenya‚Äîover the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023).\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure¬†7). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  },
  {
    "objectID": "lectures/lecture-01-content.html",
    "href": "lectures/lecture-01-content.html",
    "title": "üî• Quantitative Research Methods",
    "section": "",
    "text": "This is an introduction the knowledge and skills you need to tell credible stories with quantitative data‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-01-content.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-content.html#welcome-to-soci-3040",
    "title": "üî• Quantitative Research Methods",
    "section": "",
    "text": "This is an introduction the knowledge and skills you need to tell credible stories with quantitative data‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-01-content.html#why-data-storytelling-matters",
    "href": "lectures/lecture-01-content.html#why-data-storytelling-matters",
    "title": "üî• Quantitative Research Methods",
    "section": "Why Data Storytelling Matters",
    "text": "Why Data Storytelling Matters\n\n‚ÄúA lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.‚Äù [@alexander2023telling]\n\n\nCore foundation of quantitative research methods\nBridge between analysis and understanding\nEssential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-content.html#common-concerns",
    "href": "lectures/lecture-01-content.html#common-concerns",
    "title": "üî• Quantitative Research Methods",
    "section": "Common Concerns",
    "text": "Common Concerns\nFive Key Questions for Data Stories\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What‚Äôs missing?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data?\n\n\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-content.html#core-workflow-components",
    "href": "lectures/lecture-01-content.html#core-workflow-components",
    "title": "üî• Quantitative Research Methods",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\n\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure¬†1: Rohan Alexander‚Äôs [-@alexander2023telling] workflow for telling stories with data.\n\n\n\n\n\n\nPlan and sketch endpoint\nSimulate and consider data\nAcquire and prepare data\nExplore and understand data\nShare findings"
  },
  {
    "objectID": "lectures/lecture-01-content.html#plan-and-sketch",
    "href": "lectures/lecture-01-content.html#plan-and-sketch",
    "title": "üî• Quantitative Research Methods",
    "section": " Plan and Sketch",
    "text": "Plan and Sketch\n\n\n\ndeliberate, reasoned decisions\npurposeful adjustments\neven 10 minutes of planning is valuable\n\n\n\n\n\nThink of Alice‚Äôs conversation with the Cheshire Cat üò∏. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.\n\n\n\n\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice‚Äôs conversation with the Cheshire Cat in Alice‚Äôs Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn‚Äôt require extensive time‚Äîoften just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-content.html#simulate-data",
    "href": "lectures/lecture-01-content.html#simulate-data",
    "title": "üî• Quantitative Research Methods",
    "section": " Simulate Data",
    "text": "Simulate Data\n\nForces detailed thinking\nClarifies expected data structure and distributions.\nHelps with cleaning and preparation\nIdentifies potential issues beforehand.\nProvides clear testing framework\nEnsures data meets expectations.\n‚ÄúAlmost free‚Äù with modern computing\nProvides ‚Äúan intimate feeling for the situation‚Äù [@hamming1996]\n\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides ‚Äúan intimate feeling for the situation‚Äù and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-content.html#acquire-and-prepare",
    "href": "lectures/lecture-01-content.html#acquire-and-prepare",
    "title": "üî• Quantitative Research Methods",
    "section": " Acquire and Prepare",
    "text": "Acquire and Prepare\n\nOften overlooked but crucial stage\nMany difficult decisions required: data sources, formats, permissions.\nCan significantly affect statistical results [@huntington2021influence]\nCommon challenges: quantity (too little or too much data) and quality\n\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed‚Äîeither by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-content.html#explore-and-understand",
    "href": "lectures/lecture-01-content.html#explore-and-understand",
    "title": "üî• Quantitative Research Methods",
    "section": " Explore and Understand",
    "text": "Explore and Understand\n\nBegin with descriptive statistics\nMove to statistical models\nRemember: Models are tools, not truth\nModels reflect:\n\nEarlier decisions\nData acquisition choices\nCleaning procedures\n\n\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It‚Äôs important to remember that statistical models are tools‚Äînot absolute truths‚Äîand they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-content.html#share-findings",
    "href": "lectures/lecture-01-content.html#share-findings",
    "title": "üî• Quantitative Research Methods",
    "section": " Share Findings",
    "text": "Share Findings\n\nHigh-fidelity communication essential\nDocument all decisions\nBuild credibility through transparency\n\nInclude:\n\nWhat was done\nWhy it was done\nWhat was found\nWeaknesses of the approach\n\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it‚Äôs essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process‚Äîfrom data acquisition to analysis‚Äîbuilds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-content.html#the-foundation",
    "href": "lectures/lecture-01-content.html#the-foundation",
    "title": "üî• Quantitative Research Methods",
    "section": "The Foundation",
    "text": "The Foundation\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-content.html#communication-most-important",
    "href": "lectures/lecture-01-content.html#communication-most-important",
    "title": "üî• Quantitative Research Methods",
    "section": " Communication (Most Important)",
    "text": "Communication (Most Important)\n\n‚ÄúSimple analysis, communicated well, is more valuable than complicated analysis communicated poorly.‚Äù [@alexander2023telling]\n\n\n‚ÄúOne challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.‚Äù [@alexander2023telling]\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience‚Äôs perspective"
  },
  {
    "objectID": "lectures/lecture-01-content.html#reproducibility",
    "href": "lectures/lecture-01-content.html#reproducibility",
    "title": "üî• Quantitative Research Methods",
    "section": " Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\nRequirements:\n\nOpen access to code\nData availability or simulation\nAutomated testing\nClear documentation\nAim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-content.html#ethics",
    "href": "lectures/lecture-01-content.html#ethics",
    "title": "üî• Quantitative Research Methods",
    "section": " Ethics",
    "text": "Ethics\n\n‚ÄúThis means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?‚Äù [@alexander2023telling]\n\n\nConsider the full context of the dataset [@datafeminism2020]\nAcknowledge the social, cultural, and political forces [@crawford]\nUse data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-content.html#questions",
    "href": "lectures/lecture-01-content.html#questions",
    "title": "üî• Quantitative Research Methods",
    "section": " Questions",
    "text": "Questions\n\nQuestions evolve through understanding\nChallenge of operationalizing variables\nCuriosity is essential, drives deeper exploration\nValue of ‚Äúhybrid‚Äù knowledge that combines multiple disciplines\nComfort with asking ‚Äúdumb‚Äù questions\n\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a ‚Äúhybrid‚Äù‚Äîcultivating knowledge across various disciplines‚Äîand being comfortable with asking seemingly simple or ‚Äúdumb‚Äù questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-content.html#measurement",
    "href": "lectures/lecture-01-content.html#measurement",
    "title": "üî• Quantitative Research Methods",
    "section": " Measurement",
    "text": "Measurement\n\n‚ÄúThe world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.‚Äù [@alexander2023telling]\n\n\n\nMeasuring even simple things is challenging\nExample: Measuring height\n\nShoes on or off?\nTime of day affects height.\nDifferent tools yield different results.\n\nMore complex measurements are even harder. How do we measure happiness or pain?\n\nMeasurement requires decisions and is not value-free\nContext and purpose guide all measurement choices\n\n\n\n\n\nPicasso‚Äôs dog and the challenges of reduction.\n\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person‚Äôs height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso‚Äôs minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we‚Äôre measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-content.html#data-collection-cleaning",
    "href": "lectures/lecture-01-content.html#data-collection-cleaning",
    "title": "üî• Quantitative Research Methods",
    "section": " &  Data Collection & Cleaning",
    "text": "&  Data Collection & Cleaning\n\n‚ÄúData never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.‚Äù [@alexander2023telling]\n\n\nCollection determines possibilities\n\nWhat and how we measure matters.\n\nCleaning requires many decisions\n\nExample: Survey responses on gender\nOptions: ‚Äúman‚Äù, ‚Äúwoman‚Äù, ‚Äúprefer not to say‚Äù, ‚Äúother‚Äù\nHandling ‚Äúprefer not to say‚Äù and open-text responses.\n\nDocument every step\n\nEnsures transparency and reproducibility.\n\nConsider implications of choices\n\nEthical considerations and representation.\n\n\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like ‚Äúman,‚Äù ‚Äúwoman,‚Äù ‚Äúprefer not to say,‚Äù and ‚Äúother‚Äù (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude ‚Äúprefer not to say‚Äù responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents‚Äô specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it‚Äôs vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-content.html#eda-modeling-scaling",
    "href": "lectures/lecture-01-content.html#eda-modeling-scaling",
    "title": "üî• Quantitative Research Methods",
    "section": "+ EDA, Modeling, & Scaling",
    "text": "+ EDA, Modeling, & Scaling\n\nExploratory Data Analysis (EDA)\n\nIterative process\nNever truly complete\nShapes understanding\n\n\n\nModeling\n\nTool for understanding\nNot a recipe to follow\nJust one representation of reality\nStatistical significance \\(\\neq\\) scientific significance\nStatistical models help us explore the shape of the data; are like echolocation\n\n\n\nScaling\n\nUsing programming languages like R and Python\n\nHandle large datasets efficiently\nAutomate repetitive tasks\nShare work widely and quickly\n\nOutputs can reach many people easily\nAPIs can make analyses accessible in real-time\n\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there‚Äôs always more to explore. Although it‚Äôs challenging to delineate where EDA ends and formal statistical modeling begins‚Äîsince our beliefs and understanding evolve continuously‚ÄîEDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it‚Äôs crucial that all steps taken during EDA are recorded and shared.\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it‚Äôs a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference‚Äîthe process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it‚Äôs essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly‚Äîwhether it‚Äôs one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data",
    "title": "üî• Quantitative Research Methods",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\n‚ÄúThere is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.‚Äù [@hamming1996, 177]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-1",
    "title": "üî• Quantitative Research Methods",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world‚Äîit is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of ‚Äúplausibly measurable‚Äù. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. [@alexander2023telling]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-content.html#how-do-our-worlds-become-data-2",
    "title": "üî• Quantitative Research Methods",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction üë®‚Äçüç≥\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data‚Äîplausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it‚Äôs crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment‚Äîmuch like a chef monitoring a reduction to achieve the desired consistency without overcooking‚Äîto determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors‚Äîfrom selecting what to measure to deciding on the methods and accuracy required. Advances in instruments‚Äîfrom telescopes in astronomy to real-time internet data collection‚Äîhave expanded our ability to gather data, much like new culinary techniques enhance a chef‚Äôs ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our ‚Äúreduction‚Äù process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-content.html#what-is-data-science",
    "href": "lectures/lecture-01-content.html#what-is-data-science",
    "title": "üî• Quantitative Research Methods",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n‚ÄúData science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict.‚Äù [@alexander2023telling]\n\nKey Principles\n\nData are generated, and must be gathered, cleaned, and prepared\nThese decisions matter\nThe process will be difficult\nDevelop resilience and intrinsic motivation"
  },
  {
    "objectID": "lectures/lecture-01-content.html#the-power-of-multiple-perspectives",
    "href": "lectures/lecture-01-content.html#the-power-of-multiple-perspectives",
    "title": "üî• Quantitative Research Methods",
    "section": "The Power of Multiple Perspectives",
    "text": "The Power of Multiple Perspectives\n\n‚ÄúThe strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past.‚Äù [@alexander2023telling]\n\n\nData science is multi-disciplinary\nCombines statistics, software engineering, subject-matter expertise, and more.\nDiversity enhances understanding\nDifferent perspectives lead to better questions and solutions.\nCollaboration is key\nRespect and integrate insights from various fields."
  },
  {
    "objectID": "lectures/lecture-01-content.html#embracing-the-challenge",
    "href": "lectures/lecture-01-content.html#embracing-the-challenge",
    "title": "üî• Quantitative Research Methods",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\nOur world is messy, and so are our data. Telling stories with data is difficult but rewarding.\n\nDevelop resilience and intrinsic motivation\nAccept that failure is part of the process.\nConsider possibilities and probabilities\nLearn to make trade-offs.\nNo perfect analysis exists\nAim for transparency and continuous improvement.\n\n\n‚ÄúUltimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.‚Äù [@alexander2023telling]"
  },
  {
    "objectID": "lectures/lecture-01-content.html#key-takeaways",
    "href": "lectures/lecture-01-content.html#key-takeaways",
    "title": "üî• Quantitative Research Methods",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-25-notes.html",
    "href": "lectures/lecture-25-notes.html",
    "title": "Project Work",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#coming-soon",
    "href": "lectures/lecture-25-notes.html#coming-soon",
    "title": "Project Work",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-25-notes.html#references",
    "href": "lectures/lecture-25-notes.html#references",
    "title": "Project Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-23-notes.html",
    "href": "lectures/lecture-23-notes.html",
    "title": "Text as Data",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#coming-soon",
    "href": "lectures/lecture-23-notes.html#coming-soon",
    "title": "Text as Data",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-23-notes.html#references",
    "href": "lectures/lecture-23-notes.html#references",
    "title": "Text as Data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-21-notes.html",
    "href": "lectures/lecture-21-notes.html",
    "title": "Mutilevel Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#coming-soon",
    "href": "lectures/lecture-21-notes.html#coming-soon",
    "title": "Mutilevel Models",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-21-notes.html#references",
    "href": "lectures/lecture-21-notes.html#references",
    "title": "Mutilevel Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-19-notes.html",
    "href": "lectures/lecture-19-notes.html",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#coming-soon",
    "href": "lectures/lecture-19-notes.html#coming-soon",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-19-notes.html#references",
    "href": "lectures/lecture-19-notes.html#references",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-17-notes.html",
    "href": "lectures/lecture-17-notes.html",
    "title": "Linear Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#coming-soon",
    "href": "lectures/lecture-17-notes.html#coming-soon",
    "title": "Linear Models",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-17-notes.html#references",
    "href": "lectures/lecture-17-notes.html#references",
    "title": "Linear Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-15-notes.html",
    "href": "lectures/lecture-15-notes.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#coming-soon",
    "href": "lectures/lecture-15-notes.html#coming-soon",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-15-notes.html#references",
    "href": "lectures/lecture-15-notes.html#references",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-13-notes.html",
    "href": "lectures/lecture-13-notes.html",
    "title": "Preparing Data ‚Äì Cleaning, Testing, Validating, and Documenting",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#coming-soon",
    "href": "lectures/lecture-13-notes.html#coming-soon",
    "title": "Preparing Data ‚Äì Cleaning, Testing, Validating, and Documenting",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-13-notes.html#references",
    "href": "lectures/lecture-13-notes.html#references",
    "title": "Preparing Data ‚Äì Cleaning, Testing, Validating, and Documenting",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-11-notes.html",
    "href": "lectures/lecture-11-notes.html",
    "title": "Measurement",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#coming-soon",
    "href": "lectures/lecture-11-notes.html#coming-soon",
    "title": "Measurement",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-11-notes.html#references",
    "href": "lectures/lecture-11-notes.html#references",
    "title": "Measurement",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-09-notes.html",
    "href": "lectures/lecture-09-notes.html",
    "title": "Showing the Right Numbers",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#coming-soon",
    "href": "lectures/lecture-09-notes.html#coming-soon",
    "title": "Showing the Right Numbers",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-09-notes.html#references",
    "href": "lectures/lecture-09-notes.html#references",
    "title": "Showing the Right Numbers",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-07-notes.html",
    "href": "lectures/lecture-07-notes.html",
    "title": "Writing and Developing Research Questions",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#coming-soon",
    "href": "lectures/lecture-07-notes.html#coming-soon",
    "title": "Writing and Developing Research Questions",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-07-notes.html#references",
    "href": "lectures/lecture-07-notes.html#references",
    "title": "Writing and Developing Research Questions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-05-notes.html",
    "href": "lectures/lecture-05-notes.html",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#coming-soon",
    "href": "lectures/lecture-05-notes.html#coming-soon",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-05-notes.html#references",
    "href": "lectures/lecture-05-notes.html#references",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-03-notes.html",
    "href": "lectures/lecture-03-notes.html",
    "title": "Workflow ‚Äì Plan, Simulate, Acquire, Explore, Share",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#coming-soon",
    "href": "lectures/lecture-03-notes.html#coming-soon",
    "title": "Workflow ‚Äì Plan, Simulate, Acquire, Explore, Share",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-03-notes.html#references",
    "href": "lectures/lecture-03-notes.html#references",
    "title": "Workflow ‚Äì Plan, Simulate, Acquire, Explore, Share",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "cheatsheets/workflow.html",
    "href": "cheatsheets/workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Workflow refers to a structured, step-by-step process for conducting research from start (e.g., planning) to finish (e.g., communicating). Formal workflows offer significant benefits, such as making it easier to conduct trustworthy and reproducible research. In Telling Stories with Data, Rohan Alexander (2023) introduces a straightforward workflow that begins with planning the analysis, simulating data, acquiring real data, and then exploring and analyzing it, and finally sharing the results. This workflow, shown in Figure¬†1, encourages us to think critically at each stage of the process, and to focus on reproducibility, research ethics, and effective communication.\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure¬†1: Rohan Alexander‚Äôs (2023) workflow from Telling Stories with Data."
  },
  {
    "objectID": "cheatsheets/workflow.html#plan",
    "href": "cheatsheets/workflow.html#plan",
    "title": "Workflow",
    "section": "1. Plan",
    "text": "1. Plan"
  },
  {
    "objectID": "cheatsheets/workflow.html#simulate",
    "href": "cheatsheets/workflow.html#simulate",
    "title": "Workflow",
    "section": "2. Simulate",
    "text": "2. Simulate"
  },
  {
    "objectID": "cheatsheets/workflow.html#acquire",
    "href": "cheatsheets/workflow.html#acquire",
    "title": "Workflow",
    "section": "3. Acquire",
    "text": "3. Acquire"
  },
  {
    "objectID": "cheatsheets/workflow.html#explore-analyze",
    "href": "cheatsheets/workflow.html#explore-analyze",
    "title": "Workflow",
    "section": "4. Explore / Analyze",
    "text": "4. Explore / Analyze"
  },
  {
    "objectID": "cheatsheets/workflow.html#share",
    "href": "cheatsheets/workflow.html#share",
    "title": "Workflow",
    "section": "5. Share",
    "text": "5. Share"
  },
  {
    "objectID": "syllabus/notation.html",
    "href": "syllabus/notation.html",
    "title": " Model Notation",
    "section": "",
    "text": "üöß Under Construction\n\n\n\n I am currently developing these course materials. Please check back in January 2025.  - John",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Model Notation"
    ]
  },
  {
    "objectID": "lectures/lecture-24-slides.html#coming-soon",
    "href": "lectures/lecture-24-slides.html#coming-soon",
    "title": "Project Work",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-24-slides.html#references",
    "href": "lectures/lecture-24-slides.html#references",
    "title": "Project Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-24-slides.html#references-1",
    "href": "lectures/lecture-24-slides.html#references-1",
    "title": "Project Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#coming-soon",
    "href": "lectures/lecture-22-slides.html#coming-soon",
    "title": "Mutilevel Models",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#references",
    "href": "lectures/lecture-22-slides.html#references",
    "title": "Mutilevel Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-22-slides.html#references-1",
    "href": "lectures/lecture-22-slides.html#references-1",
    "title": "Mutilevel Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#coming-soon",
    "href": "lectures/lecture-20-slides.html#coming-soon",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#references",
    "href": "lectures/lecture-20-slides.html#references",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-20-slides.html#references-1",
    "href": "lectures/lecture-20-slides.html#references-1",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#coming-soon",
    "href": "lectures/lecture-18-slides.html#coming-soon",
    "title": "Linear Models + Model-based Graphics",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#references",
    "href": "lectures/lecture-18-slides.html#references",
    "title": "Linear Models + Model-based Graphics",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-18-slides.html#references-1",
    "href": "lectures/lecture-18-slides.html#references-1",
    "title": "Linear Models + Model-based Graphics",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#coming-soon",
    "href": "lectures/lecture-16-slides.html#coming-soon",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#references",
    "href": "lectures/lecture-16-slides.html#references",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-16-slides.html#references-1",
    "href": "lectures/lecture-16-slides.html#references-1",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#coming-soon",
    "href": "lectures/lecture-14-slides.html#coming-soon",
    "title": "Surveys",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#references",
    "href": "lectures/lecture-14-slides.html#references",
    "title": "Surveys",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-14-slides.html#references-1",
    "href": "lectures/lecture-14-slides.html#references-1",
    "title": "Surveys",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#coming-soon",
    "href": "lectures/lecture-12-slides.html#coming-soon",
    "title": "Sampling",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#references",
    "href": "lectures/lecture-12-slides.html#references",
    "title": "Sampling",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-12-slides.html#references-1",
    "href": "lectures/lecture-12-slides.html#references-1",
    "title": "Sampling",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-10-slides.html#coming-soon",
    "href": "lectures/lecture-10-slides.html#coming-soon",
    "title": "Piping, Summarizing, and Transforming",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-10-slides.html#references",
    "href": "lectures/lecture-10-slides.html#references",
    "title": "Piping, Summarizing, and Transforming",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-10-slides.html#references-1",
    "href": "lectures/lecture-10-slides.html#references-1",
    "title": "Piping, Summarizing, and Transforming",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-08-slides.html#coming-soon",
    "href": "lectures/lecture-08-slides.html#coming-soon",
    "title": "The Grammar of Graphics, Plotting in the Tidyverse",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-08-slides.html#references",
    "href": "lectures/lecture-08-slides.html#references",
    "title": "The Grammar of Graphics, Plotting in the Tidyverse",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-08-slides.html#references-1",
    "href": "lectures/lecture-08-slides.html#references-1",
    "title": "The Grammar of Graphics, Plotting in the Tidyverse",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-06-slides.html#coming-soon",
    "href": "lectures/lecture-06-slides.html#coming-soon",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-06-slides.html#references",
    "href": "lectures/lecture-06-slides.html#references",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-06-slides.html#references-1",
    "href": "lectures/lecture-06-slides.html#references-1",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#coming-soon",
    "href": "lectures/lecture-04-slides.html#coming-soon",
    "title": "Good and Bad Ways to Look at Data",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#references",
    "href": "lectures/lecture-04-slides.html#references",
    "title": "Good and Bad Ways to Look at Data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-04-slides.html#references-1",
    "href": "lectures/lecture-04-slides.html#references-1",
    "title": "Good and Bad Ways to Look at Data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-slides.html#welcome-to-soci-3040",
    "title": "Introduction",
    "section": "üëã Welcome to SOCI 3040!",
    "text": "üëã Welcome to SOCI 3040!\n\nThis is an introduction the knowledge and skills you need to tell credible stories with quantitative data‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#why-data-storytelling-matters",
    "href": "lectures/lecture-01-slides.html#why-data-storytelling-matters",
    "title": "Introduction",
    "section": "Why Data Storytelling Matters",
    "text": "Why Data Storytelling Matters\n\n‚ÄúA lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.‚Äù (Alexander 2023)\n\n\nCore foundation of quantitative research methods\nBridge between analysis and understanding\nEssential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#common-concerns",
    "href": "lectures/lecture-01-slides.html#common-concerns",
    "title": "Introduction",
    "section": "Common Concerns",
    "text": "Common Concerns\nFive Key Questions for Data Stories\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What‚Äôs missing?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data?\n\n\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#core-workflow-components",
    "href": "lectures/lecture-01-slides.html#core-workflow-components",
    "title": "Introduction",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\n\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure¬†1: Rohan Alexander‚Äôs (2023) workflow for telling stories with data.\n\n\n\n\n\n\nPlan and sketch endpoint\nSimulate and consider data\nAcquire and prepare data\nExplore and understand data\nShare findings"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#plan-and-sketch",
    "href": "lectures/lecture-01-slides.html#plan-and-sketch",
    "title": "Introduction",
    "section": " Plan and Sketch",
    "text": "Plan and Sketch\n\n\n\ndeliberate, reasoned decisions\npurposeful adjustments\neven 10 minutes of planning is valuable\n\n\n\n\n\nThink of Alice‚Äôs conversation with the Cheshire Cat üò∏. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.\n\n\n\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice‚Äôs conversation with the Cheshire Cat in Alice‚Äôs Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn‚Äôt require extensive time‚Äîoften just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#simulate-data",
    "href": "lectures/lecture-01-slides.html#simulate-data",
    "title": "Introduction",
    "section": " Simulate Data",
    "text": "Simulate Data\n\nForces detailed thinking\nClarifies expected data structure and distributions.\nHelps with cleaning and preparation\nIdentifies potential issues beforehand.\nProvides clear testing framework\nEnsures data meets expectations.\n‚ÄúAlmost free‚Äù with modern computing\nProvides ‚Äúan intimate feeling for the situation‚Äù (Hamming [1997] 2020)\n\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides ‚Äúan intimate feeling for the situation‚Äù and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#acquire-and-prepare",
    "href": "lectures/lecture-01-slides.html#acquire-and-prepare",
    "title": "Introduction",
    "section": " Acquire and Prepare",
    "text": "Acquire and Prepare\n\nOften overlooked but crucial stage\nMany difficult decisions required: data sources, formats, permissions.\nCan significantly affect statistical results (Huntington-Klein et al. 2021)\nCommon challenges: quantity (too little or too much data) and quality\n\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed‚Äîeither by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#explore-and-understand",
    "href": "lectures/lecture-01-slides.html#explore-and-understand",
    "title": "Introduction",
    "section": " Explore and Understand",
    "text": "Explore and Understand\n\nBegin with descriptive statistics\nMove to statistical models\nRemember: Models are tools, not truth\nModels reflect:\n\nEarlier decisions\nData acquisition choices\nCleaning procedures\n\n\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It‚Äôs important to remember that statistical models are tools‚Äînot absolute truths‚Äîand they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#share-findings",
    "href": "lectures/lecture-01-slides.html#share-findings",
    "title": "Introduction",
    "section": " Share Findings",
    "text": "Share Findings\n\nHigh-fidelity communication essential\nDocument all decisions\nBuild credibility through transparency\n\nInclude:\n\nWhat was done\nWhy it was done\nWhat was found\nWeaknesses of the approach\n\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it‚Äôs essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process‚Äîfrom data acquisition to analysis‚Äîbuilds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#the-foundation",
    "href": "lectures/lecture-01-slides.html#the-foundation",
    "title": "Introduction",
    "section": "The Foundation",
    "text": "The Foundation\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#communication-most-important",
    "href": "lectures/lecture-01-slides.html#communication-most-important",
    "title": "Introduction",
    "section": " Communication (Most Important)",
    "text": "Communication (Most Important)\n\n‚ÄúSimple analysis, communicated well, is more valuable than complicated analysis communicated poorly.‚Äù (Alexander 2023)\n\n\n‚ÄúOne challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.‚Äù (Alexander 2023)\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience‚Äôs perspective"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#reproducibility",
    "href": "lectures/lecture-01-slides.html#reproducibility",
    "title": "Introduction",
    "section": " Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\nRequirements:\n\nOpen access to code\nData availability or simulation\nAutomated testing\nClear documentation\nAim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#ethics",
    "href": "lectures/lecture-01-slides.html#ethics",
    "title": "Introduction",
    "section": " Ethics",
    "text": "Ethics\n\n‚ÄúThis means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?‚Äù (Alexander 2023)\n\n\nConsider the full context of the dataset (D‚ÄôIgnazio and Klein 2020)\nAcknowledge the social, cultural, and political forces (Crawford 2021)\nUse data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#questions",
    "href": "lectures/lecture-01-slides.html#questions",
    "title": "Introduction",
    "section": " Questions",
    "text": "Questions\n\nQuestions evolve through understanding\nChallenge of operationalizing variables\nCuriosity is essential, drives deeper exploration\nValue of ‚Äúhybrid‚Äù knowledge that combines multiple disciplines\nComfort with asking ‚Äúdumb‚Äù questions\n\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a ‚Äúhybrid‚Äù‚Äîcultivating knowledge across various disciplines‚Äîand being comfortable with asking seemingly simple or ‚Äúdumb‚Äù questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#measurement",
    "href": "lectures/lecture-01-slides.html#measurement",
    "title": "Introduction",
    "section": " Measurement",
    "text": "Measurement\n\n‚ÄúThe world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.‚Äù (Alexander 2023)\n\n\n\nMeasuring even simple things is challenging\nExample: Measuring height\n\nShoes on or off?\nTime of day affects height.\nDifferent tools yield different results.\n\nMore complex measurements are even harder. How do we measure happiness or pain?\n\nMeasurement requires decisions and is not value-free\nContext and purpose guide all measurement choices\n\n\n\n\n\nPicasso‚Äôs dog and the challenges of reduction.\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person‚Äôs height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso‚Äôs minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we‚Äôre measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#data-collection-cleaning",
    "href": "lectures/lecture-01-slides.html#data-collection-cleaning",
    "title": "Introduction",
    "section": " &  Data Collection & Cleaning",
    "text": "&  Data Collection & Cleaning\n\n‚ÄúData never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.‚Äù (Alexander 2023)\n\n\nCollection determines possibilities\n\nWhat and how we measure matters.\n\nCleaning requires many decisions\n\nExample: Survey responses on gender\nOptions: ‚Äúman‚Äù, ‚Äúwoman‚Äù, ‚Äúprefer not to say‚Äù, ‚Äúother‚Äù\nHandling ‚Äúprefer not to say‚Äù and open-text responses.\n\nDocument every step\n\nEnsures transparency and reproducibility.\n\nConsider implications of choices\n\nEthical considerations and representation.\n\n\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like ‚Äúman,‚Äù ‚Äúwoman,‚Äù ‚Äúprefer not to say,‚Äù and ‚Äúother‚Äù (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude ‚Äúprefer not to say‚Äù responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents‚Äô specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it‚Äôs vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#eda-modeling-scaling",
    "href": "lectures/lecture-01-slides.html#eda-modeling-scaling",
    "title": "Introduction",
    "section": "+ EDA, Modeling, & Scaling",
    "text": "+ EDA, Modeling, & Scaling\nExploratory Data Analysis (EDA)\n\nIterative process\nNever truly complete\nShapes understanding\n\nModeling\n\nTool for understanding\nNot a recipe to follow\nJust one representation of reality\nStatistical significance \\(\\neq\\) scientific significance\nStatistical models help us explore the shape of the data; are like echolocation\n\nScaling\n\nUsing programming languages like R and Python\n\nHandle large datasets efficiently\nAutomate repetitive tasks\nShare work widely and quickly\n\nOutputs can reach many people easily\nAPIs can make analyses accessible in real-time\n\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there‚Äôs always more to explore. Although it‚Äôs challenging to delineate where EDA ends and formal statistical modeling begins‚Äîsince our beliefs and understanding evolve continuously‚ÄîEDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it‚Äôs crucial that all steps taken during EDA are recorded and shared.\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it‚Äôs a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference‚Äîthe process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it‚Äôs essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly‚Äîwhether it‚Äôs one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data",
    "title": "Introduction",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\n‚ÄúThere is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.‚Äù (Hamming [1997] 2020, 177)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-1",
    "title": "Introduction",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world‚Äîit is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of ‚Äúplausibly measurable‚Äù. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-slides.html#how-do-our-worlds-become-data-2",
    "title": "Introduction",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction üë®‚Äçüç≥\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data‚Äîplausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it‚Äôs crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment‚Äîmuch like a chef monitoring a reduction to achieve the desired consistency without overcooking‚Äîto determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors‚Äîfrom selecting what to measure to deciding on the methods and accuracy required. Advances in instruments‚Äîfrom telescopes in astronomy to real-time internet data collection‚Äîhave expanded our ability to gather data, much like new culinary techniques enhance a chef‚Äôs ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our ‚Äúreduction‚Äù process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#what-is-data-science",
    "href": "lectures/lecture-01-slides.html#what-is-data-science",
    "title": "Introduction",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n‚ÄúData science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict.‚Äù (Alexander 2023)\n\nKey Principles\n\nData are generated, and must be gathered, cleaned, and prepared\nThese decisions matter\nThe process will be difficult\nDevelop resilience and intrinsic motivation"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#the-power-of-multiple-perspectives",
    "href": "lectures/lecture-01-slides.html#the-power-of-multiple-perspectives",
    "title": "Introduction",
    "section": "The Power of Multiple Perspectives",
    "text": "The Power of Multiple Perspectives\n\n‚ÄúThe strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past.‚Äù (Alexander 2023)\n\n\nData science is multi-disciplinary\nCombines statistics, software engineering, subject-matter expertise, and more.\nDiversity enhances understanding\nDifferent perspectives lead to better questions and solutions.\nCollaboration is key\nRespect and integrate insights from various fields."
  },
  {
    "objectID": "lectures/lecture-01-slides.html#embracing-the-challenge",
    "href": "lectures/lecture-01-slides.html#embracing-the-challenge",
    "title": "Introduction",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\nOur world is messy, and so are our data. Telling stories with data is difficult but rewarding.\n\nDevelop resilience and intrinsic motivation\nAccept that failure is part of the process.\nConsider possibilities and probabilities\nLearn to make trade-offs.\nNo perfect analysis exists\nAim for transparency and continuous improvement.\n\n\n‚ÄúUltimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.‚Äù (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#key-takeaways",
    "href": "lectures/lecture-01-slides.html#key-takeaways",
    "title": "Introduction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-01-slides.html#references",
    "href": "lectures/lecture-01-slides.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nD‚ÄôIgnazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. ‚ÄúThe Influence of Hidden Researcher Decisions in Applied Microeconomics.‚Äù Economic Inquiry 59: 944‚Äì60. https://doi.org/10.1111/ecin.12992."
  },
  {
    "objectID": "lectures/lecture-24-content.html#references",
    "href": "lectures/lecture-24-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-22-content.html#references",
    "href": "lectures/lecture-22-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-20-content.html#references",
    "href": "lectures/lecture-20-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-18-content.html#references",
    "href": "lectures/lecture-18-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-16-content.html#references",
    "href": "lectures/lecture-16-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-14-content.html#references",
    "href": "lectures/lecture-14-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-12-content.html#references",
    "href": "lectures/lecture-12-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-10-content.html#references",
    "href": "lectures/lecture-10-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-08-content.html#references",
    "href": "lectures/lecture-08-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-06-content.html#references",
    "href": "lectures/lecture-06-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-04-content.html#references",
    "href": "lectures/lecture-04-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus/schedule.html",
    "href": "syllabus/schedule.html",
    "title": " Course Schedule",
    "section": "",
    "text": "I am still developing these course materials. This schedule of topics may change at any time prior to the first day of class (Jan.¬†7, 2025).\n\n\n\n\n\n\n\n\n\nNo.\n\n\nClass Date\n\n\nClass & Lab Notes + Slides\n\n\nReading\n\n\nDue by 5:00 pm\n\n\n\n\n\n\n01\n\n\nTuesday, January 7, 2025\n\n\nIntroduction\n\n\nThe syllabus; Browse this site\n\n\n¬†\n\n\n\n\n02\n\n\nThursday, January 9, 2025\n\n\nTelling Stories with Data\n\n\n\n\n\n¬†\n\n\n\n\n03\n\n\nTuesday, January 14, 2025\n\n\nWorkflow ‚Äì Plan, Simulate, Acquire, Explore, Share\n\n\n\n\n\n¬†\n\n\n\n\n04\n\n\nThursday, January 16, 2025\n\n\nGood and Bad Ways to Look at Data\n\n\n\n\n\n¬†\n\n\n\n\n05\n\n\nTuesday, January 21, 2025\n\n\nReproducibility + Getting Started with R and RStudio\n\n\n\n\n\n¬†\n\n\n\n\n06\n\n\nThursday, January 23, 2025\n\n\nReproducibility + Getting Started with R and RStudio\n\n\n\n\n\nReport 1 (Solo)\n\n\n\n\n07\n\n\nTuesday, January 28, 2025\n\n\nWriting and Developing Research Questions\n\n\n\n\n\n¬†\n\n\n\n\n08\n\n\nThursday, January 30, 2025\n\n\nThe Grammar of Graphics, Plotting in the Tidyverse\n\n\n\n\n\n¬†\n\n\n\n\n09\n\n\nTuesday, February 4, 2025\n\n\nShowing the Right Numbers\n\n\n\n\n\n¬†\n\n\n\n\n10\n\n\nThursday, February 6, 2025\n\n\nPiping, Summarizing, and Transforming\n\n\n\n\n\n¬†\n\n\n\n\n11\n\n\nTuesday, February 11, 2025\n\n\nMeasurement\n\n\n\n\n\n¬†\n\n\n\n\n12\n\n\nThursday, February 13, 2025\n\n\nSampling\n\n\n\n\n\n¬†\n\n\n\n\n13\n\n\nTuesday, February 18, 2025\n\n\nPreparing Data ‚Äì Cleaning, Testing, Validating, and Documenting\n\n\n\n\n\n¬†\n\n\n\n\n14\n\n\nThursday, February 20, 2025\n\n\nPreparing Data ‚Äì Cleaning, Testing, Validating, and Documenting\n\n\n\n\n\nReport 2 (Solo)\n\n\n\n\n15\n\n\nTuesday, March 4, 2025\n\n\nExploratory Data Analysis (EDA)\n\n\n\n\n\n¬†\n\n\n\n\n16\n\n\nThursday, March 6, 2025\n\n\nExploratory Data Analysis (EDA)\n\n\n\n\n\n¬†\n\n\n\n\n17\n\n\nTuesday, March 11, 2025\n\n\nLinear Models\n\n\n\n\n\n¬†\n\n\n\n\n18\n\n\nThursday, March 13, 2025\n\n\nLinear Models + Model-based Graphics\n\n\n and \n\n\n¬†\n\n\n\n\n19\n\n\nTuesday, March 18, 2025\n\n\nGeneralized Linear Models (Binary Outcomes)\n\n\n\n\n\n¬†\n\n\n\n\n20\n\n\nThursday, March 20, 2025\n\n\nGeneralized Linear Models (Count Outcomes)\n\n\n\n\n\nReport 3 (Co-authored)\n\n\n\n\n21\n\n\nTuesday, March 25, 2025\n\n\nMutilevel Models\n\n\n (Something else)\n\n\n¬†\n\n\n\n\n22\n\n\nThursday, March 27, 2025\n\n\nMutilevel Models\n\n\n (Something else)\n\n\n¬†\n\n\n\n\n23\n\n\nTuesday, April 1, 2025\n\n\nText as Data\n\n\n\n\n\n¬†\n\n\n\n\n24\n\n\nThursday, April 3, 2025\n\n\nText as Data\n\n\n\n\n\n¬†\n\n\n\n\n25\n\n\nTuesday, April 8, 2025\n\n\nProject Work\n\n\n¬†\n\n\nReport 4 (Co-authored)\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Course Schedule"
    ]
  },
  {
    "objectID": "lectures/list-lectures.html",
    "href": "lectures/list-lectures.html",
    "title": " Class & Lab Notes",
    "section": "",
    "text": "Introduction\n\n\n\nTuesday, January 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTelling Stories with Data\n\n\n\nThursday, January 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow ‚Äì Plan, Simulate, Acquire, Explore, Share\n\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGood and Bad Ways to Look at Data\n\n\n\nThursday, January 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility + Getting Started with R and RStudio\n\n\n\nTuesday, January 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility + Getting Started with R and RStudio\n\n\n\nThursday, January 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWriting and Developing Research Questions\n\n\n\nTuesday, January 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Grammar of Graphics, Plotting in the Tidyverse\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nShowing the Right Numbers\n\n\n\nTuesday, February 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPiping, Summarizing, and Transforming\n\n\n\nThursday, February 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement\n\n\n\nTuesday, February 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSampling\n\n\n\nThursday, February 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing Data ‚Äì Cleaning, Testing, Validating, and Documenting\n\n\n\nTuesday, February 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing Data ‚Äì Cleaning, Testing, Validating, and Documenting\n\n\n\nThursday, February 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nTuesday, March 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nThursday, March 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models\n\n\n\nTuesday, March 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models + Model-based Graphics\n\n\n\nThursday, March 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models (Binary Outcomes)\n\n\n\nTuesday, March 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models (Count Outcomes)\n\n\n\nThursday, March 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMutilevel Models\n\n\n\nTuesday, March 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMutilevel Models\n\n\n\nThursday, March 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nText as Data\n\n\n\nTuesday, April 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nText as Data\n\n\n\nThursday, April 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProject Work\n\n\n\nTuesday, April 8, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "deliverables/assignment-4.html",
    "href": "deliverables/assignment-4.html",
    "title": "Report 4 (Co-authored)",
    "section": "",
    "text": "Instructions\nComing soon‚Ä¶\n\n\nGrading Rubric\nComing soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Report 4 (Co-authored)"
    ]
  },
  {
    "objectID": "deliverables/assignment-2.html",
    "href": "deliverables/assignment-2.html",
    "title": "Report 2 (Solo)",
    "section": "",
    "text": "Instructions\nComing soon‚Ä¶\n\n\nGrading Rubric\nComing soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Report 2 (Solo)"
    ]
  },
  {
    "objectID": "cheatsheets/diagrams.html",
    "href": "cheatsheets/diagrams.html",
    "title": "Diagrams",
    "section": "",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "deliverables/assignment-1.html",
    "href": "deliverables/assignment-1.html",
    "title": "Report 1 (Solo)",
    "section": "",
    "text": "Coming soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Report 1 (Solo)"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html#instructions",
    "href": "deliverables/assignment-1.html#instructions",
    "title": "Report 1 (Solo)",
    "section": "",
    "text": "Coming soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Report 1 (Solo)"
    ]
  },
  {
    "objectID": "deliverables/assignment-1.html#grading-rubric",
    "href": "deliverables/assignment-1.html#grading-rubric",
    "title": "Report 1 (Solo)",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nComing soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Report 1 (Solo)"
    ]
  },
  {
    "objectID": "deliverables/assignment-3.html",
    "href": "deliverables/assignment-3.html",
    "title": "Report 3 (Co-authored)",
    "section": "",
    "text": "Instructions\nComing soon‚Ä¶\n\n\nGrading Rubric\nComing soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Report 3 (Co-authored)"
    ]
  },
  {
    "objectID": "deliverables/assignment-5.html",
    "href": "deliverables/assignment-5.html",
    "title": "Contributions to the Course Blog",
    "section": "",
    "text": "Instructions\nComing soon‚Ä¶\n\n\nGrading Rubric\nComing soon‚Ä¶",
    "crumbs": [
      "Syllabus",
      "<strong>DELIVERABLES</strong>",
      "&nbsp;&nbsp;Blog Posts and Comments"
    ]
  },
  {
    "objectID": "syllabus/computing.html",
    "href": "syllabus/computing.html",
    "title": " Computing",
    "section": "",
    "text": "R and Quarto üòÅ ‚Äì No Experience Necessary  You will learn to use R (a free/open source programming language and environment for statistical computing and graphics) and Quarto (a free/open source publishing system for creating dynamic and reproducible manuscripts, reports, websites, and presentations) in this course. Note that I assume no prior experience with R or Quarto. Everything you need to know about both will be introduced in the course. While some prior programming experience is an asset, it is by no means necessary.",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#cp-2003",
    "href": "syllabus/computing.html#cp-2003",
    "title": " Computing",
    "section": "CP-2003",
    "text": "CP-2003",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#remote-access",
    "href": "syllabus/computing.html#remote-access",
    "title": " Computing",
    "section": "Remote Access",
    "text": "Remote Access",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#macos",
    "href": "syllabus/computing.html#macos",
    "title": " Computing",
    "section": "macOS",
    "text": "macOS",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#windows",
    "href": "syllabus/computing.html#windows",
    "title": " Computing",
    "section": "Windows",
    "text": "Windows",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#linux",
    "href": "syllabus/computing.html#linux",
    "title": " Computing",
    "section": "Linux",
    "text": "Linux",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#github-codespaces",
    "href": "syllabus/computing.html#github-codespaces",
    "title": " Computing",
    "section": "GitHub CodeSpaces",
    "text": "GitHub CodeSpaces",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "syllabus/computing.html#posit-cloud",
    "href": "syllabus/computing.html#posit-cloud",
    "title": " Computing",
    "section": "Posit Cloud",
    "text": "Posit Cloud",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Computing"
    ]
  },
  {
    "objectID": "lectures/lecture-03-content.html#references",
    "href": "lectures/lecture-03-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-05-content.html#references",
    "href": "lectures/lecture-05-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-07-content.html#references",
    "href": "lectures/lecture-07-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-09-content.html#references",
    "href": "lectures/lecture-09-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-11-content.html#references",
    "href": "lectures/lecture-11-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-13-content.html#references",
    "href": "lectures/lecture-13-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-15-content.html#references",
    "href": "lectures/lecture-15-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-17-content.html#references",
    "href": "lectures/lecture-17-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-19-content.html#references",
    "href": "lectures/lecture-19-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-21-content.html#references",
    "href": "lectures/lecture-21-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-23-content.html#references",
    "href": "lectures/lecture-23-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-25-content.html#references",
    "href": "lectures/lecture-25-content.html#references",
    "title": "üî• Quantitative Research Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#coming-soon",
    "href": "lectures/lecture-03-slides.html#coming-soon",
    "title": "Workflow ‚Äì Plan, Simulate, Acquire, Explore, Share",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#references",
    "href": "lectures/lecture-03-slides.html#references",
    "title": "Workflow ‚Äì Plan, Simulate, Acquire, Explore, Share",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-03-slides.html#references-1",
    "href": "lectures/lecture-03-slides.html#references-1",
    "title": "Workflow ‚Äì Plan, Simulate, Acquire, Explore, Share",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#coming-soon",
    "href": "lectures/lecture-05-slides.html#coming-soon",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#references",
    "href": "lectures/lecture-05-slides.html#references",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-05-slides.html#references-1",
    "href": "lectures/lecture-05-slides.html#references-1",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#coming-soon",
    "href": "lectures/lecture-07-slides.html#coming-soon",
    "title": "Writing and Developing Research Questions",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#references",
    "href": "lectures/lecture-07-slides.html#references",
    "title": "Writing and Developing Research Questions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-07-slides.html#references-1",
    "href": "lectures/lecture-07-slides.html#references-1",
    "title": "Writing and Developing Research Questions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#coming-soon",
    "href": "lectures/lecture-09-slides.html#coming-soon",
    "title": "Showing the Right Numbers",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#references",
    "href": "lectures/lecture-09-slides.html#references",
    "title": "Showing the Right Numbers",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-09-slides.html#references-1",
    "href": "lectures/lecture-09-slides.html#references-1",
    "title": "Showing the Right Numbers",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-11-slides.html#coming-soon",
    "href": "lectures/lecture-11-slides.html#coming-soon",
    "title": "Measurement",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-11-slides.html#references",
    "href": "lectures/lecture-11-slides.html#references",
    "title": "Measurement",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-11-slides.html#references-1",
    "href": "lectures/lecture-11-slides.html#references-1",
    "title": "Measurement",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#coming-soon",
    "href": "lectures/lecture-13-slides.html#coming-soon",
    "title": "Experiments",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#references",
    "href": "lectures/lecture-13-slides.html#references",
    "title": "Experiments",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-13-slides.html#references-1",
    "href": "lectures/lecture-13-slides.html#references-1",
    "title": "Experiments",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#coming-soon",
    "href": "lectures/lecture-15-slides.html#coming-soon",
    "title": "Data Cleaning and Preparation",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#references",
    "href": "lectures/lecture-15-slides.html#references",
    "title": "Data Cleaning and Preparation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-15-slides.html#references-1",
    "href": "lectures/lecture-15-slides.html#references-1",
    "title": "Data Cleaning and Preparation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#coming-soon",
    "href": "lectures/lecture-17-slides.html#coming-soon",
    "title": "Linear Models",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#references",
    "href": "lectures/lecture-17-slides.html#references",
    "title": "Linear Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-17-slides.html#references-1",
    "href": "lectures/lecture-17-slides.html#references-1",
    "title": "Linear Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#coming-soon",
    "href": "lectures/lecture-19-slides.html#coming-soon",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#references",
    "href": "lectures/lecture-19-slides.html#references",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-19-slides.html#references-1",
    "href": "lectures/lecture-19-slides.html#references-1",
    "title": "Generalized Linear Models (Binary Outcomes)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#coming-soon",
    "href": "lectures/lecture-21-slides.html#coming-soon",
    "title": "Mutilevel Models",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#references",
    "href": "lectures/lecture-21-slides.html#references",
    "title": "Mutilevel Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-21-slides.html#references-1",
    "href": "lectures/lecture-21-slides.html#references-1",
    "title": "Mutilevel Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#coming-soon",
    "href": "lectures/lecture-23-slides.html#coming-soon",
    "title": "Multilevel Regression with Poststratification (MRP)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#references",
    "href": "lectures/lecture-23-slides.html#references",
    "title": "Multilevel Regression with Poststratification (MRP)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-23-slides.html#references-1",
    "href": "lectures/lecture-23-slides.html#references-1",
    "title": "Multilevel Regression with Poststratification (MRP)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-25-slides.html#coming-soon",
    "href": "lectures/lecture-25-slides.html#coming-soon",
    "title": "Project Work",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-25-slides.html#references",
    "href": "lectures/lecture-25-slides.html#references",
    "title": "Project Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-25-slides.html#references-1",
    "href": "lectures/lecture-25-slides.html#references-1",
    "title": "Project Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus/policies.html",
    "href": "syllabus/policies.html",
    "title": " Policies",
    "section": "",
    "text": "üöß Under Construction\n\n\n\n I am currently developing these course materials. Please check back in January 2025.  - John",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/policies.html#course-policies",
    "href": "syllabus/policies.html#course-policies",
    "title": " Policies",
    "section": "Course Policies",
    "text": "Course Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/policies.html#department-and-faculty-policies",
    "href": "syllabus/policies.html#department-and-faculty-policies",
    "title": " Policies",
    "section": "Department and Faculty Policies",
    "text": "Department and Faculty Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "syllabus/policies.html#university-policies",
    "href": "syllabus/policies.html#university-policies",
    "title": " Policies",
    "section": "University Policies",
    "text": "University Policies",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Policies"
    ]
  },
  {
    "objectID": "lectures/lecture-01-notes.html",
    "href": "lectures/lecture-01-notes.html",
    "title": "Introduction",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#welcome-to-soci-3040",
    "href": "lectures/lecture-01-notes.html#welcome-to-soci-3040",
    "title": "Introduction",
    "section": "üëã Welcome to SOCI 3040!",
    "text": "üëã Welcome to SOCI 3040!\n\nThis is an introduction the knowledge and skills you need to tell credible stories with quantitative data‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#why-data-storytelling-matters",
    "href": "lectures/lecture-01-notes.html#why-data-storytelling-matters",
    "title": "Introduction",
    "section": "Why Data Storytelling Matters",
    "text": "Why Data Storytelling Matters\n\n‚ÄúA lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing.‚Äù (Alexander 2023)\n\n\nCore foundation of quantitative research methods\nBridge between analysis and understanding\nEssential skill for modern researchers"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#common-concerns",
    "href": "lectures/lecture-01-notes.html#common-concerns",
    "title": "Introduction",
    "section": "Common Concerns",
    "text": "Common Concerns\nFive Key Questions for Data Stories\n\nWhat is the dataset? Who generated it and why?\nWhat is the underlying process? What‚Äôs missing?\nWhat is the dataset trying to say? What else could it say?\nWhat do we want others to see? How do we convince them?\nWho is affected? Are they represented in the data?\n\n\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#core-workflow-components",
    "href": "lectures/lecture-01-notes.html#core-workflow-components",
    "title": "Introduction",
    "section": "Core Workflow Components",
    "text": "Core Workflow Components\n\n\n\n\n\n\nflowchart LR\n    p[[Plan]]\n    sim[[Simulate]]\n    a[[Acquire]]\n    e[[Explore / Analyze]]\n    s[[Share]]\n\n    p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n\n\nFigure¬†1: Rohan Alexander‚Äôs (2023) workflow for telling stories with data.\n\n\n\n\n\n\nPlan and sketch endpoint\nSimulate and consider data\nAcquire and prepare data\nExplore and understand data\nShare findings"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#plan-and-sketch",
    "href": "lectures/lecture-01-notes.html#plan-and-sketch",
    "title": "Introduction",
    "section": " Plan and Sketch",
    "text": "Plan and Sketch\n\n\n\ndeliberate, reasoned decisions\npurposeful adjustments\neven 10 minutes of planning is valuable\n\n\n\n\n\nThink of Alice‚Äôs conversation with the Cheshire Cat üò∏. Without a clear goal, any path will do. We need clear direction to prevent aimless wandering.\n\n\n\n\n\nPlanning and sketching an endpoint is the first crucial step in the workflow because it ensures we have a clear objective and direction for our analysis. By thoughtfully considering where we want to go, we stay focused and efficient, preventing aimless wandering and scope creep. Like Alice‚Äôs conversation with the Cheshire Cat in Alice‚Äôs Adventures in Wonderland, without a defined goal, any path will suffice, but we typically cannot afford to wander aimlessly. While our endpoint may change, having an initial objective allows for deliberate and reasoned adjustments. This planning doesn‚Äôt require extensive time‚Äîoften just ten minutes with paper and pen can provide significant value."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#simulate-data",
    "href": "lectures/lecture-01-notes.html#simulate-data",
    "title": "Introduction",
    "section": " Simulate Data",
    "text": "Simulate Data\n\nForces detailed thinking\nClarifies expected data structure and distributions.\nHelps with cleaning and preparation\nIdentifies potential issues beforehand.\nProvides clear testing framework\nEnsures data meets expectations.\n‚ÄúAlmost free‚Äù with modern computing\nProvides ‚Äúan intimate feeling for the situation‚Äù (Hamming [1997] 2020)\n\n\nSimulating data is the second step, forcing us into the details of our analysis by focusing on expected data structures and distributions. By creating simulated data, we define clear features that our real dataset should satisfy, aiding in data cleaning and preparation. For example, simulating an age-group variable with specific categories allows us to test the real data for consistency. Simulation is also vital for validating statistical models; by applying models to data with known properties, we can ensure they perform as intended before using them on real data. Since simulation is inexpensive and quick with modern computing resources, it provides ‚Äúan intimate feeling for the situation‚Äù and helps build confidence in our analytical tools."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#acquire-and-prepare",
    "href": "lectures/lecture-01-notes.html#acquire-and-prepare",
    "title": "Introduction",
    "section": " Acquire and Prepare",
    "text": "Acquire and Prepare\n\nOften overlooked but crucial stage\nMany difficult decisions required: data sources, formats, permissions.\nCan significantly affect statistical results (Huntington-Klein et al. 2021)\nCommon challenges: quantity (too little or too much data) and quality\n\n\nAcquiring and preparing the actual data is often an overlooked yet challenging stage of the workflow that requires many critical decisions. This phase can significantly affect statistical results, as the choices made determine the quality and usability of the data. Researchers may feel overwhelmed‚Äîeither by having too little data, raising concerns about the feasibility of analysis, or by having too much data, making it difficult to manage and process. Careful consideration, thorough cleaning, and preparation at this stage are crucial for the success of subsequent analysis, ensuring that the data are suitable for the questions being asked."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#explore-and-understand",
    "href": "lectures/lecture-01-notes.html#explore-and-understand",
    "title": "Introduction",
    "section": " Explore and Understand",
    "text": "Explore and Understand\n\nBegin with descriptive statistics\nMove to statistical models\nRemember: Models are tools, not truth\nModels reflect:\n\nEarlier decisions\nData acquisition choices\nCleaning procedures\n\n\n\nIn the fourth step, we explore and understand the actual data by examining relationships within the dataset. This process typically starts with descriptive statistics and progresses to statistical modeling. It‚Äôs important to remember that statistical models are tools‚Äînot absolute truths‚Äîand they operate based on the instructions we provide. They help us understand the data more clearly but do not offer definitive results. At this stage, the models we develop are heavily influenced by prior decisions made during data acquisition and preparation. Sophisticated modelers understand that models are like the visible tip of an iceberg, reliant on the substantial groundwork laid in earlier stages. They recognize that modeling results are shaped by choices about data inclusion, measurement, and recording, reflecting broader aspects of the world even before data reach the workflow."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#share-findings",
    "href": "lectures/lecture-01-notes.html#share-findings",
    "title": "Introduction",
    "section": " Share Findings",
    "text": "Share Findings\n\nHigh-fidelity communication essential\nDocument all decisions\nBuild credibility through transparency\n\nInclude:\n\nWhat was done\nWhy it was done\nWhat was found\nWeaknesses of the approach\n\n\nThe final step is to share what was done and what was found, communicating with as much clarity and fidelity as possible. Effective communication involves detailing the decisions made throughout the workflow, the reasons behind them, the findings, and the limitations of the approach. We aim to uncover something important, so it‚Äôs essential to document everything initially, even if other forms of communication supplement the written record later. Openness about the entire process‚Äîfrom data acquisition to analysis‚Äîbuilds credibility and ensures others can fully engage with and understand the work. Without clear communication, even excellent work can be overlooked or misunderstood. While the world may not always reward merit alone, thorough and transparent communication enhances the impact of our work, and achieving mastery in this area requires significant experience and practice."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#the-foundation",
    "href": "lectures/lecture-01-notes.html#the-foundation",
    "title": "Introduction",
    "section": "The Foundation",
    "text": "The Foundation\n\n\n\n Communication Reproducibility Ethics Questions Measurement Data Collection Data Cleaning Exploratory Data Analysis Modeling Scaling\n\n\n\n\n\n\nEssential foundation for the data storytelling workflow."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#communication-most-important",
    "href": "lectures/lecture-01-notes.html#communication-most-important",
    "title": "Introduction",
    "section": " Communication (Most Important)",
    "text": "Communication (Most Important)\n\n‚ÄúSimple analysis, communicated well, is more valuable than complicated analysis communicated poorly.‚Äù (Alexander 2023)\n\n\n‚ÄúOne challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it.‚Äù (Alexander 2023)\n\n\nWrite in plain language\nUse tables, graphs, and models effectively\nFocus on the audience‚Äôs perspective"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#reproducibility",
    "href": "lectures/lecture-01-notes.html#reproducibility",
    "title": "Introduction",
    "section": " Reproducibility",
    "text": "Reproducibility\nEverything must be independently repeatable.\nRequirements:\n\nOpen access to code\nData availability or simulation\nAutomated testing\nClear documentation\nAim for autonomous end-to-end reproducibility"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#ethics",
    "href": "lectures/lecture-01-notes.html#ethics",
    "title": "Introduction",
    "section": " Ethics",
    "text": "Ethics\n\n‚ÄúThis means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen?‚Äù (Alexander 2023)\n\n\nConsider the full context of the dataset (D‚ÄôIgnazio and Klein 2020)\nAcknowledge the social, cultural, and political forces (Crawford 2021)\nUse data ethically with concern for impact and equity"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#questions",
    "href": "lectures/lecture-01-notes.html#questions",
    "title": "Introduction",
    "section": " Questions",
    "text": "Questions\n\nQuestions evolve through understanding\nChallenge of operationalizing variables\nCuriosity is essential, drives deeper exploration\nValue of ‚Äúhybrid‚Äù knowledge that combines multiple disciplines\nComfort with asking ‚Äúdumb‚Äù questions\n\n\nCuriosity is a key source of internal motivation that drives us to thoroughly explore a dataset and its associated processes. As we delve deeper, each question we pose tends to generate additional questions, leading to continual improvement and refinement of our understanding. This iterative questioning contrasts with the traditional Popperian approach of fixed hypothesis testing often taught quantitative methods courses in the sciences; instead, questions evolve continuously throughout the exploration. Finding an initial research question can be challenging, especially when attempting to operationalize it into measurable and available variables.\nStrategies to overcome this include selecting an area of genuine interest, sketching broad claims that can be honed into specific questions, and combining insights from different fields. Developing comfort with the inherent messiness of real-world data allows us to ask new questions as the data evolve. Knowing a dataset in detail often reveals unexpected patterns or anomalies, which we can explore further with subject-matter experts. Becoming a ‚Äúhybrid‚Äù‚Äîcultivating knowledge across various disciplines‚Äîand being comfortable with asking seemingly simple or ‚Äúdumb‚Äù questions are particularly valuable in enhancing our understanding and fostering meaningful insights."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#measurement",
    "href": "lectures/lecture-01-notes.html#measurement",
    "title": "Introduction",
    "section": " Measurement",
    "text": "Measurement\n\n‚ÄúThe world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect.‚Äù (Alexander 2023)\n\n\n\nMeasuring even simple things is challenging\nExample: Measuring height\n\nShoes on or off?\nTime of day affects height.\nDifferent tools yield different results.\n\nMore complex measurements are even harder. How do we measure happiness or pain?\n\nMeasurement requires decisions and is not value-free\nContext and purpose guide all measurement choices\n\n\n\n\n\nPicasso‚Äôs dog and the challenges of reduction.\n\n\n\n\n\nMeasurement and data collection involve the complex task of deciding how to translate the vibrant, multifaceted world into quantifiable data. This process is challenging because even seemingly simple measurements, like a person‚Äôs height, can vary based on factors like the time of day or the tools used (e.g., tape measure versus laser), making consistent comparison difficult and often unfeasible. The difficulty intensifies with more abstract concepts such as sadness or pain, where defining and measuring them consistently is even more problematic. This reduction of the world into data is not value-free; it requires critical decisions about what to measure, how to measure it, and what to ignore, all influenced by context and purpose. Like Picasso‚Äôs minimalist drawings that capture the essence of a dog but lack details necessary for specific assessments (e.g., determining if the dog is sick), we must deeply understand and respect what we‚Äôre measuring, carefully deciding which features are essential and which can be stripped away to serve our research objectives."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#data-collection-cleaning",
    "href": "lectures/lecture-01-notes.html#data-collection-cleaning",
    "title": "Introduction",
    "section": " &  Data Collection & Cleaning",
    "text": "&  Data Collection & Cleaning\n\n‚ÄúData never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.‚Äù (Alexander 2023)\n\n\nCollection determines possibilities\n\nWhat and how we measure matters.\n\nCleaning requires many decisions\n\nExample: Survey responses on gender\nOptions: ‚Äúman‚Äù, ‚Äúwoman‚Äù, ‚Äúprefer not to say‚Äù, ‚Äúother‚Äù\nHandling ‚Äúprefer not to say‚Äù and open-text responses.\n\nDocument every step\n\nEnsures transparency and reproducibility.\n\nConsider implications of choices\n\nEthical considerations and representation.\n\n\n\nData cleaning and preparation is a critical and complex part of data analysis that requires careful attention and numerous decisions. Using the example of a survey collecting gender information with options like ‚Äúman,‚Äù ‚Äúwoman,‚Äù ‚Äúprefer not to say,‚Äù and ‚Äúother‚Äù (which includes open-text responses), the text illustrates the challenges researchers face in handling sensitive and diverse data entries. Decisions such as whether to exclude ‚Äúprefer not to say‚Äù responses (which would ignore certain participants) or how to categorize open-text entries (where merging them with other categories might disrespect respondents‚Äô specific choices) have significant implications. There is no universally correct approach; choices depend on the context and purpose of the analysis. Therefore, it‚Äôs vital to meticulously record every step of the data cleaning process to ensure transparency and allow others to understand the decisions made. Ultimately, data do not speak for themselves; they reflect the interpretations and choices of those who prepare and analyze them."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#eda-modeling-scaling",
    "href": "lectures/lecture-01-notes.html#eda-modeling-scaling",
    "title": "Introduction",
    "section": "+ EDA, Modeling, & Scaling",
    "text": "+ EDA, Modeling, & Scaling\n\nExploratory Data Analysis (EDA)\n\nIterative process\nNever truly complete\nShapes understanding\n\n\n\nModeling\n\nTool for understanding\nNot a recipe to follow\nJust one representation of reality\nStatistical significance \\(\\neq\\) scientific significance\nStatistical models help us explore the shape of the data; are like echolocation\n\n\n\nScaling\n\nUsing programming languages like R and Python\n\nHandle large datasets efficiently\nAutomate repetitive tasks\nShare work widely and quickly\n\nOutputs can reach many people easily\nAPIs can make analyses accessible in real-time\n\n\nExploratory Data Analysis (EDA) is an open-ended, iterative process that involves immersing ourselves in the data to understand its shape and structure before formal modeling begins. It includes producing summary statistics, creating graphs and tables, and sometimes even preliminary modeling. EDA requires a variety of skills and never truly finishes, as there‚Äôs always more to explore. Although it‚Äôs challenging to delineate where EDA ends and formal statistical modeling begins‚Äîsince our beliefs and understanding evolve continuously‚ÄîEDA is foundational in shaping the story we tell about our data. While not typically included explicitly in the final narrative, it‚Äôs crucial that all steps taken during EDA are recorded and shared.\nStatistical modeling builds upon the insights gained from EDA and has a rich history spanning hundreds of years. Statistics is not merely a collection of dry theorems and proofs; it‚Äôs a way of exploring and understanding the world. A statistical model is not a rigid recipe to follow mechanically but a tool for making sense of data. Modeling is usually required to infer statistical patterns, formally known as statistical inference‚Äîthe process of using data to infer the distribution that generated them. Importantly, statistical significance does not equate to scientific significance, and relying on arbitrary pass/fail tests is rarely appropriate. Instead, we should use statistical modeling as a form of echolocation, listening to what the models tell us about the shape of the world while recognizing that they offer just one representation of reality.\nScaling our work becomes feasible with the use of programming languages like R and Python, which allow us to handle vast amounts of data efficiently. Scaling refers to both inputs and outputs; it‚Äôs essentially as easy to analyze ten observations as it is to analyze a million. This capability enables us to quickly determine the extent to which our findings apply. Additionally, our outputs can be disseminated to a wide audience effortlessly‚Äîwhether it‚Äôs one person or a hundred. By utilizing Application Programming Interfaces (APIs), our analyses and stories can be accessed thousands of times per second, greatly enhancing their impact and accessibility."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data",
    "title": "Introduction",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\n‚ÄúThere is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.‚Äù (Hamming [1997] 2020, 177)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-1",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-1",
    "title": "Introduction",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n\nTo a certain extent we are wasting our time. We have a perfect model of the world‚Äîit is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.  There are different approximations of ‚Äúplausibly measurable‚Äù. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data. (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-2",
    "href": "lectures/lecture-01-notes.html#how-do-our-worlds-become-data-2",
    "title": "Introduction",
    "section": "How Do Our Worlds Become Data?",
    "text": "How Do Our Worlds Become Data?\n Through skillfulreduction üë®‚Äçüç≥\n\nJust as a chef reduces a rich sauce to concentrate its essential flavors, we simplify reality into data‚Äîplausibly measurable approximations that capture the essence of the complex world. This reduction process involves deliberate choices about what aspects of reality to include, much like deciding which ingredients to emphasize in a culinary reduction. Our datasets, therefore, are distilled versions of reality, highlighting specific components while inevitably leaving out others.\nAs we employ statistical models to explore and understand these datasets, it‚Äôs crucial to recognize both what the data include and what they omit. Similar to how a reduction in cooking intensifies certain flavors while others may be lost or muted, the process of data simplification can inadvertently exclude important nuances or perspectives. Particularly in data science, where human-generated data are prevalent, we must consider who or what is systematically missing from our datasets. Some individuals or phenomena may not fit neatly into our chosen methods and might be oversimplified or excluded entirely. The abstraction and simplification inherent in turning the world into data require careful judgment‚Äîmuch like a chef monitoring a reduction to achieve the desired consistency without overcooking‚Äîto determine when simplification is appropriate and when it risks losing critical information.\nMeasurement itself presents significant challenges, and those deeply involved in the data collection process often have less trust in the data than those removed from it. Just as the process of reducing a sauce demands constant attention to prevent burning or altering the intended flavor, converting the world into data involves numerous decisions and potential errors‚Äîfrom selecting what to measure to deciding on the methods and accuracy required. Advances in instruments‚Äîfrom telescopes in astronomy to real-time internet data collection‚Äîhave expanded our ability to gather data, much like new culinary techniques enhance a chef‚Äôs ability to create complex dishes. However, the world still imperfectly becomes data, and to truly learn from it, we must actively seek to understand the imperfections in our datasets and consider how our ‚Äúreduction‚Äù process may have altered or omitted important aspects of reality."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#what-is-data-science",
    "href": "lectures/lecture-01-notes.html#what-is-data-science",
    "title": "Introduction",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n‚ÄúData science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict.‚Äù (Alexander 2023)\n\nKey Principles\n\nData are generated, and must be gathered, cleaned, and prepared\nThese decisions matter\nThe process will be difficult\nDevelop resilience and intrinsic motivation"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#the-power-of-multiple-perspectives",
    "href": "lectures/lecture-01-notes.html#the-power-of-multiple-perspectives",
    "title": "Introduction",
    "section": "The Power of Multiple Perspectives",
    "text": "The Power of Multiple Perspectives\n\n‚ÄúThe strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past.‚Äù (Alexander 2023)\n\n\nData science is multi-disciplinary\nCombines statistics, software engineering, subject-matter expertise, and more.\nDiversity enhances understanding\nDifferent perspectives lead to better questions and solutions.\nCollaboration is key\nRespect and integrate insights from various fields."
  },
  {
    "objectID": "lectures/lecture-01-notes.html#embracing-the-challenge",
    "href": "lectures/lecture-01-notes.html#embracing-the-challenge",
    "title": "Introduction",
    "section": "Embracing the Challenge",
    "text": "Embracing the Challenge\nOur world is messy, and so are our data. Telling stories with data is difficult but rewarding.\n\nDevelop resilience and intrinsic motivation\nAccept that failure is part of the process.\nConsider possibilities and probabilities\nLearn to make trade-offs.\nNo perfect analysis exists\nAim for transparency and continuous improvement.\n\n\n‚ÄúUltimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.‚Äù (Alexander 2023)"
  },
  {
    "objectID": "lectures/lecture-01-notes.html#key-takeaways",
    "href": "lectures/lecture-01-notes.html#key-takeaways",
    "title": "Introduction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData storytelling bridges analysis and understanding\nEffective communication is paramount\nEthics and reproducibility are foundational\nAsk meaningful questions and measure thoughtfully and transparently\nData collection and cleaning shape your analysis\nEmbrace the iterative nature of exploration and modeling\nLeverage technology to scale and share your work\nBe mindful of the limitations of your data"
  },
  {
    "objectID": "lectures/lecture-04-notes.html",
    "href": "lectures/lecture-04-notes.html",
    "title": "Good and Bad Ways to Look at Data",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#coming-soon",
    "href": "lectures/lecture-04-notes.html#coming-soon",
    "title": "Good and Bad Ways to Look at Data",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-04-notes.html#references",
    "href": "lectures/lecture-04-notes.html#references",
    "title": "Good and Bad Ways to Look at Data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-06-notes.html",
    "href": "lectures/lecture-06-notes.html",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-06-notes.html#coming-soon",
    "href": "lectures/lecture-06-notes.html#coming-soon",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-06-notes.html#references",
    "href": "lectures/lecture-06-notes.html#references",
    "title": "Reproducibility + Getting Started with R and RStudio",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-08-notes.html",
    "href": "lectures/lecture-08-notes.html",
    "title": "The Grammar of Graphics, Plotting in the Tidyverse",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-08-notes.html#coming-soon",
    "href": "lectures/lecture-08-notes.html#coming-soon",
    "title": "The Grammar of Graphics, Plotting in the Tidyverse",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-08-notes.html#references",
    "href": "lectures/lecture-08-notes.html#references",
    "title": "The Grammar of Graphics, Plotting in the Tidyverse",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-10-notes.html",
    "href": "lectures/lecture-10-notes.html",
    "title": "Piping, Summarizing, and Transforming",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-10-notes.html#coming-soon",
    "href": "lectures/lecture-10-notes.html#coming-soon",
    "title": "Piping, Summarizing, and Transforming",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-10-notes.html#references",
    "href": "lectures/lecture-10-notes.html#references",
    "title": "Piping, Summarizing, and Transforming",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-12-notes.html",
    "href": "lectures/lecture-12-notes.html",
    "title": "Sampling",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#coming-soon",
    "href": "lectures/lecture-12-notes.html#coming-soon",
    "title": "Sampling",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-12-notes.html#references",
    "href": "lectures/lecture-12-notes.html#references",
    "title": "Sampling",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-14-notes.html",
    "href": "lectures/lecture-14-notes.html",
    "title": "Preparing Data ‚Äì Cleaning, Testing, Validating, and Documenting",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#coming-soon",
    "href": "lectures/lecture-14-notes.html#coming-soon",
    "title": "Preparing Data ‚Äì Cleaning, Testing, Validating, and Documenting",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-14-notes.html#references",
    "href": "lectures/lecture-14-notes.html#references",
    "title": "Preparing Data ‚Äì Cleaning, Testing, Validating, and Documenting",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-16-notes.html",
    "href": "lectures/lecture-16-notes.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#coming-soon",
    "href": "lectures/lecture-16-notes.html#coming-soon",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-16-notes.html#references",
    "href": "lectures/lecture-16-notes.html#references",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-18-notes.html",
    "href": "lectures/lecture-18-notes.html",
    "title": "Linear Models + Model-based Graphics",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#coming-soon",
    "href": "lectures/lecture-18-notes.html#coming-soon",
    "title": "Linear Models + Model-based Graphics",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-18-notes.html#references",
    "href": "lectures/lecture-18-notes.html#references",
    "title": "Linear Models + Model-based Graphics",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-20-notes.html",
    "href": "lectures/lecture-20-notes.html",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#coming-soon",
    "href": "lectures/lecture-20-notes.html#coming-soon",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-20-notes.html#references",
    "href": "lectures/lecture-20-notes.html#references",
    "title": "Generalized Linear Models (Count Outcomes)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-22-notes.html",
    "href": "lectures/lecture-22-notes.html",
    "title": "Mutilevel Models",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#coming-soon",
    "href": "lectures/lecture-22-notes.html#coming-soon",
    "title": "Mutilevel Models",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-22-notes.html#references",
    "href": "lectures/lecture-22-notes.html#references",
    "title": "Mutilevel Models",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lectures/lecture-24-notes.html",
    "href": "lectures/lecture-24-notes.html",
    "title": "Text as Data",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#coming-soon",
    "href": "lectures/lecture-24-notes.html#coming-soon",
    "title": "Text as Data",
    "section": "Coming soon‚Ä¶",
    "text": "Coming soon‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-24-notes.html#references",
    "href": "lectures/lecture-24-notes.html#references",
    "title": "Text as Data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus/assessment.html",
    "href": "syllabus/assessment.html",
    "title": " Assessment",
    "section": "",
    "text": "üöß Under Construction\n\n\n\n I am currently developing these course materials. Please check back in January 2025.  - John\n\n\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\n\n\n\n\n\nDue (On or Before)\n\n\nBy\n\n\nAssignment\n\n\nGrade Weight\n\n\n\n\n\n\nTuesday, January 7, 2025\n\n\n11:59 pm\n\n\nReport 1 (Solo)\n\n\n10%\n\n\n\n\nWednesday, January 8, 2025\n\n\n11:59 pm\n\n\nReport 2 (Solo)\n\n\n15%\n\n\n\n\nThursday, January 9, 2025\n\n\n11:59 pm\n\n\nReport 3 (Co-authored)\n\n\n25%\n\n\n\n\nFriday, January 10, 2025\n\n\n11:59 pm\n\n\nReport 4 (Co-authored)\n\n\n25%\n\n\n\n\nSunday, January 12, 2025\n\n\n11:59 pm\n\n\nContributions to the Course Blog\n\n\n15%\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "<strong>SYLLABUS</strong>",
      "&nbsp;&nbsp;Learning Assessment"
    ]
  },
  {
    "objectID": "lectures/lecture-02-slides.html#the-firehose",
    "href": "lectures/lecture-02-slides.html#the-firehose",
    "title": "Telling Stories with Data",
    "section": "the firehose",
    "text": "the firehose\n\n\n\n\n\nflowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander‚Äôs (2023) Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#the-firehose-1",
    "href": "lectures/lecture-02-slides.html#the-firehose-1",
    "title": "Telling Stories with Data",
    "section": "the firehose",
    "text": "the firehose\n\nWhenever you‚Äôre learning a new tool, for a long time, you‚Äôre going to suck\\(\\dots\\) But the good news is that is typical; that‚Äôs something that happens to everyone, and it‚Äôs only temporary.\nHadley Wickham as quoted by Barrett (2021)\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan Alexander (2023)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#preliminaries-1",
    "href": "lectures/lecture-02-slides.html#preliminaries-1",
    "title": "Telling Stories with Data",
    "section": "preliminaries",
    "text": "preliminaries\n\nRStudio / CodeSpaces / Whatever‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#preliminaries-2",
    "href": "lectures/lecture-02-slides.html#preliminaries-2",
    "title": "Telling Stories with Data",
    "section": "preliminaries",
    "text": "preliminaries\n\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#import-libraries",
    "href": "lectures/lecture-02-slides.html#import-libraries",
    "title": "Telling Stories with Data",
    "section": "import libraries",
    "text": "import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan-1",
    "href": "lectures/lecture-02-slides.html#plan-1",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\nAustralian Elections\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan-2",
    "href": "lectures/lecture-02-slides.html#plan-2",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure¬†1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a ‚Äúdivision‚Äù in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-1",
    "href": "lectures/lecture-02-slides.html#simulate-1",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-2",
    "href": "lectures/lecture-02-slides.html#simulate-2",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\nWe‚Äôll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-3",
    "href": "lectures/lecture-02-slides.html#simulate-3",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we‚Äôre creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named ‚ÄúDivision‚Äù. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it‚Äôs used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options ‚Äî ‚ÄúLiberal‚Äù, ‚ÄúLabor‚Äù, ‚ÄúNational‚Äù, ‚ÄúGreen‚Äù, and ‚ÄúOther‚Äù ‚Äî into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the ‚ÄúDivision‚Äù column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we‚Äôre picking ‚Äúwith replacement‚Äù (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn‚Äôt match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-4",
    "href": "lectures/lecture-02-slides.html#simulate-4",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\nü§ò We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 √ó 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Labor   \n 2        2 Labor   \n 3        3 Other   \n 4        4 National\n 5        5 Green   \n 6        6 Other   \n 7        7 National\n 8        8 Green   \n 9        9 Labor   \n10       10 Labor   \n# ‚Ñπ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-1",
    "href": "lectures/lecture-02-slides.html#acquire-1",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-2",
    "href": "lectures/lecture-02-slides.html#acquire-2",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nWe‚Äôll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n‚úåÔ∏è R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief ‚ÄúOde to the here package,‚Äù ‚Äúhere here,‚Äù which you can read‚Ä¶ here."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-3",
    "href": "lectures/lecture-02-slides.html#acquire-3",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\nü§ò We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 √ó 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA‚Ä¶\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather‚Ä¶ KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge‚Ä¶ ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE‚Ä¶\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ‚Ñπ 141 more rows\n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-4",
    "href": "lectures/lecture-02-slides.html#acquire-4",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 √ó 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA‚Ä¶\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri‚Ä¶ KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-5",
    "href": "lectures/lecture-02-slides.html#acquire-5",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 √ó 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma‚Ä¶ STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO‚Ä¶\n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-6",
    "href": "lectures/lecture-02-slides.html#acquire-6",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n‚ÄúWe are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.‚Äù (Alexander 2023)\n\n\nLet‚Äôs clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-7",
    "href": "lectures/lecture-02-slides.html#acquire-7",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let‚Äôs look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ‚Ñπ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-8",
    "href": "lectures/lecture-02-slides.html#acquire-8",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\n\n‚úåÔ∏è R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-9",
    "href": "lectures/lecture-02-slides.html#acquire-9",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-10",
    "href": "lectures/lecture-02-slides.html#acquire-10",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n‚úåÔ∏è R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet‚Äôs rename."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-11",
    "href": "lectures/lecture-02-slides.html#acquire-11",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-12",
    "href": "lectures/lecture-02-slides.html#acquire-12",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let‚Äôs simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-13",
    "href": "lectures/lecture-02-slides.html#acquire-13",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-14",
    "href": "lectures/lecture-02-slides.html#acquire-14",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! üòé"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#aus_elections_clean_path",
    "href": "lectures/lecture-02-slides.html#aus_elections_clean_path",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nLet‚Äôs save the cleaned data so that we can start with it data in the next stage. We‚Äôll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-1",
    "href": "lectures/lecture-02-slides.html#explore-understand-1",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-2",
    "href": "lectures/lecture-02-slides.html#explore-understand-2",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n‚úåÔ∏è R Tip\n\n\nI‚Äôm using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won‚Äôt work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-3",
    "href": "lectures/lecture-02-slides.html#explore-understand-3",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\nüòé"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-4",
    "href": "lectures/lecture-02-slides.html#explore-understand-4",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 √ó 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-5",
    "href": "lectures/lecture-02-slides.html#explore-understand-5",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nRemember, we‚Äôre trying to make something like this.\n\n\n\n\n\n\n‚úåÔ∏è R Tip\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer‚Äôs understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer‚Äôs eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-6",
    "href": "lectures/lecture-02-slides.html#explore-understand-6",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\nLet‚Äôs visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it‚Äôs cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-6-output",
    "href": "lectures/lecture-02-slides.html#explore-understand-6-output",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\n\nFigure¬†2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-7",
    "href": "lectures/lecture-02-slides.html#explore-understand-7",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-7-output",
    "href": "lectures/lecture-02-slides.html#explore-understand-7-output",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\n\nFigure¬†3: Number of seats won, by political party, at the 2022 Australian Federal Election. üòé"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section",
    "href": "lectures/lecture-02-slides.html#section",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure¬†4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#share-1",
    "href": "lectures/lecture-02-slides.html#share-1",
    "title": "Telling Stories with Data",
    "section": " share",
    "text": "share\nExample taken directly from Alexander (2023), here.\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties‚Äî‚ÄúLiberal‚Äù and ‚ÄúLabor‚Äù‚Äîtwo minor parties‚Äî‚ÄúNationals‚Äù and ‚ÄúGreens‚Äù‚Äîand many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure¬†3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan-4",
    "href": "lectures/lecture-02-slides.html#plan-4",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure¬†5 (a) (next slide).\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure¬†5 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan-5",
    "href": "lectures/lecture-02-slides.html#plan-5",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a table\n\n\n\n\n\n\n\nFigure¬†5: Sketches of a dataset and table of the average number of beds occupied each month for shelters in Toronto."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-6",
    "href": "lectures/lecture-02-slides.html#simulate-6",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process. When we turn to analysis, it will provide us with a guide. Conducting analysis without first using simulation can be thought of as shooting arrows without a target‚Äîwhile you are certainly doing something, it is not clear whether you are doing it well."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-7",
    "href": "lectures/lecture-02-slides.html#simulate-7",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            ) # Draw 1,095 times from the Poisson distribution\n    )\n\nsimulated_occupancy_data\n\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it later in the course."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-7-output",
    "href": "lectures/lecture-02-slides.html#simulate-7-output",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n# A tibble: 1,095 √ó 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ‚Ñπ 1,085 more rows"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-16",
    "href": "lectures/lecture-02-slides.html#acquire-16",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    # Each package is associated with a unique id  found in the \"For\n    # Developers\" tab of the relevant page from Open Data Toronto\n    # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    # Within that package, we are interested in the 2021 dataset\n    filter(name ==\n        \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    # Having reduced the dataset to one row we can get the resource\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-17",
    "href": "lectures/lecture-02-slides.html#acquire-17",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 √ó 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME    \n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                \n1     1 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n2     2 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n3     3 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n4     4 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n5     5 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n6     6 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n# ‚Ñπ 28 more variables: SHELTER_ID &lt;dbl&gt;,\n#   SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;,\n#   LOCATION_POSTAL_CODE &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-18",
    "href": "lectures/lecture-02-slides.html#acquire-18",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nWe‚Äôll change the names to make them easier to type using clean_names(), and select() the relevant columns.\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 √ó 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#acquire-19",
    "href": "lectures/lecture-02-slides.html#acquire-19",
    "title": "Telling Stories with Data",
    "section": " acquire",
    "text": "acquire\n\nAll that remains for this step is to save the cleaned dataset.\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\nWHERE ARE THESE NAs COMING FROM?"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-8",
    "href": "lectures/lecture-02-slides.html#explore-understand-8",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\ntoronto_shelters_clean\n\n# A tibble: 50,944 √ó 2\n   occupancy_date occupied_beds\n   &lt;date&gt;                 &lt;dbl&gt;\n 1 2021-01-01                NA\n 2 2021-01-01                NA\n 3 2021-01-01                NA\n 4 2021-01-01                NA\n 5 2021-01-01                NA\n 6 2021-01-01                 6\n 7 2021-01-01                NA\n 8 2021-01-01                NA\n 9 2021-01-01                NA\n10 2021-01-01                NA\n# ‚Ñπ 50,934 more rows"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-9",
    "href": "lectures/lecture-02-slides.html#explore-understand-9",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments‚Äî‚Äúlabel‚Äù and ‚Äúabbr‚Äù‚Äîto get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in sec-farm-data and sec-exploratory-data-analysis. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create tbl-homelessoccupancyd."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-9-output",
    "href": "lectures/lecture-02-slides.html#explore-understand-9-output",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\nTable¬†1: Shelter usage in Toronto in 2021\n\n\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-10",
    "href": "lectures/lecture-02-slides.html#explore-understand-10",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable(\n        col.names = c(\"Month\", \"Average daily number of&lt;br&gt;occupied beds (per shelter)\"),\n        digits = 1\n    )\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (tbl-homelessoccupancy). In particular we make the column names easier to read, and only show an appropriate number of decimal places."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#explore-understand-10-output",
    "href": "lectures/lecture-02-slides.html#explore-understand-10-output",
    "title": "Telling Stories with Data",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\nMonth\nAverage daily number ofoccupied beds (per shelter)\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#share-3",
    "href": "lectures/lecture-02-slides.html#share-3",
    "title": "Telling Stories with Data",
    "section": " share",
    "text": "share\nExample taken directly from Alexander (2023), here.\n\n\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R (R Core Team 2023) as well as the tidyverse (Wickham 2017), janitor (Firke 2023), opendatatoronto (Gelfand 2022), lubridate (Grolemund and Wickham 2011), and knitr (Xie 2023). We then made a table of the average number of occupied beds each night for each month (tbl-homelessoccupancy).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (tbl-homelessoccupancy). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\n\n\n\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of Hao (2019), that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan-7",
    "href": "lectures/lecture-02-slides.html#plan-7",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure¬†6 (a) (next slide). We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure¬†6 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#plan-8",
    "href": "lectures/lecture-02-slides.html#plan-8",
    "title": "Telling Stories with Data",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\n\n\n\nFigure¬†6: Sketches of a dataset and graph about the neonatal mortality rate (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-9",
    "href": "lectures/lecture-02-slides.html#simulate-9",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nTo simulate some data that aligns with our plan, we will need three columns: country, year, and NMR. We can do this by repeating the name of each country 50 times with rep(), and enabling the passing of 50 years. Then we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-10",
    "href": "lectures/lecture-02-slides.html#simulate-10",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", 50), rep(\"Australia\", 50),\n                rep(\"Canada\", 50), rep(\"Kenya\", 50)\n            ),\n        year =\n            rep(c(1971:2020), 4),\n        nmr =\n            runif(n = 200, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-10-output",
    "href": "lectures/lecture-02-slides.html#simulate-10-output",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-11",
    "href": "lectures/lecture-02-slides.html#simulate-11",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-12",
    "href": "lectures/lecture-02-slides.html#simulate-12",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n                rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)\n            ),\n        year =\n            rep(c(1:number_of_years + 1970), 4),\n        nmr =\n            runif(n = number_of_years * 4, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-12-output",
    "href": "lectures/lecture-02-slides.html#simulate-12-output",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-13",
    "href": "lectures/lecture-02-slides.html#simulate-13",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat ‚Äúcountry‚Äù is exclusively one of these four: ‚ÄúArgentina‚Äù, ‚ÄúAustralia‚Äù, ‚ÄúCanada‚Äù, or ‚ÄúKenya‚Äù.\nConversely, ‚Äúcountry‚Äù contains all those four countries.\nThat ‚Äúyear‚Äù is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat ‚Äúnmr‚Äù is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-14",
    "href": "lectures/lecture-02-slides.html#simulate-14",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\nsimulated_nmr_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#simulate-15",
    "href": "lectures/lecture-02-slides.html#simulate-15",
    "title": "Telling Stories with Data",
    "section": " simulate",
    "text": "simulate\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-1",
    "href": "lectures/lecture-02-slides.html#section-1",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "The UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\nigme_data_path &lt;- here(\"data\", \"igme.csv\")\nigme_data_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/igme.csv\"\n\n\n\nraw_igme_data &lt;-\n    read_csv(\n        file =\n            \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n        show_col_types = FALSE\n    )\n\nwrite_csv(x = raw_igme_data, file = igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-2",
    "href": "lectures/lecture-02-slides.html#section-2",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "raw_igme_data &lt;-\n    read_csv(\n        file = igme_data_path,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-3",
    "href": "lectures/lecture-02-slides.html#section-3",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "With established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail()\n\nhead(raw_igme_data)\n\n# A tibble: 6 √ó 29\n  `Geographic area` Indicator        Sex   `Wealth Quintile`\n  &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n2 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n3 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n4 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n5 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n6 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n# ‚Ñπ 25 more variables: `Series Name` &lt;chr&gt;,\n#   `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;,\n#   COUNTRY_NOTES &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-4",
    "href": "lectures/lecture-02-slides.html#section-4",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "and what the names of the columns are with names()\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\""
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-5",
    "href": "lectures/lecture-02-slides.html#section-5",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where ‚ÄúSex‚Äù is ‚ÄúTotal‚Äù, ‚ÄúSeries Name‚Äù is ‚ÄúUN IGME estimate‚Äù, ‚ÄúGeographic area‚Äù is one of ‚ÄúArgentina‚Äù, ‚ÄúAustralia‚Äù, ‚ÄúCanada‚Äù, and ‚ÄúKenya‚Äù, and the ‚ÄúIndicator‚Äù is ‚ÄúNeonatal mortality rate‚Äù. After this we are interested in just a few columns: ‚Äúgeographic_area‚Äù, ‚Äútime_period‚Äù, and ‚Äúobs_value‚Äù."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-6",
    "href": "lectures/lecture-02-slides.html#section-6",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    clean_names(raw_igme_data) |&gt;\n    filter(\n        sex == \"Total\",\n        series_name == \"UN IGME estimate\",\n        geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n        indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n    select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-6-output",
    "href": "lectures/lecture-02-slides.html#section-6-output",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "# A tibble: 6 √ó 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-7",
    "href": "lectures/lecture-02-slides.html#section-7",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We need to fix two other aspects: the class of ‚Äútime_period‚Äù is character when we need it to be a year, and the name of ‚Äúobs_value‚Äù should be ‚Äúnmr‚Äù to be more informative."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-8",
    "href": "lectures/lecture-02-slides.html#section-8",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    cleaned_igme_data |&gt;\n    mutate(\n        time_period = str_remove(time_period, \"-06\"),\n        time_period = as.integer(time_period)\n    ) |&gt;\n    filter(time_period &gt;= 1971) |&gt;\n    rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-9",
    "href": "lectures/lecture-02-slides.html#section-9",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "Finally, we can check that our dataset passes the tests that we developed based on the simulated dataset."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-10",
    "href": "lectures/lecture-02-slides.html#section-10",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "cleaned_igme_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-11",
    "href": "lectures/lecture-02-slides.html#section-11",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "All that remains is to save the nicely cleaned dataset.\n\ncleaned_igme_data_path &lt;- here(\"data\", \"cleaned_igme_data.csv\")\nwrite_csv(x = cleaned_igme_data, file = cleaned_igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-12",
    "href": "lectures/lecture-02-slides.html#section-12",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\ncleaned_igme_data &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_igme_data.csv\"),\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-13",
    "href": "lectures/lecture-02-slides.html#section-13",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "We can now make a graph of how NMR has changed over time and the differences between countries (Figure¬†7).\n\n\ncleaned_igme_data |&gt;\n    ggplot(aes(x = year, y = nmr, color = country)) +\n    geom_point() +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#section-13-output",
    "href": "lectures/lecture-02-slides.html#section-13-output",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "Figure¬†7: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)"
  },
  {
    "objectID": "lectures/lecture-02-slides.html#share-5",
    "href": "lectures/lecture-02-slides.html#share-5",
    "title": "Telling Stories with Data",
    "section": " share",
    "text": "share\nExample taken directly from Alexander (2023), here.\n\n\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries‚ÄîArgentina, Australia, Canada, and Kenya‚Äîover the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023).\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure¬†7). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  },
  {
    "objectID": "lectures/lecture-02-slides.html#references",
    "href": "lectures/lecture-02-slides.html#references",
    "title": "Telling Stories with Data",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nBarrett, Malcolm. 2021. Data Science as an Atomic Habit. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\nBronner, Laura. 2020. ‚ÄúWhy Statistics Don‚Äôt Capture the Full Extent of the Systemic Bias in Policing.‚Äù FiveThirtyEight, June. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\nCardoso, Tom. 2020. ‚ÄúBias behind bars: A Globe investigation finds a prison system stacked against Black and Indigenous inmates.‚Äù The Globe and Mail, October. https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. ‚ÄúDates and Times Made Easy with lubridate.‚Äù Journal of Statistical Software 40 (3): 1‚Äì25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHao, Karen. 2019. ‚ÄúThis is How AI Bias Really Happens‚ÄîAnd Why It‚Äôs So Hard To Fix.‚Äù MIT Technology Review, February. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nUN IGME. 2021. ‚ÄúLevels and Trends in Child Mortality, 2021.‚Äù https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nWickham, Hadley. 2017. tidyverse: Easily Install and Load the ‚ÄúTidyverse‚Äù. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse.‚Äù Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Quantitative Research Methods",
    "section": "",
    "text": "TESTING GH-PAGES WORKFLOW\n\n\n\n\n\n\n\n\nCourse Instructor John McLevey (he/him) Professor, Department of Sociology Memorial University\n\n¬† john.mclevey@uwaterloo.ca Note: I do not check or respond to email in the evenings or on weekends.\n\n\nWhere is class? CP-2003 (Chemistry-Physics, Computer Lab) When is class? Tuesdays & Thursdays, 1:30 - 2:50 pm Office Hours: A4054, Tuesdays & Thursdays, 3:00 - 4:00 pm\n\nSOCI 3040, Quantitative Research Methods, will familiarize students with the procedures for understanding and conducting quantitative social science research. It will introduce students to the quantitative research process, hypothesis development and testing, and the application of appropriate tools for analyzing quantitative data. All sections of this course count towards the HSS Quantitative Reasoning Requirement (see mun.ca/hss/qr). (PR: SOCI 1000 or the former SOCI 2000)\n\n\n\n\n\nüëã Hello!\n\n\n\nThis course is built around Rohan Alexander‚Äôs (2023) Telling Stories with Data and Kieran Healy‚Äôs (2019) Data Visualization: A Practical Introduction.\n\n\n\n\nThis section of SOCI 3440 is an introduction to quantitative research methods, from planning an analysis to sharing the final results. Following the workflow from Rohan Alexander‚Äôs (2023) Telling Stories with Data, you will learn how to:\n\nplan an analysis and sketch your data and endpoint\nsimulate some data to ‚Äúforce you into the details‚Äù\nacquire, assess, and prepare empirical data for analysis\nexplore and analyze data by creating visualizations and fitting models\nshare the results of your work with the world!\n\nYou will use this workflow in the context of learning foundational quantitative research skills, including conducting exploratory data analyses and fitting, assessing, and interpreting linear models, generalized linear models, and multilevel models. Reproducibility and research ethics are considered throughout the workflow, and the entire course.\n\n\nAbout the Instructor\nJohn McLevey (he/him) Pronounced like mic-Leave-ee\n\n\nLand Acknowledgement\nWe acknowledge that the lands on which Memorial University‚Äôs campuses are situated are in the traditional territories of diverse Indigenous groups, and we acknowledge with respect the diverse histories and cultures of the Beothuk, Mi‚Äôkmaq, Innu, and Inuit of this province.\n\n\n\n\n\nReferences\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nHealy, Kieran. 2019. Data Visualization: A Practical Introduction. Princeton University Press."
  },
  {
    "objectID": "lectures/lecture-02-content.html",
    "href": "lectures/lecture-02-content.html",
    "title": "preliminaries",
    "section": "",
    "text": "flowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander‚Äôs [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#the-firehose",
    "href": "lectures/lecture-02-content.html#the-firehose",
    "title": "preliminaries",
    "section": "",
    "text": "flowchart LR\n  p[[Plan]]\n  sim[[Simulate]]\n  a[[Acquire]]\n  e[[Explore/Understand]]\n  s[[Share]]\n\n  p --&gt; sim --&gt; a --&gt; e --&gt; s\n\n\n Rohan Alexander‚Äôs [-@alexander2023telling] Telling Stories with Data workflow \n\n\n\n\n\nAustralian elections\nToronto shelters\nNeonatal mortality rates (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#the-firehose-1",
    "href": "lectures/lecture-02-content.html#the-firehose-1",
    "title": "preliminaries",
    "section": "the firehose",
    "text": "the firehose\n\nWhenever you‚Äôre learning a new tool, for a long time, you‚Äôre going to suck\\(\\dots\\) But the good news is that is typical; that‚Äôs something that happens to everyone, and it‚Äôs only temporary.\nHadley Wickham as quoted by @citeBarrett\n\n\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nRohan @alexander2023telling"
  },
  {
    "objectID": "lectures/lecture-02-content.html#preliminaries-1",
    "href": "lectures/lecture-02-content.html#preliminaries-1",
    "title": "preliminaries",
    "section": "preliminaries",
    "text": "preliminaries\n\nRStudio / CodeSpaces / Whatever‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-content.html#preliminaries-2",
    "href": "lectures/lecture-02-content.html#preliminaries-2",
    "title": "preliminaries",
    "section": "preliminaries",
    "text": "preliminaries\n\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")"
  },
  {
    "objectID": "lectures/lecture-02-content.html#import-libraries",
    "href": "lectures/lecture-02-content.html#import-libraries",
    "title": "preliminaries",
    "section": "import libraries",
    "text": "import libraries\n\n\nlibrary(\"janitor\")\nlibrary(\"knitr\")\nlibrary(\"lubridate\")\nlibrary(\"opendatatoronto\")\nlibrary(\"tidyverse\")\nlibrary(\"here\")"
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan-1",
    "href": "lectures/lecture-02-content.html#plan-1",
    "title": "preliminaries",
    "section": " plan",
    "text": "plan\n\n\nAustralian Elections\n\n\n\n\n\n\nHow many seats did each political party win in the 2022 Australian Federal Election?\n\n\n\n Australia is a parliamentary democracywith 151 seats in the House of Representatives. \nMajor parties: Liberal and Labour Minor parties: Nationals and Greens Many smaller parties and independents"
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan-2",
    "href": "lectures/lecture-02-content.html#plan-2",
    "title": "preliminaries",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Sketch of a possible dataset to create a graph\n\n\n\n\n\n\n\n\n\n\n\n(b) Sketch of a possible graph to answer our question\n\n\n\n\n\n\n\nFigure¬†1: Sketches of a potential dataset and graph related to an Australian election. The basic requirement for the dataset is that it has the name of the seat (i.e., a ‚Äúdivision‚Äù in Australia) and the party of the person elected."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-1",
    "href": "lectures/lecture-02-content.html#simulate-1",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-2",
    "href": "lectures/lecture-02-content.html#simulate-2",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\nWe‚Äôll simulate a dataset with two variables,Division and Party, and some values for each.\n\ndivisionthe name of one of the 131 Australian divisions  partythe name of one of the political partiesLiberal, Labor, National, Green, or Other"
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-3",
    "href": "lectures/lecture-02-content.html#simulate-3",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nsimulated_data &lt;-\n    tibble(\n        # Use 1 through to 151 to represent each division\n        \"Division\" = 1:151,\n        # Randomly pick an option, with replacement, 151 times\n        \"Party\" = sample(\n            x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n            size = 151,\n            replace = TRUE\n        )\n    )\n\n\nThe &lt;- symbol is an assignment operator in R. It assigns the value on the right to the variable name on the left. Here, we‚Äôre creating a new data object called simulated_data, which will store a table of simulated information.\ntibble() is a function from the tidyverse package that creates a data frame, which is a type of table used to organize data. Unlike traditional data frames, tibble handles data more cleanly and is especially useful in data analysis.\nInside the tibble() function, we specify columns and the values we want in each. On Line 4, we create a column named ‚ÄúDivision‚Äù. 1:151 generates a sequence of numbers from 1 to 151. This sequence will represent each unique division (or group) in our simulated dataset and helps to identify each row in the data.\nThen we create another column in our tibble called Party. sample() is a function that randomly selects values from a specified set. Here, it‚Äôs used to pick a political party for each division, simulating party representation across divisions.\nx defines the set of values that sample() will pick from. The c() function combines these five options ‚Äî ‚ÄúLiberal‚Äù, ‚ÄúLabor‚Äù, ‚ÄúNational‚Äù, ‚ÄúGreen‚Äù, and ‚ÄúOther‚Äù ‚Äî into a list of possible parties. In other words, each division will be randomly assigned one of these five party names, representing the political party that wins the division in our simulation. size = 151 specifies that sample() should generate 151 random selections, matching the number of divisions we created in the ‚ÄúDivision‚Äù column.\nWhen sampling, replace = TRUE allows each party name to be selected multiple times, as though we‚Äôre picking ‚Äúwith replacement‚Äù (i.e., once we sample a party name, it goes back into the bag so it can be drawn again). Without this, each party could only be chosen once, which wouldn‚Äôt match our goal of assigning a random party to each division.\nWe can print the simulated_data object to view the simulated dataset. When we run this line, R will display the table with two columns, Division and Party, where each division is assigned one of the five parties randomly."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-4",
    "href": "lectures/lecture-02-content.html#simulate-4",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\nü§ò We have our fake data!\n\nsimulated_data\n\n# A tibble: 151 √ó 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Liberal \n 2        2 Labor   \n 3        3 Other   \n 4        4 Labor   \n 5        5 Green   \n 6        6 Liberal \n 7        7 Labor   \n 8        8 National\n 9        9 Labor   \n10       10 Liberal \n# ‚Ñπ 141 more rows"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-1",
    "href": "lectures/lecture-02-content.html#acquire-1",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nThe data we want is provided by the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can download the data using this link, but we want to do it programatically, storing the results to a dataframe object called raw_elections_data.\n\n\ndata_url &lt;- \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\"\n\nraw_elections_data &lt;-\n    read_csv(\n        file = data_url,\n        show_col_types = FALSE,\n        skip = 1\n    )"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-2",
    "href": "lectures/lecture-02-content.html#acquire-2",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nWe‚Äôll save the data as a CSV file.\n\nlibrary(here)\n\nwrite_csv(\n    x = raw_elections_data,\n    file = here(\"data\", \"australian_voting.csv\")\n)\n\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\nThe here() function, from the here library, simplifies file paths by always referencing the root directory for a project. This makes code more reproducible and eliminates issues with working directories, especially when you are using more than one machine, collaborating, or sharing code with someone else. Jenny Bryan wrote a brief ‚ÄúOde to the here package,‚Äù ‚Äúhere here,‚Äù which you can read‚Ä¶ here."
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-3",
    "href": "lectures/lecture-02-content.html#acquire-3",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\nü§ò We have our real data!\n\n\nraw_elections_data\n\n# A tibble: 151 √ó 8\n   DivisionID DivisionNm StateAb CandidateID GivenNm Surname\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1        179 Adelaide   SA            36973 Steve   GEORGA‚Ä¶\n 2        197 Aston      VIC           36704 Alan    TUDGE  \n 3        198 Ballarat   VIC           36409 Cather‚Ä¶ KING   \n 4        103 Banks      NSW           37018 David   COLEMAN\n 5        180 Barker     SA            37083 Tony    PASIN  \n 6        104 Barton     NSW           36820 Linda   BURNEY \n 7        192 Bass       TAS           37134 Bridge‚Ä¶ ARCHER \n 8        318 Bean       ACT           36231 David   SMITH  \n 9        200 Bendigo    VIC           36424 Lisa    CHESTE‚Ä¶\n10        105 Bennelong  NSW           36827 Jerome  LAXALE \n# ‚Ñπ 141 more rows\n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-4",
    "href": "lectures/lecture-02-content.html#acquire-4",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\nhead() shows the first six rows.\n\n\nhead(raw_elections_data)\n\n# A tibble: 6 √ó 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve    GEORGA‚Ä¶\n2        197 Aston      VIC           36704 Alan     TUDGE  \n3        198 Ballarat   VIC           36409 Catheri‚Ä¶ KING   \n4        103 Banks      NSW           37018 David    COLEMAN\n5        180 Barker     SA            37083 Tony     PASIN  \n6        104 Barton     NSW           36820 Linda    BURNEY \n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-5",
    "href": "lectures/lecture-02-content.html#acquire-5",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\ntail() shows the last six rows.\n\n\ntail(raw_elections_data)\n\n# A tibble: 6 √ó 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm  Surname\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra  SPENDER\n2        153 Werriwa    NSW           36810 Anne Ma‚Ä¶ STANLEY\n3        150 Whitlam    NSW           36811 Stephen  JONES  \n4        178 Wide Bay   QLD           37506 Llew     O'BRIEN\n5        234 Wills      VIC           36452 Peter    KHALIL \n6        316 Wright     QLD           37500 Scott    BUCHHO‚Ä¶\n# ‚Ñπ 2 more variables: PartyNm &lt;chr&gt;, PartyAb &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-6",
    "href": "lectures/lecture-02-content.html#acquire-6",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n‚ÄúWe are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision.‚Äù [@alexander2023telling]\n\n\nLet‚Äôs clean.\n\naus_voting_data &lt;- here(\"data\", \"australian_voting.csv\")\n\nraw_elections_data &lt;-\n    read_csv(\n        file = aus_voting_data,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-7",
    "href": "lectures/lecture-02-content.html#acquire-7",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nclean_names() makes variables easier to type.\n\ncleaned_elections_data &lt;- clean_names(raw_elections_data)\n\n Let‚Äôs look at the first 6 rows.\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 8\n  division_id division_nm state_ab candidate_id given_nm \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1         179 Adelaide    SA              36973 Steve    \n2         197 Aston       VIC             36704 Alan     \n3         198 Ballarat    VIC             36409 Catherine\n4         103 Banks       NSW             37018 David    \n5         180 Barker      SA              37083 Tony     \n6         104 Barton      NSW             36820 Linda    \n# ‚Ñπ 3 more variables: surname &lt;chr&gt;, party_nm &lt;chr&gt;,\n#   party_ab &lt;chr&gt;"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-8",
    "href": "lectures/lecture-02-content.html#acquire-8",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n\n\n‚úåÔ∏è R Tip\nWe can choose certain variables of interest with select() from dplyr, which we loaded as part of the tidyverse. The pipe operator |&gt; pushes the output of one line to be the first input of the function on the next line.\n\n\n\n\nWe are primarily interested in two variables:\ndivision_nm (division name)party_nm (party name)\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    select(\n        division_nm,\n        party_nm\n    )"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-9",
    "href": "lectures/lecture-02-content.html#acquire-9",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\n\nThis looks good, but some of the variable names are still not obvious because they are abbreviated."
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-10",
    "href": "lectures/lecture-02-content.html#acquire-10",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\nWe can look at the names of the columns (i.e., variables) in a dataset using names(). We can change them using rename() from dplyr.\n\n\n\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\nLet‚Äôs rename."
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-11",
    "href": "lectures/lecture-02-content.html#acquire-11",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    rename(\n        division = division_nm,\n        elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-12",
    "href": "lectures/lecture-02-content.html#acquire-12",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nWhat are the unique values in elected_party?\n\ncleaned_elections_data$elected_party |&gt;\n    unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\nCool, but let‚Äôs simplify the party names in elected_party to match what we simulated. We can do this with case_match() from dplyr."
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-13",
    "href": "lectures/lecture-02-content.html#acquire-13",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\ncleaned_elections_data &lt;-\n    cleaned_elections_data |&gt;\n    mutate(\n        elected_party =\n            case_match(\n                elected_party,\n                \"Australian Labor Party\" ~ \"Labor\",\n                \"Liberal National Party of Queensland\" ~ \"Liberal\",\n                \"Liberal\" ~ \"Liberal\",\n                \"The Nationals\" ~ \"Nationals\",\n                \"The Greens\" ~ \"Greens\",\n                \"Independent\" ~ \"Other\",\n                \"Katter's Australian Party (KAP)\" ~ \"Other\",\n                \"Centre Alliance\" ~ \"Other\"\n            )\n    )"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-14",
    "href": "lectures/lecture-02-content.html#acquire-14",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\n\nOur data now matches our plan! üòé"
  },
  {
    "objectID": "lectures/lecture-02-content.html#aus_elections_clean_path",
    "href": "lectures/lecture-02-content.html#aus_elections_clean_path",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nLet‚Äôs save the cleaned data so that we can start with it data in the next stage. We‚Äôll use a new filename to preserve the original and make it easy to identify the clean version.\n\naus_elections_clean_path &lt;- here(\"data\", \"cleaned_elections_data.csv\")\n\nwrite_csv(\n    x = cleaned_elections_data,\n    file = aus_elections_clean_path\n)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-1",
    "href": "lectures/lecture-02-content.html#explore-understand-1",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n How do we build the graph that we planned?"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-2",
    "href": "lectures/lecture-02-content.html#explore-understand-2",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\nFirst, we read in the cleaned dataset that we just created.\n\ncleaned_elections_data &lt;-\n    read_csv(\n        file = aus_elections_clean_path,\n        show_col_types = FALSE\n    )\n\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\n\n\n\nI‚Äôm using the filepath object I previously created: aus_elections_clean_path.\n\naus_elections_clean_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/cleaned_elections_data.csv\"\n\n\n This won‚Äôt work in a new script unless we re-create the object. Can you explain why?"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-3",
    "href": "lectures/lecture-02-content.html#explore-understand-3",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 √ó 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\nüòé"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-4",
    "href": "lectures/lecture-02-content.html#explore-understand-4",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nHow many seats did each party win?\n\n\n\n\nWe can get a quick count with count() from dplyr.\n\ncleaned_elections_data |&gt;\n    count(elected_party)\n\n# A tibble: 5 √ó 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-5",
    "href": "lectures/lecture-02-content.html#explore-understand-5",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\n\n\n\n\nRemember, we‚Äôre trying to make something like this.\n\n\n\n\n\n\n\n\n\n‚úåÔ∏è R Tip\n\n\n\nThe grammar of graphics is a conceptual framework for constructing data visualizations. It breaks down plots to their most basic elements, like data, scales, geoms (geometric objects), coordinates, and statistical transformations. The idea is to plan and build our vizualizations by layering these basic elements together rather than mindlessly relying on generic chart types.\nggplot2, a data visualization library from the tidyverse, is designed around the grammar of graphics idea. We build data visualizations by layering the desired elements of our plots. For example, we use aes() to specify aesthetic mappings that link our data to visual elements like position, color, size, shape, and transparency. We can create and tweak just about any visualization we want by layering data, aesthetics, and geoms using the add operator, +.\n\n\n\n\n\n, allowing the viewer to interpret the values and relationships in the dataset visually. By mapping data to these properties, we can layer information on the same plot and enhance the viewer‚Äôs understanding of patterns, trends, and differences.\nIn ggplot2, aesthetics are specified within the aes() function, where each aesthetic is mapped to a data variable. For instance, x and y represent positions on the axes, while color, fill, size, and shape control other visual aspects. By carefully selecting aesthetics, we can add depth to the plot without clutter, guiding the viewer‚Äôs eye to the most important parts."
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-6",
    "href": "lectures/lecture-02-content.html#explore-understand-6",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\nLet‚Äôs visualize the counts as vertical bars using geom_bar() from ggplot2.\n\nggplot(\n    cleaned_elections_data, # specify the data\n    aes(x = elected_party) # specify aesthetics\n) + # add a layer with the + operator\n    geom_bar() # specify a geometric shape (bar)\n\n\nBut it‚Äôs cleaner to use the pipe operator |&gt;.\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\n\n\n\n\n\n\nFigure¬†2: Meh. We can do better."
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-7",
    "href": "lectures/lecture-02-content.html#explore-understand-7",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() + # Improve the theme\n    labs(x = \"Party\", y = \"Number of seats\") # Improve the labels\n\n\n\n\n\n\n\nFigure¬†3: Number of seats won, by political party, at the 2022 Australian Federal Election. üòé"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section",
    "href": "lectures/lecture-02-content.html#section",
    "title": "preliminaries",
    "section": "",
    "text": "cleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar()\n\ncleaned_elections_data |&gt;\n    ggplot(aes(x = elected_party)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(x = \"Party\", y = \"Number of seats\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Default theme and labels\n\n\n\n\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\n\n\n\nFigure¬†4: Both versions of the plot, and the code that produced them, side-by-side for comparison."
  },
  {
    "objectID": "lectures/lecture-02-content.html#share-1",
    "href": "lectures/lecture-02-content.html#share-1",
    "title": "preliminaries",
    "section": " share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties‚Äî‚ÄúLiberal‚Äù and ‚ÄúLabor‚Äù‚Äîtwo minor parties‚Äî‚ÄúNationals‚Äù and ‚ÄúGreens‚Äù‚Äîand many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R [@citeR] including the tidyverse [@tidyverse] and janitor [@janitor]. We then created a graph of the number of seats that each political party won (Figure¬†3).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\n\n\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, @biasbehindbars and @bronnerftw."
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan-4",
    "href": "lectures/lecture-02-content.html#plan-4",
    "title": "preliminaries",
    "section": " plan",
    "text": "plan\n\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure¬†5 (a) (next slide).\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure¬†5 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan-5",
    "href": "lectures/lecture-02-content.html#plan-5",
    "title": "preliminaries",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a table\n\n\n\n\n\n\n\nFigure¬†5: Sketches of a dataset and table of the average number of beds occupied each month for shelters in Toronto."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-6",
    "href": "lectures/lecture-02-content.html#simulate-6",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process. When we turn to analysis, it will provide us with a guide. Conducting analysis without first using simulation can be thought of as shooting arrows without a target‚Äîwhile you are certainly doing something, it is not clear whether you are doing it well."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-7",
    "href": "lectures/lecture-02-content.html#simulate-7",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n    tibble(\n        date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n        # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n        shelter = c(\n            rep(x = \"Shelter 1\", times = 365),\n            rep(x = \"Shelter 2\", times = 365),\n            rep(x = \"Shelter 3\", times = 365)\n        ),\n        number_occupied =\n            rpois(\n                n = 365 * 3,\n                lambda = 30\n            ) # Draw 1,095 times from the Poisson distribution\n    )\n\nsimulated_occupancy_data\n\n# A tibble: 1,095 √ó 3\n   date       shelter   number_occupied\n   &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n 1 2021-01-01 Shelter 1              28\n 2 2021-01-02 Shelter 1              29\n 3 2021-01-03 Shelter 1              35\n 4 2021-01-04 Shelter 1              25\n 5 2021-01-05 Shelter 1              21\n 6 2021-01-06 Shelter 1              30\n 7 2021-01-07 Shelter 1              28\n 8 2021-01-08 Shelter 1              31\n 9 2021-01-09 Shelter 1              27\n10 2021-01-10 Shelter 1              27\n# ‚Ñπ 1,085 more rows\n\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it later in the course."
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-16",
    "href": "lectures/lecture-02-content.html#acquire-16",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    # Each package is associated with a unique id  found in the \"For\n    # Developers\" tab of the relevant page from Open Data Toronto\n    # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n    list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n    # Within that package, we are interested in the 2021 dataset\n    filter(name ==\n        \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n    # Having reduced the dataset to one row we can get the resource\n    get_resource()\n\nwrite_csv(\n    x = toronto_shelters,\n    file = here(\"data\", \"toronto_shelters.csv\")\n)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-17",
    "href": "lectures/lecture-02-content.html#acquire-17",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\n\ntoronto_shelters &lt;-\n    read_csv(\n        here(\"data\", \"toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\nhead(toronto_shelters)\n\n# A tibble: 6 √ó 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME    \n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                \n1     1 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n2     2 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n3     3 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n4     4 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n5     5 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n6     6 21-01-01                    24 COSTI Immigrant Serv‚Ä¶\n# ‚Ñπ 28 more variables: SHELTER_ID &lt;dbl&gt;,\n#   SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;,\n#   LOCATION_POSTAL_CODE &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-18",
    "href": "lectures/lecture-02-content.html#acquire-18",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nWe‚Äôll change the names to make them easier to type using clean_names(), and select() the relevant columns.\n\ntoronto_shelters_clean &lt;-\n    clean_names(toronto_shelters) |&gt;\n    mutate(occupancy_date = ymd(occupancy_date)) |&gt;\n    select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 √ó 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6"
  },
  {
    "objectID": "lectures/lecture-02-content.html#acquire-19",
    "href": "lectures/lecture-02-content.html#acquire-19",
    "title": "preliminaries",
    "section": " acquire",
    "text": "acquire\n\nAll that remains for this step is to save the cleaned dataset.\n\nwrite_csv(\n    x = toronto_shelters_clean,\n    file = here(\"data\", \"cleaned_toronto_shelters.csv\")\n)\n\n\nWHERE ARE THESE NAs COMING FROM?"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-8",
    "href": "lectures/lecture-02-content.html#explore-understand-8",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_toronto_shelters.csv\"),\n        show_col_types = FALSE\n    )\n\ntoronto_shelters_clean\n\n# A tibble: 50,944 √ó 2\n   occupancy_date occupied_beds\n   &lt;date&gt;                 &lt;dbl&gt;\n 1 2021-01-01                NA\n 2 2021-01-01                NA\n 3 2021-01-01                NA\n 4 2021-01-01                NA\n 5 2021-01-01                NA\n 6 2021-01-01                 6\n 7 2021-01-01                NA\n 8 2021-01-01                NA\n 9 2021-01-01                NA\n10 2021-01-01                NA\n# ‚Ñπ 50,934 more rows"
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-9",
    "href": "lectures/lecture-02-content.html#explore-understand-9",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable()\n\n\n\nTable¬†1: Shelter usage in Toronto in 2021\n\n\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426\n\n\n\n\n\n\n\n\n\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments‚Äî‚Äúlabel‚Äù and ‚Äúabbr‚Äù‚Äîto get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in sec-farm-data and sec-exploratory-data-analysis. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create tbl-homelessoccupancyd."
  },
  {
    "objectID": "lectures/lecture-02-content.html#explore-understand-10",
    "href": "lectures/lecture-02-content.html#explore-understand-10",
    "title": "preliminaries",
    "section": " explore / understand",
    "text": "explore / understand\n\n\ntoronto_shelters_clean |&gt;\n    mutate(occupancy_month = month(\n        occupancy_date,\n        label = TRUE,\n        abbr = FALSE\n    )) |&gt;\n    arrange(month(occupancy_date)) |&gt;\n    drop_na(occupied_beds) |&gt;\n    summarise(\n        number_occupied = mean(occupied_beds),\n        .by = occupancy_month\n    ) |&gt;\n    kable(\n        col.names = c(\"Month\", \"Average daily number of&lt;br&gt;occupied beds (per shelter)\"),\n        digits = 1\n    )\n\n\n\n\nMonth\nAverage daily number ofoccupied beds (per shelter)\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5\n\n\n\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (tbl-homelessoccupancy). In particular we make the column names easier to read, and only show an appropriate number of decimal places."
  },
  {
    "objectID": "lectures/lecture-02-content.html#share-3",
    "href": "lectures/lecture-02-content.html#share-3",
    "title": "preliminaries",
    "section": " share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R [@citeR] as well as the tidyverse [@Wickham2017], janitor [@janitor], opendatatoronto [@citeSharla], lubridate [@GrolemundWickham2011], and knitr [@citeknitr]. We then made a table of the average number of occupied beds each night for each month (tbl-homelessoccupancy).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (tbl-homelessoccupancy). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\n\n\n\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of @hao2019, that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan-7",
    "href": "lectures/lecture-02-content.html#plan-7",
    "title": "preliminaries",
    "section": " plan",
    "text": "plan\n\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure¬†6 (a) (next slide). We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure¬†6 (b) (next slide)."
  },
  {
    "objectID": "lectures/lecture-02-content.html#plan-8",
    "href": "lectures/lecture-02-content.html#plan-8",
    "title": "preliminaries",
    "section": " plan",
    "text": "plan\n\n\n\n\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\n\n\n\nFigure¬†6: Sketches of a dataset and graph about the neonatal mortality rate (NMR)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-9",
    "href": "lectures/lecture-02-content.html#simulate-9",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nTo simulate some data that aligns with our plan, we will need three columns: country, year, and NMR. We can do this by repeating the name of each country 50 times with rep(), and enabling the passing of 50 years. Then we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-10",
    "href": "lectures/lecture-02-content.html#simulate-10",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", 50), rep(\"Australia\", 50),\n                rep(\"Canada\", 50), rep(\"Kenya\", 50)\n            ),\n        year =\n            rep(c(1971:2020), 4),\n        nmr =\n            runif(n = 200, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4"
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-11",
    "href": "lectures/lecture-02-content.html#simulate-11",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-12",
    "href": "lectures/lecture-02-content.html#simulate-12",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n    tibble(\n        country =\n            c(\n                rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n                rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)\n            ),\n        year =\n            rep(c(1:number_of_years + 1970), 4),\n        nmr =\n            runif(n = number_of_years * 4, min = 0, max = 100)\n    )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-13",
    "href": "lectures/lecture-02-content.html#simulate-13",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat ‚Äúcountry‚Äù is exclusively one of these four: ‚ÄúArgentina‚Äù, ‚ÄúAustralia‚Äù, ‚ÄúCanada‚Äù, or ‚ÄúKenya‚Äù.\nConversely, ‚Äúcountry‚Äù contains all those four countries.\nThat ‚Äúyear‚Äù is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat ‚Äúnmr‚Äù is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass."
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-14",
    "href": "lectures/lecture-02-content.html#simulate-14",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\nsimulated_nmr_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |&gt; max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-02-content.html#simulate-15",
    "href": "lectures/lecture-02-content.html#simulate-15",
    "title": "preliminaries",
    "section": " simulate",
    "text": "simulate\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others."
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-1",
    "href": "lectures/lecture-02-content.html#section-1",
    "title": "preliminaries",
    "section": "",
    "text": "The UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\nigme_data_path &lt;- here(\"data\", \"igme.csv\")\nigme_data_path\n\n[1] \"/Users/johnmclevey/Projects/SOCI3040/data/igme.csv\"\n\n\n\nraw_igme_data &lt;-\n    read_csv(\n        file =\n            \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n        show_col_types = FALSE\n    )\n\nwrite_csv(x = raw_igme_data, file = igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-2",
    "href": "lectures/lecture-02-content.html#section-2",
    "title": "preliminaries",
    "section": "",
    "text": "raw_igme_data &lt;-\n    read_csv(\n        file = igme_data_path,\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-3",
    "href": "lectures/lecture-02-content.html#section-3",
    "title": "preliminaries",
    "section": "",
    "text": "With established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail()\n\nhead(raw_igme_data)\n\n# A tibble: 6 √ó 29\n  `Geographic area` Indicator        Sex   `Wealth Quintile`\n  &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n2 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n3 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n4 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n5 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n6 Afghanistan       Neonatal mortal‚Ä¶ Total Total            \n# ‚Ñπ 25 more variables: `Series Name` &lt;chr&gt;,\n#   `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;,\n#   COUNTRY_NOTES &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-4",
    "href": "lectures/lecture-02-content.html#section-4",
    "title": "preliminaries",
    "section": "",
    "text": "and what the names of the columns are with names()\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\""
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-5",
    "href": "lectures/lecture-02-content.html#section-5",
    "title": "preliminaries",
    "section": "",
    "text": "We would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where ‚ÄúSex‚Äù is ‚ÄúTotal‚Äù, ‚ÄúSeries Name‚Äù is ‚ÄúUN IGME estimate‚Äù, ‚ÄúGeographic area‚Äù is one of ‚ÄúArgentina‚Äù, ‚ÄúAustralia‚Äù, ‚ÄúCanada‚Äù, and ‚ÄúKenya‚Äù, and the ‚ÄúIndicator‚Äù is ‚ÄúNeonatal mortality rate‚Äù. After this we are interested in just a few columns: ‚Äúgeographic_area‚Äù, ‚Äútime_period‚Äù, and ‚Äúobs_value‚Äù."
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-6",
    "href": "lectures/lecture-02-content.html#section-6",
    "title": "preliminaries",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    clean_names(raw_igme_data) |&gt;\n    filter(\n        sex == \"Total\",\n        series_name == \"UN IGME estimate\",\n        geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n        indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n    select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 √ó 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-7",
    "href": "lectures/lecture-02-content.html#section-7",
    "title": "preliminaries",
    "section": "",
    "text": "We need to fix two other aspects: the class of ‚Äútime_period‚Äù is character when we need it to be a year, and the name of ‚Äúobs_value‚Äù should be ‚Äúnmr‚Äù to be more informative."
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-8",
    "href": "lectures/lecture-02-content.html#section-8",
    "title": "preliminaries",
    "section": "",
    "text": "cleaned_igme_data &lt;-\n    cleaned_igme_data |&gt;\n    mutate(\n        time_period = str_remove(time_period, \"-06\"),\n        time_period = as.integer(time_period)\n    ) |&gt;\n    filter(time_period &gt;= 1971) |&gt;\n    rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 √ó 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-9",
    "href": "lectures/lecture-02-content.html#section-9",
    "title": "preliminaries",
    "section": "",
    "text": "Finally, we can check that our dataset passes the tests that we developed based on the simulated dataset."
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-10",
    "href": "lectures/lecture-02-content.html#section-10",
    "title": "preliminaries",
    "section": "",
    "text": "cleaned_igme_data$country |&gt;\n    unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n    unique() |&gt;\n    length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-11",
    "href": "lectures/lecture-02-content.html#section-11",
    "title": "preliminaries",
    "section": "",
    "text": "All that remains is to save the nicely cleaned dataset.\n\ncleaned_igme_data_path &lt;- here(\"data\", \"cleaned_igme_data.csv\")\nwrite_csv(x = cleaned_igme_data, file = cleaned_igme_data_path)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-12",
    "href": "lectures/lecture-02-content.html#section-12",
    "title": "preliminaries",
    "section": "",
    "text": "We would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\ncleaned_igme_data &lt;-\n    read_csv(\n        here(\"data\", \"cleaned_igme_data.csv\"),\n        show_col_types = FALSE\n    )"
  },
  {
    "objectID": "lectures/lecture-02-content.html#section-13",
    "href": "lectures/lecture-02-content.html#section-13",
    "title": "preliminaries",
    "section": "",
    "text": "We can now make a graph of how NMR has changed over time and the differences between countries (Figure¬†7).\n\n\ncleaned_igme_data |&gt;\n    ggplot(aes(x = year, y = nmr, color = country)) +\n    geom_point() +\n    theme_minimal() +\n    labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n    scale_color_brewer(palette = \"Set1\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure¬†7: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)"
  },
  {
    "objectID": "lectures/lecture-02-content.html#share-5",
    "href": "lectures/lecture-02-content.html#share-5",
    "title": "preliminaries",
    "section": " share",
    "text": "share\nExample taken directly from @alexander2023telling, here.\n\n\n\n\n\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries‚ÄîArgentina, Australia, Canada, and Kenya‚Äîover the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R [@citeR].\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure¬†7). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  }
]